# LAB 02 ‚Äì Bronze ‚Üí Silver

-	Dedup theo order_id
-	Filter status = SUCCESS
-	Chu·∫©n ho√° schema
-	Upsert (MERGE) b·∫±ng:
>+	Iceberg (chu·∫©n prod nh·∫•t) üî•
>+	ho·∫∑c Delta (n·∫øu mu·ªën)

---

##  LAB SILVER (Bronze -> Silver)

### üéØ Silver rules (ƒë∆°n gi·∫£n nh∆∞ng ƒë√∫ng production mindset)
	
**ƒê·ªçc stream t·ª´ Bronze (Parquet on MinIO)**

***Chu·∫©n ho√°:**
-	event_time = to_timestamp(event_ts)
-	amount cast chu·∫©n
-	Dedup theo order_id (gi·ªØ b·∫£n ghi m·ªõi nh·∫•t theo event_time)
-	Ghi Silver Parquet, partition theo event_date, country
-	D√πng foreachBatch ƒë·ªÉ x·ª≠ l√Ω ‚Äúmicro-batch merge-lite‚Äù

---

## üìÅ File: `spark/lab/lab_stream_bronze_to_silver_orders.py`

```python

from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.window import Window

spark = (
    SparkSession.builder
    .appName("orders-bronze-to-silver")
    .config("spark.sql.sources.partitionOverwriteMode", "dynamic")
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

BRONZE_PATH = "s3a://lakehouse/bronze/orders"
SILVER_PATH = "s3a://lakehouse/silver/orders"
CHECKPOINT_PATH = "s3a://lakehouse/_checkpoints/orders_silver"

def upsert_like(batch_df, batch_id: int):
    if batch_df.rdd.isEmpty():
        return

    df = (
        batch_df
        .withColumn("event_time", to_timestamp("event_ts"))
        .withColumn("event_date", to_date(col("event_time")))
        .withColumn("amount", col("amount").cast("double"))
        .filter(col("order_id").isNotNull())
        .filter(col("event_time").isNotNull())
    )

    # keep latest per order_id in this micro-batch
    w = Window.partitionBy("order_id").orderBy(col("event_time").desc())
    df_latest = (
        df.withColumn("rn", row_number().over(w))
          .filter(col("rn") == 1)
          .drop("rn")
    )

    (
        df_latest
        .write
        .mode("overwrite")   # dynamic partition overwrite (only touched partitions)
        .format("parquet")
        .partitionBy("event_date", "country")
        .save(SILVER_PATH)
    )

bronze_stream = (
    spark.readStream
    .format("parquet")
    .load(BRONZE_PATH)
)

query = (
    bronze_stream.writeStream
    .foreachBatch(upsert_like)
    .option("checkpointLocation", CHECKPOINT_PATH)
    .trigger(processingTime="30 seconds")
    .start()
)

query.awaitTermination()
```

---

## ‚ñ∂Ô∏è Ch·∫°y LAB SILVER

```bash
scripts/spark_submit.sh spark/lab/lab_stream_bronze_to_silver_orders.py
```

---

## ‚úÖ Verify Silver nhanh

```bash
scripts/spark_inline.sh <<'PY'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

df = spark.read.parquet("s3a://lakehouse/silver/orders")
df.orderBy("event_ts", ascending=False).show(10, truncate=False)
print("count =", df.count())
PY
```

