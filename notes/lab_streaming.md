# Streaming

```mermaid
flowchart LR
  A[Bronze: orders_raw files] -->|readStream + schema| B[Streaming Micro-batch]
  B -->|foreachBatch| C[Silver: orders_fact_dt_stream 'partition by dt']
  C --> D[Gold: agg by dt/country/segment]
  E[Dims: customers_dim, merchants_dim] --> D
  F[Governance/Provenance] --> C
  F --> D
```
---

# âœ… LAB 3.3 â€“ Streaming â†’ Silver (late data + incremental + overwrite partition)

## 0) SÆ¡ Ä‘á»“ tá»•ng quan (Ä‘Ãºng â€œreal-lifeâ€)

```mermaid
flowchart LR
  A[Bronze files 'JSON/CSV/Parquet'] -->|Structured Streaming 'file source'| B[Micro-batches]
  B --> |Watermark + Dedup| C[Upsert by partition dt]
  C --> D[Silver: Parquet partitioned by dt]
  D --> E[Gold KPI daily]
  E --> F[Governance: job_runs + schema_registry]
```

---

## 1) Táº¡o dá»¯ liá»‡u Bronze theo â€œmicro-batchâ€ (giáº£ láº­p stream)

Táº¡o file: `spark/prepare_data/lab_bronze_generate_orders.py`

```python
# spark/lab/lab_bronze_generate_orders.py
import os
import json
import time
import random
from datetime import datetime, timedelta

BRONZE_DIR = "data/bronze_lab33/orders_raw"
os.makedirs(BRONZE_DIR, exist_ok=True)

def gen_batch(batch_id: int, n: int, base_event_time: datetime, late_ratio: float = 0.15):
    rows = []
    for i in range(n):
        order_id = str(batch_id * 1000000 + i)
        customer_id = str(1 if random.random() < 0.25 else random.randint(2, 50000))  # skew nháº¹
        merchant_id = str(random.randint(1, 3000))
        amount = round(random.random() * 5000, 6)
        country = random.choice(["VN", "TH", "SG", "ID", "MY"])
        channel = random.choice(["POS", "ECOM", "ATM"])
        status = random.choice(["SUCCESS", "FAILED", "REVERSED"])

        # event_ts: pháº§n lá»›n lÃ  ngÃ y â€œhÃ´m nayâ€, má»™t pháº§n lÃ  late data (lÃ¹i 1-3 ngÃ y)
        if random.random() < late_ratio:
            event_ts = base_event_time - timedelta(days=random.randint(1, 3))
        else:
            event_ts = base_event_time

        ingest_ts = datetime.now()
        dt = event_ts.date().isoformat()

        rows.append({
            "order_id": order_id,
            "customer_id": customer_id,
            "merchant_id": merchant_id,
            "amount": amount,
            "event_ts": event_ts.strftime("%Y-%m-%d %H:%M:%S"),
            "channel": channel,
            "country": country,
            "status": status,
            "ingest_ts": ingest_ts.strftime("%Y-%m-%d %H:%M:%S"),
            "dt": dt
        })
    return rows

def write_batch_json(batch_id: int, rows: list[dict]):
    path = os.path.join(BRONZE_DIR, f"batch_{batch_id:04d}.json")
    with open(path, "w", encoding="utf-8") as f:
        for r in rows:
            f.write(json.dumps(r) + "\n")
    print(f"âœ… wrote {path} rows={len(rows)}")

if __name__ == "__main__":
    random.seed(7)
    base = datetime(2026, 1, 10, 10, 0, 0)

    # 6 micro-batches, má»—i batch 20k rows (tuá»³ mÃ¡y báº¡n tÄƒng/giáº£m)
    for b in range(6):
        rows = gen_batch(batch_id=b, n=20000, base_event_time=base, late_ratio=0.20)
        write_batch_json(b, rows)
        time.sleep(1)  # táº¡o cáº£m giÃ¡c â€œfile Ä‘áº¿n theo thá»i gianâ€
```

--- 

Cháº¡y

```bash
rm -rf data/bronze_lab33/orders_raw
mkdir -p data/bronze_lab33/orders_raw

python spark/lab/lab_bronze_generate_orders.py
ls -lah data/bronze_lab33/orders_raw | head
```

---

## 2) Streaming ingest Bronze â†’ Silver (Upsert theo partition dt)

VÃ¬ sao báº¡n tá»«ng bá»‹ lá»—i FILE_NOT_EXIST?

Báº¡n Ä‘Ã£ lÃ m kiá»ƒu â€œÄ‘á»c silver partition cÅ© + overwrite partition Ä‘Ã³â€ ngay trong foreachBatch, rá»“i láº¡i .count() trÃªn DF cÃ³ thá»ƒ trigger Ä‘á»c/scan file Ä‘ang bá»‹ thay tháº¿ â†’ local FS ráº¥t dá»… â€œÄ‘á»©t fileâ€.

#### âœ… Fix thá»±c táº¿ cho local Parquet: ghi ra tmp/ trÆ°á»›c â†’ swap (replace) thÆ° má»¥c partition (atomic-ish).

Táº¡o file: `spark/lab/lab_stream_to_silver_orders_v2.py`

```python
# spark/lab/lab_stream_to_silver_orders_v2.py
import os
import shutil
from datetime import datetime

from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, TimestampType, DateType
)
from pyspark.sql.functions import col, to_timestamp, to_date, lit, row_number
from pyspark.sql.window import Window

BRONZE_ORDERS_PATH = "data/bronze_lab33/orders_raw"
SILVER_OUT = "data/silver/orders_fact_dt_stream"
CHECKPOINT = "checkpoints/orders_to_silver_v2"

TMP_ROOT = "data/tmp/orders_upsert"

os.makedirs(SILVER_OUT, exist_ok=True)
os.makedirs(CHECKPOINT, exist_ok=True)
os.makedirs(TMP_ROOT, exist_ok=True)

spark = (
    SparkSession.builder
    .appName("lab_stream_to_silver_orders_v2")
    .config("spark.sql.shuffle.partitions", "50")
    # streaming khÃ´ng dÃ¹ng AQE: Spark sáº½ tá»± warning & disable
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

# 1) Streaming file source báº¯t buá»™c schema (báº¡n gáº·p lá»—i nÃ y láº§n Ä‘áº§u)
schema = StructType([
    StructField("order_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("merchant_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("event_ts", StringType(), True),   # bronze lÃ  string
    StructField("channel", StringType(), True),
    StructField("country", StringType(), True),
    StructField("status", StringType(), True),
    StructField("ingest_ts", StringType(), True),
    StructField("dt", StringType(), True),         # bronze dt string
])

def atomic_replace_dir(src_dir: str, dst_dir: str):
    """
    Thao tÃ¡c â€œan toÃ n tÆ°Æ¡ng Ä‘á»‘iâ€ trÃªn local:
    - XoÃ¡ dst náº¿u tá»“n táº¡i
    - Move src -> dst (rename/move lÃ  atomic hÆ¡n copy)
    """
    if os.path.exists(dst_dir):
        shutil.rmtree(dst_dir)
    os.makedirs(os.path.dirname(dst_dir), exist_ok=True)
    shutil.move(src_dir, dst_dir)

def upsert_batch(batch_df, batch_id: int):
    """
    foreachBatch: cháº¡y trÃªn má»—i micro-batch (DataFrame tÄ©nh).
    Má»¥c tiÃªu:
    - watermark + dedup theo order_id (giáº£ láº­p idempotent)
    - upsert theo dt: rewrite tá»«ng dt partition báº±ng swap folder
    """
    if batch_df.rdd.isEmpty():
        print(f"â„¹ï¸ batch={batch_id} empty")
        return

    # 2) Chuáº©n hoÃ¡ kiá»ƒu dá»¯ liá»‡u
    df = (
        batch_df
        .withColumn("event_ts", to_timestamp(col("event_ts")))
        .withColumn("ingest_ts", to_timestamp(col("ingest_ts")))
        .withColumn("dt", to_date(col("dt")))
    )

    # 3) Dedup thá»±c táº¿: giá»¯ record má»›i nháº¥t theo ingest_ts cho má»—i order_id
    w = Window.partitionBy("order_id").orderBy(col("ingest_ts").desc())
    dedup = (
        df
        .withColumn("rn", row_number().over(w))
        .where(col("rn") == 1)
        .drop("rn")
    )

    # Láº¥y danh sÃ¡ch dt trong batch Ä‘á»ƒ upsert tá»«ng partition
    dts = [r["dt"] for r in dedup.select("dt").distinct().collect()]

    # TÃ­nh rows trÆ°á»›c khi write Ä‘á»ƒ khÃ´ng trigger Ä‘á»c sau khi replace
    rows_in_batch = dedup.count()
    print(f"âœ… batch={batch_id} rows_after_dedup={rows_in_batch} partitions={len(dts)} dts={dts}")

    for dtv in dts:
        dt_str = dtv.isoformat()
        part_path = os.path.join(SILVER_OUT, f"dt={dt_str}")
        tmp_path = os.path.join(TMP_ROOT, f"batch={batch_id}", f"dt={dt_str}")

        # (a) lá»c partition dt cá»§a batch
        part_new = dedup.where(col("dt") == lit(dtv))

        # (b) náº¿u partition Ä‘Ã£ tá»“n táº¡i â†’ Ä‘á»c cÅ©, union, dedup láº¡i theo order_id
        if os.path.exists(part_path):
            old = spark.read.parquet(part_path)
            merged = old.unionByName(part_new)

            w2 = Window.partitionBy("order_id").orderBy(col("ingest_ts").desc())
            final_part = (
                merged
                .withColumn("rn", row_number().over(w2))
                .where(col("rn") == 1)
                .drop("rn")
            )
        else:
            final_part = part_new

        # (c) write ra tmp (khÃ´ng Ä‘á»¥ng partition tháº­t)
        if os.path.exists(tmp_path):
            shutil.rmtree(tmp_path)
        os.makedirs(tmp_path, exist_ok=True)

        (
            final_part
            .coalesce(1)   # local lab: 1 file/partition cho dá»… nhÃ¬n (production: KHÃ”NG lÃ m váº­y)
            .write
            .mode("overwrite")
            .parquet(tmp_path)
        )

        # (d) swap folder: tmp -> dt=...
        atomic_replace_dir(tmp_path, part_path)
        print(f"   â†ª upserted dt={dt_str} ok")

# 4) Äá»c stream tá»« file source
stream_df = (
    spark.readStream
    .schema(schema)
    .json(BRONZE_ORDERS_PATH)
)

# 5) Start streaming
query = (
    stream_df
    .writeStream
    .foreachBatch(upsert_batch)
    .option("checkpointLocation", CHECKPOINT)
    .trigger(processingTime="5 seconds")
    .start()
)

print("ğŸš€ Streaming started. Spark UI:", query.sparkSession.sparkContext.uiWebUrl)

query.awaitTermination()

```

---


Cháº¡y (Ä‘Ãºng thá»© tá»±)

```bash
rm -rf data/silver/orders_fact_dt_stream
rm -rf checkpoints/orders_to_silver_v2
rm -rf data/tmp/orders_upsert
mkdir -p data/silver/orders_fact_dt_stream checkpoints/orders_to_silver_v2 data/tmp/orders_upsert

python spark/lab/lab_stream_to_silver_orders_v2.py
```

Dá»«ng Ä‘Ãºng cÃ¡ch: thay vÃ¬ Ctrl+C â€œgáº¯tâ€, báº¡n cÃ³ thá»ƒ Ä‘á»ƒ cháº¡y háº¿t batch rá»“i dá»«ng. Náº¿u váº«n cáº§n Ctrl+C, Ä‘Ã´i khi Py4J sáº½ bÃ¡o â€œreentrant callâ€ (trÃªn mac terminal hay gáº·p). KhÃ´ng sao â€” miá»…n dá»¯ liá»‡u Ä‘Ã£ ghi xong.

---

## 3) CÃ¡ch kiá»ƒm tra â€œÄ‘Ã£ sáº¡ch warning dt chÆ°aâ€

Sau khi cháº¡y xong stream v2:

#### A) Check folder partition

```bash
find data/silver/orders_fact_dt_stream -maxdepth 2 -type d | head
```

#### B) Check schema khÃ´ng warning

```bash
python - <<'PY'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = spark.read.parquet("data/silver/orders_fact_dt_stream")
print("columns:", df.columns)
df.printSchema()
print("sample:", df.orderBy("ingest_ts", ascending=False).limit(5).toPandas())
spark.stop()
PY
```

#### âœ… Náº¿u báº¡n khÃ´ng cÃ²n tháº¥y:

```code
WARN DataSource: [COLUMN_ALREADY_EXISTS] The column dt already exists
```

thÃ¬ dataset Ä‘Ã£ â€œchuáº©nâ€.

Náº¿u warning cÃ²n xuáº¥t hiá»‡n: 99% lÃ  do váº«n cÃ²n â€œrÃ¡câ€ file tá»« run cÅ© trong cÃ¹ng path. Khi Ä‘Ã³: rm -rf data/silver/orders_fact_dt_stream rá»“i cháº¡y láº¡i lÃ  dá»©t Ä‘iá»ƒm.

---

### âœ… Tiáº¿p: Governance/Provenance (fix luÃ´n warning dt á»Ÿ Gold)

Báº¡n Ä‘ang cháº¡y lab_gov_kpi_daily.py vÃ  tháº¥y warning dt already exists â€” cÃ¡i nÃ y khÃ´ng pháº£i do code join, mÃ  do GOLD_OUT Ä‘Ã£ tá»«ng Ä‘Æ°á»£c ghi theo layout khÃ¡c (lÃºc khÃ´ng partition, lÃºc partition).

Báº¡n Ä‘Ã£ thÃªm Ä‘oáº¡n xoÃ¡ GOLD_OUT trÆ°á»›c khi write rá»“i, Ä‘Ãºng hÆ°á»›ng.

CÃ¡ch xÃ¡c Ä‘á»‹nh â€œÄ‘Ã£ háº¿t warning tháº­t sá»± chÆ°a?â€

Sau khi báº¡n â€œÄ‘áº­p sáº¡châ€ gold:

```bash
rm -rf data/gold/kpi_daily
python spark/lab/lab_gov_kpi_daily.py
```

Rá»“i kiá»ƒm tra:


```bash
python - <<'PY'
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = spark.read.parquet("data/gold/kpi_daily")
print("columns:", df.columns)
df.printSchema()
spar
```

âœ… Náº¿u schema chá»‰ cÃ³ 1 cá»™t dt vÃ  lÃºc read khÃ´ng warning â†’ OK.

---

### âœ… BÆ°á»›c tiáº¿p theo cá»§a báº¡n (gá»£i Ã½ â€œÄ‘Ãºng flow labâ€)

1.	Generate bronze:

```bash
python spark/lab/lab_bronze_generate_orders.py
```

2.	Run streaming v2 (upsert á»•n Ä‘á»‹nh):

```bash
python spark/lab/lab_stream_to_silver_orders_v2.py
```

3.	Run gold KPI + governance:

```bash
rm -rf data/gold/kpi_daily
python spark/lab/lab_gov_kpi_daily.py
```

4.	Verify:

*	`spark.read.parquet("data/silver/orders_fact_dt_stream")` khÃ´ng warning dt
*	`spark.read.parquet("data/gold/kpi_daily")` khÃ´ng warning dt

