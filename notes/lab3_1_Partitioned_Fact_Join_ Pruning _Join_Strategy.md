C
# LAB 3.1: Partitioned Fact Join + Pruning + Join Strategy

Lab n√†y t·∫≠p trung v√†o c√°c k·ªπ thu·∫≠t t·ªëi ∆∞u h√≥a hi·ªáu nƒÉng Spark trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø (Data Lakehouse/Warehouse), bao g·ªìm:

* **Partition Pruning**: Gi·∫£m IO b·∫±ng c√°ch ch·ªâ ƒë·ªçc d·ªØ li·ªáu c·∫ßn thi·∫øt.
* **Join Strategies**: So s√°nh Broadcast Join v√† Shuffle Join.
* **AQE (Adaptive Query Execution)**: T·ª± ƒë·ªông t·ªëi ∆∞u h√≥a trong l√∫c th·ª±c thi.

## 1. Ki·∫øn tr√∫c d·ªØ li·ªáu (Silver Layer)

M√¥ h√¨nh d·ªØ li·ªáu gi·∫£ l·∫≠p t√¨nh hu·ªëng th·ª±c t·∫ø v·ªõi b·∫£ng Fact l·ªõn v√† b·∫£ng Dimension nh·ªè.

```mermaid
flowchart LR
  subgraph Lakehouse[Data Lakehouse - Silver Layer]
    O[orders_fact<br/>2M rows<br/>partitioned by dt]:::fact
    C[customers_dim<br/>50k rows]:::dim
  end

  Q[Query: dt='2026-01-10'<br/>join customers<br/>group by segment,country]:::q

  C --> Q
  O --> Q

  classDef fact fill:#dff1ff,stroke:#1b75bc,stroke-width:1px;
  classDef dim fill:#e9ffe8,stroke:#2e8b57,stroke-width:1px;
  classDef q fill:#fff2cc,stroke:#b8860b,stroke-width:1px;

```

**√ù t∆∞·ªüng th·ª±c t·∫ø:**
-	orders_fact l√† b·∫£ng l·ªõn (fact)
-	customers_dim l√† b·∫£ng nh·ªè (dimension)
-	Query ph·ªï bi·∫øn nh·∫•t: l·ªçc theo ng√†y (dt) r·ªìi join dim ƒë·ªÉ ra b√°o c√°o.

---

## 2. Giai ƒëo·∫°n A: Chu·∫©n b·ªã d·ªØ li·ªáu (Data Preparation)

File n√†y t·∫°o ra d·ªØ li·ªáu m·∫´u c√≥ t√≠nh ch·∫•t **Skew** (l·ªách d·ªØ li·ªáu) v√† ƒë∆∞·ª£c **Partition** theo ng√†y (`dt`).

### M√£ ngu·ªìn: `lab3_1_prepare_fact_partitioned.py`

```python
from __future__ import annotations
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, rand, expr, element_at, pmod, date_sub, to_date
)

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n l∆∞u tr·ªØ
BASE = "data/silver_lab31"
CUSTOMERS_PATH = f"{BASE}/customers"
ORDERS_RAW_PATH = f"{BASE}/orders_raw"
ORDERS_FACT_PATH = f"{BASE}/orders_fact_dt"

N_CUSTOMERS = 50_000
N_ORDERS = 2_000_000

def build_spark(app: str) -> SparkSession:
    """Kh·ªüi t·∫°o SparkSession v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u local."""
    return (
        SparkSession.builder
        .appName(app)
        .config("spark.sql.shuffle.partitions", "50") # Gi·∫£m partition ƒë·ªÉ ch·∫°y nhanh tr√™n m√°y c√° nh√¢n
        .config("spark.sql.adaptive.enabled", "true") # B·∫≠t AQE
        .getOrCreate()
    )

def main() -> None:
    spark = build_spark("lab3_1_prepare_fact_partitioned")

    # 1) T·∫°o b·∫£ng CUSTOMERS DIM (50k d√≤ng)
    customers = (
        spark.range(0, N_CUSTOMERS)
        .select(
            (col("id") + 1).cast("string").alias("customer_id"),
            # pmod(id, 3) + 1: T·∫°o index 1, 2, 3 ƒë·ªÉ l·∫•y gi√° tr·ªã t·ª´ m·∫£ng
            element_at(expr("array('MASS','AFFLUENT','SME')"), 
                       (pmod(col("id"), 3) + 1).cast("int")).alias("segment"),
            element_at(expr("array('LOW','MED','HIGH')"), 
                       (pmod(col("id"), 3) + 1).cast("int")).alias("risk_tier"),
            date_sub(expr("date('2026-01-11')"), pmod(col("id"), 365).cast("int")).alias("created_date"),
        )
    )
    customers.write.mode("overwrite").parquet(CUSTOMERS_PATH)

    # 2) T·∫°o b·∫£ng ORDERS RAW (2M d√≤ng) - C√≥ g√¢y SKEW
    orders = (
        spark.range(0, N_ORDERS)
        .select(
            (col("id") + 1).cast("string").alias("order_id"),
            # K·ªπ thu·∫≠t g√¢y Skew: 25% d·ªØ li·ªáu s·∫Ω c√≥ customer_id = '1'
            expr("CASE WHEN rand(7) < 0.25 THEN '1' ELSE cast(pmod(id * 17, 49999) + 2 as string) END").alias("customer_id"),
            (rand(11) * 5000).alias("amount"),
            expr("timestamp('2026-01-12 10:23:17')").alias("base_ts"),
            (pmod(col("id"), 30).cast("int")).alias("day_back"),
            element_at(expr("array('VN','SG','TH','ID','MY')"), 
                       (pmod(col('id'), 5) + 1).cast("int")).alias("country")
        )
        # T√≠nh to√°n th·ªùi gian th·ª±c t·∫ø: base_ts - s·ªë ng√†y l√πi l·∫°i
        .withColumn("order_ts", expr("base_ts - make_interval(0,0,0,day_back,0,0,0)"))
        .withColumn("dt", to_date(col("order_ts")))
        .drop("base_ts", "day_back")
    )

    # 3) Ghi d·ªØ li·ªáu theo PARTITION dt
    # ƒê√¢y l√† b∆∞·ªõc quan quan tr·ªçng nh·∫•t ƒë·ªÉ t·ªëi ∆∞u Query Performance
    (orders.write.mode("overwrite").partitionBy("dt").parquet(ORDERS_FACT_PATH))
    
    spark.stop()

if __name__ == "__main__":
    main()

```

### Gi·∫£i th√≠ch k·ªπ thu·∫≠t:

#### Python k·ªπ thu·∫≠t ƒëang d√πng
-	BASE, *_PATH: h·∫±ng s·ªë ƒë·ªÉ tr√°nh hardcode path trong nhi·ªÅu file
-	build_spark(): t√°ch h√†m t·∫°o SparkSession ‚Üí d·ªÖ t√°i d√πng ·ªü nhi·ªÅu lab
-	main() + if __name__ == "__main__"::
>-	chu·∫©n Python script
>-	ch·∫°y ƒë∆∞·ª£c t·ª´ terminal, kh√¥ng ph·ª• thu·ªôc notebook

#### Spark k·ªπ thu·∫≠t c·ªët l√µi
-	partitionBy("dt"): t·∫°o layout d·∫°ng
-	.../orders_fact_dt/dt=2026-01-10/*.parquet
-	Skew c√≥ ch·ªß ƒë√≠ch: customer_id='1' chi·∫øm ~25%

---
#### C√°c k·ªπ thu·∫≠t quan tr·ªçng:


* **`pmod(col("id"), n)`**: H√†m l·∫•y d∆∞ (modulo) gi√∫p ph√¢n ph·ªëi d·ªØ li·ªáu ƒë·ªÅu v√†o c√°c nh√≥m (V√≠ d·ª•: t·∫°o ra c√°c ph√¢n kh√∫c kh√°ch h√†ng lu√¢n phi√™n).
* **`partitionBy("dt")`**: Spark s·∫Ω t·∫°o th∆∞ m·ª•c ri√™ng cho m·ªói ng√†y (e.g., `dt=2026-01-10/`). Khi truy v·∫•n l·ªçc theo ng√†y n√†y, Spark ch·ªâ qu√©t ƒë√∫ng th∆∞ m·ª•c ƒë√≥ thay v√¨ to√†n b·ªô 2 tri·ªáu d√≤ng.
* **Skew**: Vi·ªác g√°n c·ªë ƒë·ªãnh 25% records cho `customer_id='1'` s·∫Ω gi√∫p ch√∫ng ta th·∫•y ƒë∆∞·ª£c v·∫•n ƒë·ªÅ "n√∫t th·∫Øt c·ªï chai" khi th·ª±c hi·ªán Shuffle Join ·ªü c√°c b√†i sau.

---

## 3. Giai ƒëo·∫°n B: Query 1 ‚Äî Broadcast Join (M·∫∑c ƒë·ªãnh t·ªëi ∆∞u)

### M√£ ngu·ªìn: `lab3_1_q1_prune_broadcast.py`

```python
# ... (Ph·∫ßn import v√† spark config gi·ªØ nguy√™n)

def main() -> None:
    s = spark("lab3_1_q1_prune_broadcast")
    orders = s.read.parquet(FACT).alias("o")
    customers = s.read.parquet(DIM).alias("c")

    target_dt = "2026-01-10"

    q = (
        orders
        .where(col("dt") == target_dt)  # ‚úÖ PARTITION PRUNING
        .join(customers, col("o.customer_id") == col("c.customer_id"), "left")
        .groupBy("dt", "country", "c.segment")
        .agg(_count("*").alias("txns"), _sum("amount").alias("total_amount"))
    )

    q.explain("formatted") # Xem k·∫ø ho·∫°ch th·ª±c thi
    q.show()

```

### Gi·∫£i th√≠ch chi ti·∫øt t·ª´ Python ƒë·∫øn Spark:

1. **`where(col("dt") == target_dt)`**:
* **Python**: L·ªánh l·ªçc d·ªØ li·ªáu th√¥ng th∆∞·ªùng.
* **Spark Engine**: K√≠ch ho·∫°t **Partition Pruning**. Spark nh√¨n v√†o c·∫•u tr√∫c th∆∞ m·ª•c tr√™n ·ªï ƒëƒ©a v√† ch·ªâ n·∫°p d·ªØ li·ªáu t·ª´ folder `dt=2026-01-10`.


2. **`join(customers, ...)`**:
* **Spark Engine**: V√¨ b·∫£ng `customers` nh·ªè (50k d√≤ng ~ v√†i MB), Spark t·ª± ƒë·ªông d√πng **BroadcastHashJoin**. N√≥ copy to√†n b·ªô b·∫£ng nh·ªè sang RAM c·ªßa t·∫•t c·∫£ c√°c Worker.
* **L·ª£i √≠ch**: Kh√¥ng ph√°t sinh Shuffle cho b·∫£ng l·ªõn, t·ªëc ƒë·ªô c·ª±c nhanh.

### Gi·∫£i th√≠ch t·ª´ng d√≤ng tr·ªçng ƒëi·ªÉm

#### orders.where(col("dt") == target_dt)
-	V√¨ dt l√† partition column, Spark s·∫Ω t·∫°o:
-	PartitionFilters: (dt = 2026-01-10)
-	K·∫øt qu·∫£: ƒë·ªçc √≠t file h∆°n ‚Üí gi·∫£m IO c·ª±c m·∫°nh

#### .join(customers, ...)
-	customers 50k rows th∆∞·ªùng nh·ªè ‚Üí Spark s·∫Ω ch·ªçn BroadcastHashJoin
-	B·∫°n s·∫Ω th·∫•y trong explain:
-	BroadcastExchange
-	BroadcastHashJoin ... BuildRight

#### .groupBy(...).agg(...)
-	Aggregate th∆∞·ªùng t·∫°o Exchange (shuffle) ƒë·ªÉ gom key v·ªÅ ƒë√∫ng partition
	Exchange n√†y l√† ‚Äúshuffle c·∫ßn thi·∫øt‚Äù (kh√°c v·ªõi IO pruning)

### K·∫øt q·ªßa c·ªßa Query 1:

```code
== Physical Plan ==                                                             
AdaptiveSparkPlan (13)
+- Sort (12)
   +- Exchange (11)
      +- HashAggregate (10)
         +- Exchange (9)
            +- HashAggregate (8)
               +- Project (7)
                  +- BroadcastHashJoin LeftOuter BuildRight (6)
                     :- Scan parquet  (1)
                     +- BroadcastExchange (5)
                        +- Project (4)
                           +- Filter (3)
                              +- Scan parquet  (2)


(1) Scan parquet 
Output [4]: [customer_id#1, amount#2, country#4, dt#7]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab31/orders_fact_dt]
PartitionFilters: [isnotnull(dt#7), (dt#7 = 2026-01-10)]
ReadSchema: struct<customer_id:string,amount:double,country:string>

(2) Scan parquet 
Output [3]: [customer_id#8, segment#9, risk_tier#10]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab31/customers]
PushedFilters: [IsNotNull(customer_id)]
ReadSchema: struct<customer_id:string,segment:string,risk_tier:string>

(3) Filter
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Condition : isnotnull(customer_id#8)

(4) Project
Output [3]: [customer_id#8 AS c_customer_id#12, segment#9 AS c_segment#13, risk_tier#10 AS c_risk_tier#14]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(5) BroadcastExchange
Input [3]: [c_customer_id#12, c_segment#13, c_risk_tier#14]
Arguments: HashedRelationBroadcastMode(List(input[0, string, true]),false), [plan_id=31]

(6) BroadcastHashJoin
Left keys [1]: [customer_id#1]
Right keys [1]: [c_customer_id#12]
Join type: LeftOuter
Join condition: None

(7) Project
Output [5]: [amount#2, country#4, dt#7, c_segment#13, c_risk_tier#14]
Input [7]: [customer_id#1, amount#2, country#4, dt#7, c_customer_id#12, c_segment#13, c_risk_tier#14]

(8) HashAggregate
Input [5]: [amount#2, country#4, dt#7, c_segment#13, c_risk_tier#14]
Keys [4]: [dt#7, country#4, c_segment#13, c_risk_tier#14]
Functions [3]: [partial_count(1), partial_sum(amount#2), partial_avg(amount#2)]
Aggregate Attributes [4]: [count#34L, sum#35, sum#36, count#37L]
Results [8]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count#38L, sum#39, sum#40, count#41L]

(9) Exchange
Input [8]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count#38L, sum#39, sum#40, count#41L]
Arguments: hashpartitioning(dt#7, country#4, c_segment#13, c_risk_tier#14, 50), ENSURE_REQUIREMENTS, [plan_id=36]

(10) HashAggregate
Input [8]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count#38L, sum#39, sum#40, count#41L]
Keys [4]: [dt#7, country#4, c_segment#13, c_risk_tier#14]
Functions [3]: [count(1), sum(amount#2), avg(amount#2)]
Aggregate Attributes [3]: [count(1)#31L, sum(amount#2)#32, avg(amount#2)#33]
Results [7]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count(1)#31L AS txns#17L, sum(amount#2)#32 AS total_amount#18, avg(amount#2)#33 AS avg_amount#19]

(11) Exchange
Input [7]: [dt#7, country#4, c_segment#13, c_risk_tier#14, txns#17L, total_amount#18, avg_amount#19]
Arguments: rangepartitioning(country#4 ASC NULLS FIRST, c_segment#13 ASC NULLS FIRST, c_risk_tier#14 ASC NULLS FIRST, 50), ENSURE_REQUIREMENTS, [plan_id=39]

(12) Sort
Input [7]: [dt#7, country#4, c_segment#13, c_risk_tier#14, txns#17L, total_amount#18, avg_amount#19]
Arguments: [country#4 ASC NULLS FIRST, c_segment#13 ASC NULLS FIRST, c_risk_tier#14 ASC NULLS FIRST], true, 0

(13) AdaptiveSparkPlan
Output [7]: [dt#7, country#4, c_segment#13, c_risk_tier#14, txns#17L, total_amount#18, avg_amount#19]
Arguments: isFinalPlan=false


+----------+-------+---------+-----------+-----+--------------------+------------------+
|dt        |country|c_segment|c_risk_tier|txns |total_amount        |avg_amount        |
+----------+-------+---------+-----------+-----+--------------------+------------------+
|2026-01-10|TH     |AFFLUENT |MED        |16614|4.158147985775259E7 |2502.797631982219 |
|2026-01-10|TH     |MASS     |LOW        |33457|8.33218736782916E7  |2490.416764153738 |
|2026-01-10|TH     |SME      |HIGH       |16596|4.1406404070577055E7|2494.9628868749733|
+----------+-------+---------+-----------+-----+--------------------+------------------+
```

---

#### üîç Gi·∫£i m√£ Physical Plan - LAB 3.1 (Query 1)

K·∫ø ho·∫°ch th·ª±c thi c·ªßa b·∫°n cho th·∫•y Spark ƒëang ho·∫°t ƒë·ªông ·ªü tr·∫°ng th√°i t·ªëi ∆∞u nh·∫•t. H√£y nh√¨n v√†o c√°c con s·ªë ƒë√°nh d·∫•u (1) ƒë·∫øn (13):

##### 1. S·ª± l·ª£i h·∫°i c·ªßa Partition Pruning

T·∫°i node **(1) Scan parquet**:

* **D√≤ng quan tr·ªçng:** `PartitionFilters: [isnotnull(dt#7), (dt#7 = 2026-01-10)]`
* **√ù nghƒ©a:** Spark kh√¥ng ƒë·ªçc to√†n b·ªô 2 tri·ªáu d√≤ng. N√≥ nh√¨n v√†o ·ªï ƒëƒ©a, th·∫•y folder `dt=2026-01-10` v√† **ch·ªâ nh·∫£y v√†o ƒë√≥ ƒë·ªçc**. ƒê√¢y l√† c√°ch b·∫°n ti·∫øt ki·ªám ti·ªÅn v√† th·ªùi gian khi l√†m Big Data.

##### 2. Chi·∫øn thu·∫≠t Join: BroadcastHashJoin

T·∫°i node **(5)** v√† **(6)**:

* **(5) BroadcastExchange:** Spark nh·∫≠n th·∫•y b·∫£ng `customers` ƒë·ªß nh·ªè. N√≥ "ph√≥ng" (broadcast) b·∫£ng n√†y ƒë·∫øn t·∫•t c·∫£ c√°c m√°y t√≠nh kh√°c trong c·ª•m (cluster).
* **(6) BroadcastHashJoin ... BuildRight:** * `BuildRight` nghƒ©a l√† b·∫£ng b√™n ph·∫£i (`customers`) ƒë∆∞·ª£c d√πng ƒë·ªÉ t·∫°o Hash Table trong b·ªô nh·ªõ.
* **L·ª£i √≠ch:** Kh√¥ng c√≥ node "Exchange" (Shuffle) cho b·∫£ng `orders`. Vi·ªác Join di·ªÖn ra ngay t·∫°i ch·ªó (local) tr√™n m·ªói m√°y.



##### 3. C∆° ch·∫ø Aggregation 2 b∆∞·ªõc (T·ªëi ∆∞u h√≥a Shuffle)

T·∫°i node **(8), (9), (10)**:

* **(8) HashAggregate (partial):** Spark t√≠nh to√°n "nh√°p" tr∆∞·ªõc (t√≠nh t·ªïng v√† ƒë·∫øm t·∫°m th·ªùi) tr√™n t·ª´ng m√°y.
* **(9) Exchange (hashpartitioning):** Sau khi c√≥ k·∫øt qu·∫£ nh√°p, Spark m·ªõi Shuffle d·ªØ li·ªáu v·ªÅ c√πng m·ªôt ch·ªó d·ª±a tr√™n c√°c Key (`dt`, `country`, `segment`).
* **(10) HashAggregate (final):** T√≠nh to√°n k·∫øt qu·∫£ cu·ªëi c√πng t·ª´ c√°c b·∫£n nh√°p.
* **T·∫°i sao l√†m v·∫≠y?** ƒê·ªÉ gi·∫£m l∆∞·ª£ng d·ªØ li·ªáu bay qua m·∫°ng (Network IO). Thay v√¨ g·ª≠i 1 tri·ªáu d√≤ng ƒë·ªÉ group, n√≥ ch·ªâ g·ª≠i v√†i ngh√¨n d√≤ng k·∫øt qu·∫£ t·∫°m th·ªùi.

---

#### üìä S∆° ƒë·ªì lu·ªìng th·ª±c thi (D·ª±a tr√™n log th·ª±c t·∫ø)

```mermaid
graph TD
    subgraph Scan_Phase
        A[Scan Orders - Node 1] -- "Partition Pruning (dt=2026-01-10)" --> B[Join]
        C[Scan Customers - Node 2] --> D[Broadcast - Node 5]
        D --> B
    end

    subgraph Join_and_Agg
        B -- "BroadcastHashJoin - Node 6" --> E[Partial Aggregate - Node 8]
        E -- "Shuffle - Node 9" --> F[Final Aggregate - Node 10]
    end

    subgraph Output
        F --> G[Sort - Node 12]
        G --> H[Result]
    end

    style A fill:#d4f1f4,stroke:#05445e
    style D fill:#f9d5e5,stroke:#ee4540
    style F fill:#e1f8dc,stroke:#283618

```

---

#### üí° Di·ªÖn gi·∫£i Code chi ti·∫øt (D√†nh cho b·∫£n tin c·ªßa b·∫°n)

Trong file Python b·∫°n ch·∫°y, c√≥ c√°c h√†m Spark t∆∞∆°ng ·ª©ng tr·ª±c ti·∫øp v·ªõi c√°c node trong `explain`:

| Code Python (PySpark) | Node trong Plan | Gi·∫£i th√≠ch k·ªπ thu·∫≠t |
| --- | --- | --- |
| `.where(col("dt") == "2026-01-10")` | **(1) PartitionFilters** | Lo·∫°i b·ªè d·ªØ li·ªáu th·ª´a ngay t·ª´ l·ªõp v·∫≠t l√Ω (·ªï ƒëƒ©a). |
| `.join(customers, ...)` | **(6) BroadcastHashJoin** | K·∫øt h·ª£p 2 b·∫£ng b·∫±ng b·ªô nh·ªõ RAM, tr√°nh Shuffle b·∫£ng l·ªõn. |
| `.groupBy(...).agg(...)` | **(8), (9), (10)** | Th·ª±c hi·ªán gom nh√≥m qua 2 giai ƒëo·∫°n: Partial (t·∫°i ch·ªó) v√† Final (sau shuffle). |
| `.orderBy(...)` | **(11), (12)** | **RangePartitioning** v√† **Sort**. Spark chia v√πng d·ªØ li·ªáu ƒë·ªÉ s·∫Øp x·∫øp song song. |

---

#### ‚úÖ Nh·∫≠n x√©t k·∫øt qu·∫£

K·∫øt qu·∫£ c·ªßa b·∫°n tr·∫£ v·ªÅ `TH | AFFLUENT | MED | 16614 txns`. ƒêi·ªÅu n√†y cho th·∫•y d·ªØ li·ªáu gi·∫£ l·∫≠p ƒë√£ ch·∫°y t·ªët.

* S·ªë l∆∞·ª£ng `txns` kh√° l·ªõn cho m·ªôt ng√†y (`~16k` d√≤ng cho m·ªôt ph√¢n kh√∫c t·∫°i Th√°i Lan), ch·ª©ng t·ªè b·∫£ng Fact 2 tri·ªáu d√≤ng c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c ph√¢n b·ªï kh√° ƒë·ªÅu (ngo·∫°i tr·ª´ ph·∫ßn Skew `customer_id=1`).


---

## 4. Giai ƒëo·∫°n C: Query 2 ‚Äî √âp Shuffle Join (So s√°nh)

### M√£ ngu·ªìn: `lab3_1_q2_prune_shuffle_join.py`

```python
from __future__ import annotations

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, avg as _avg, count as _count

BASE = "data/silver_lab31"
FACT = f"{BASE}/orders_fact_dt"
DIM  = f"{BASE}/customers"

def spark(app: str) -> SparkSession:
    return (
        SparkSession.builder
        .appName(app)
        .config("spark.sql.shuffle.partitions", "50")
        .config("spark.sql.adaptive.enabled", "true")
        .config("spark.sql.autoBroadcastJoinThreshold", "-1")  # ‚ùå disable broadcast
        .getOrCreate()
    )

def main() -> None:
    s = spark("lab3_1_q2_prune_shuffle_join")

    o = s.read.parquet(FACT).alias("o")
    c = (
        s.read.parquet(DIM)
        .select(
            col("customer_id").alias("c_customer_id"),
            col("segment").alias("c_segment"),
            col("risk_tier").alias("c_risk_tier"),
        )
        .alias("c")
    )

    target_dt = "2026-01-10"

    q = (
        o.where(col("dt") == target_dt)
        .join(c, col("o.customer_id") == col("c.c_customer_id"), "left")
        .groupBy("dt", "country", "c_segment", "c_risk_tier")
        .agg(
            _count("*").alias("txns"),
            _sum("amount").alias("total_amount"),
            _avg("amount").alias("avg_amount"),
        )
    )

    q.explain("formatted")
    q.show(20, truncate=False)

    s.stop()

if __name__ == "__main__":
    main()

```

### Gi·∫£i th√≠ch c∆° ch·∫ø:

* **Shuffle Join (SortMergeJoin)**: Spark s·∫Ω bƒÉm (hash) `customer_id` c·ªßa c·∫£ 2 b·∫£ng v√† ƒë·∫©y c√°c d√≤ng c√≥ c√πng hash v·ªÅ c√πng m·ªôt Worker qua m·∫°ng.
* **Chi ph√≠**: T·ªën t√†i nguy√™n m·∫°ng (Network IO) v√† CPU ƒë·ªÉ s·∫Øp x·∫øp (Sort) d·ªØ li·ªáu tr∆∞·ªõc khi Join.

---

### K·∫øt q·ªßa query 2:

```code
== Physical Plan ==
AdaptiveSparkPlan (14)
+- HashAggregate (13)
   +- Exchange (12)
      +- HashAggregate (11)
         +- Project (10)
            +- SortMergeJoin LeftOuter (9)
               :- Sort (3)
               :  +- Exchange (2)
               :     +- Scan parquet  (1)
               +- Sort (8)
                  +- Exchange (7)
                     +- Project (6)
                        +- Filter (5)
                           +- Scan parquet  (4)


(1) Scan parquet 
Output [4]: [customer_id#1, amount#2, country#4, dt#7]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab31/orders_fact_dt]
PartitionFilters: [isnotnull(dt#7), (dt#7 = 2026-01-10)]
ReadSchema: struct<customer_id:string,amount:double,country:string>

(2) Exchange
Input [4]: [customer_id#1, amount#2, country#4, dt#7]
Arguments: hashpartitioning(customer_id#1, 50), ENSURE_REQUIREMENTS, [plan_id=28]

(3) Sort
Input [4]: [customer_id#1, amount#2, country#4, dt#7]
Arguments: [customer_id#1 ASC NULLS FIRST], false, 0

(4) Scan parquet 
Output [3]: [customer_id#8, segment#9, risk_tier#10]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab31/customers]
PushedFilters: [IsNotNull(customer_id)]
ReadSchema: struct<customer_id:string,segment:string,risk_tier:string>

(5) Filter
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Condition : isnotnull(customer_id#8)

(6) Project
Output [3]: [customer_id#8 AS c_customer_id#12, segment#9 AS c_segment#13, risk_tier#10 AS c_risk_tier#14]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(7) Exchange
Input [3]: [c_customer_id#12, c_segment#13, c_risk_tier#14]
Arguments: hashpartitioning(c_customer_id#12, 50), ENSURE_REQUIREMENTS, [plan_id=29]

(8) Sort
Input [3]: [c_customer_id#12, c_segment#13, c_risk_tier#14]
Arguments: [c_customer_id#12 ASC NULLS FIRST], false, 0

(9) SortMergeJoin
Left keys [1]: [customer_id#1]
Right keys [1]: [c_customer_id#12]
Join type: LeftOuter
Join condition: None

(10) Project
Output [5]: [amount#2, country#4, dt#7, c_segment#13, c_risk_tier#14]
Input [7]: [customer_id#1, amount#2, country#4, dt#7, c_customer_id#12, c_segment#13, c_risk_tier#14]

(11) HashAggregate
Input [5]: [amount#2, country#4, dt#7, c_segment#13, c_risk_tier#14]
Keys [4]: [dt#7, country#4, c_segment#13, c_risk_tier#14]
Functions [3]: [partial_count(1), partial_sum(amount#2), partial_avg(amount#2)]
Aggregate Attributes [4]: [count#34L, sum#35, sum#36, count#37L]
Results [8]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count#38L, sum#39, sum#40, count#41L]

(12) Exchange
Input [8]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count#38L, sum#39, sum#40, count#41L]
Arguments: hashpartitioning(dt#7, country#4, c_segment#13, c_risk_tier#14, 50), ENSURE_REQUIREMENTS, [plan_id=36]

(13) HashAggregate
Input [8]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count#38L, sum#39, sum#40, count#41L]
Keys [4]: [dt#7, country#4, c_segment#13, c_risk_tier#14]
Functions [3]: [count(1), sum(amount#2), avg(amount#2)]
Aggregate Attributes [3]: [count(1)#31L, sum(amount#2)#32, avg(amount#2)#33]
Results [7]: [dt#7, country#4, c_segment#13, c_risk_tier#14, count(1)#31L AS txns#17L, sum(amount#2)#32 AS total_amount#18, avg(amount#2)#33 AS avg_amount#19]

(14) AdaptiveSparkPlan
Output [7]: [dt#7, country#4, c_segment#13, c_risk_tier#14, txns#17L, total_amount#18, avg_amount#19]
Arguments: isFinalPlan=false


+----------+-------+---------+-----------+-----+--------------------+------------------+
|dt        |country|c_segment|c_risk_tier|txns |total_amount        |avg_amount        |
+----------+-------+---------+-----------+-----+--------------------+------------------+
|2026-01-10|TH     |MASS     |LOW        |33457|8.332187367829165E7 |2490.416764153739 |
|2026-01-10|TH     |SME      |HIGH       |16596|4.14064040705768E7  |2494.962886874958 |
|2026-01-10|TH     |AFFLUENT |MED        |16614|4.1581479857752666E7|2502.7976319822237|
+----------+-------+---------+-----------+-----+--------------------+------------------+
```


D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt s·ª± kh√°c bi·ªát c·ªßa **Query 2** so v·ªõi **Query 1**:

---

### üîç Ph√¢n t√≠ch "N·ªói ƒëau" c·ªßa Shuffle Join (SortMergeJoin)

Trong b·∫£n `explain` n√†y, b·∫°n h√£y ch√∫ √Ω v√†o c√°c node (2), (3), (7), (8) v√† (9). ƒê√¢y l√† n∆°i chi ph√≠ t√†i nguy√™n tƒÉng v·ªçt.

#### 1. Xu·∫•t hi·ªán Exchange (Shuffle) ·ªü c·∫£ hai nh√°nh

* **Node (2) v√† (7):** Spark th·ª±c hi·ªán `hashpartitioning(customer_id, 50)`.
* **V·∫•n ƒë·ªÅ:** Spark ph·∫£i bƒÉm (hash) t·ª´ng d√≤ng d·ªØ li·ªáu v√† g·ª≠i ch√∫ng qua m·∫°ng (Network) ƒë·ªÉ ƒë·∫£m b·∫£o c√°c d√≤ng c√≥ c√πng `customer_id` c·ªßa c·∫£ hai b·∫£ng s·∫Ω "h·ªôi qu√¢n" t·∫°i c√πng m·ªôt Worker.
* **H·∫≠u qu·∫£:** T·ªën bƒÉng th√¥ng m·∫°ng v√† I/O ƒëƒ©a n·∫øu d·ªØ li·ªáu tr√†n b·ªô nh·ªõ (spill to disk).

#### 2. Chi ph√≠ s·∫Øp x·∫øp (Sort)

* **Node (3) v√† (8):** Sau khi shuffle xong, Spark bu·ªôc ph·∫£i **S·∫Øp x·∫øp** d·ªØ li·ªáu theo `customer_id`.
* **L√Ω do:** Thu·∫≠t to√°n **SortMergeJoin** (Node 9) ch·ªâ ho·∫°t ƒë·ªông khi d·ªØ li·ªáu hai b√™n ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp th·ª© t·ª±. Vi·ªác s·∫Øp x·∫øp n√†y c·ª±c k·ª≥ t·ªën CPU.

#### 3. SortMergeJoin (Node 9)

* ƒê√¢y l√† m·ªôt thu·∫≠t to√°n join r·∫•t "tr√¢u b√≤", n√≥ c√≥ th·ªÉ join ƒë∆∞·ª£c c√°c b·∫£ng d·ªØ li·ªáu kh·ªïng l·ªì (v∆∞·ª£t qu√° RAM). Tuy nhi√™n, v√¨ b·∫°n ƒë√£ t·∫Øt Broadcast (`autoBroadcastJoinThreshold = -1`), Spark bu·ªôc ph·∫£i d√πng "xe t·∫£i" (Shuffle) ƒë·ªÉ ch·ªü d·ªØ li·ªáu thay v√¨ d√πng "m√°y bay" (Broadcast).

---

### üìä B·∫£ng so s√°nh "ƒê·∫Øt gi√°" gi·ªØa Q1 v√† Q2

D·ª±a tr√™n th·ª±c t·∫ø log b·∫°n v·ª´a g·ª≠i, ch√∫ng ta c√≥ b·∫£ng ƒë·ªëi chi·∫øu sau:

| ƒê·∫∑c ƒëi·ªÉm | Query 1 (Broadcast) | Query 2 (Shuffle) | ƒê√°nh gi√° |
| --- | --- | --- | --- |
| **Join Type** | `BroadcastHashJoin` | `SortMergeJoin` | Q1 nhanh h∆°n |
| **S·ªë l·∫ßn Shuffle (Exchange)** | 2 l·∫ßn (cho Aggregation & Sort) | **4 l·∫ßn** (2 cho Join + 2 cho Aggregation) | Q2 t·ªën Network g·∫•p ƒë√¥i |
| **Thao t√°c n·∫∑ng nh·∫•t** | Qu√©t d·ªØ li·ªáu (Scan) | **S·∫Øp x·∫øp (Sort)** | Q2 t·ªën CPU h∆°n |
| **T√≠nh ·ªïn ƒë·ªãnh** | D·ªÖ crash n·∫øu b·∫£ng dim qu√° l·ªõn so v·ªõi RAM | R·∫•t b·ªÅn b·ªâ v·ªõi d·ªØ li·ªáu c·ª±c l·ªõn | Q2 an to√†n cho Big Data |

---

### üí° Gi·∫£i th√≠ch Code Spark: T·∫°i sao k·∫øt qu·∫£ v·∫´n ƒë√∫ng?

D√π c√°ch th·ª±c thi kh√°c nhau, nh∆∞ng Spark v·∫´n gi·ªØ nguy√™n c√°c b∆∞·ªõc t·ªëi ∆∞u h√≥a kh√°c:

* **Partition Pruning (Node 1):** V·∫´n ho·∫°t ƒë·ªông! Spark v·∫´n ƒë·ªß th√¥ng minh ƒë·ªÉ ch·ªâ ƒë·ªçc folder `dt=2026-01-10`. ƒê√¢y l√† c·ª©u c√°nh duy nh·∫•t gi√∫p Q2 kh√¥ng b·ªã ch·∫≠m ƒëi qu√° nhi·ªÅu.
* **Partial Aggregation (Node 11):** V·∫´n gom nh√≥m nh√°p t·∫°i ch·ªó tr∆∞·ªõc khi g·ª≠i ƒëi (Node 12).

---

### üõ† S∆° ƒë·ªì Pipeline c·ªßa Query 2 (Shuffle Join)

```mermaid
graph TD
    subgraph Table_Orders
        A1[Scan Orders Node 1] --> A2[Exchange: Shuffle by ID Node 2]
        A2 --> A3[Sort ID Node 3]
    end

    subgraph Table_Customers
        B1[Scan Customers Node 4] --> B2[Exchange: Shuffle by ID Node 7]
        B2 --> B3[Sort ID Node 8]
    end

    A3 --> J[SortMergeJoin Node 9]
    B3 --> J

    J --> AG1[Partial Aggregate Node 11]
    AG1 --> EX[Exchange: Shuffle by Group Key Node 12]
    EX --> AG2[Final Aggregate Node 13]

    style A2 fill:#ff9999,stroke:#333
    style B2 fill:#ff9999,stroke:#333
    style A3 fill:#ffff99,stroke:#333
    style B3 fill:#ffff99,stroke:#333

```

---

### üöÄ Checklist suy ng·∫´m cho Data Engineer

N·∫øu ƒë√¢y l√† m·ªôt d·ª± √°n th·ª±c t·∫ø t·∫°i c√¥ng ty:

1. **Khi n√†o d√πng Q1?** Khi b·∫£ng Dimension (Customers) d∆∞·ªõi 10-100MB. Lu√¥n ∆∞u ti√™n v√¨ n√≥ tr√°nh ƒë∆∞·ª£c vi·ªác x√°o tr·ªôn d·ªØ li·ªáu b·∫£ng Fact.
2. **Khi n√†o d√πng Q2?** Khi b·∫£ng Dimension qu√° l·ªõn (v√≠ d·ª• b·∫£ng `customer_history` c√≥ 1 t·ª∑ d√≤ng). Khi ƒë√≥ Broadcast s·∫Ω g√¢y l·ªói `OutOfMemory`.
3. **T√¨nh tr·∫°ng Skew:** ·ªû Q2, n·∫øu `customer_id=1` chi·∫øm 25%, b·∫°n s·∫Ω th·∫•y m·ªôt Worker ch·∫°y c·ª±c l√¢u trong khi c√°c Worker kh√°c ƒë√£ xong. ƒê√≥ l√† l·ªói **Data Skew**.



---

## 5. So s√°nh K·∫ø ho·∫°ch th·ª±c thi (Physical Plan)

```mermaid
flowchart TD
  subgraph Q1[Q1: Prune + Broadcast Join]
    A1[Scan orders_fact_dt<br/>PartitionFilters: dt=2026-01-10] --> J1[BroadcastHashJoin]
    B1[Scan customers_dim] --> BE[BroadcastExchange] --> J1
    J1 --> AG1[HashAggregate] --> SH1[Exchange: Shuffle by group keys] --> AG2[Final Aggregate]
  end

  subgraph Q2[Q2: Prune + Shuffle Join]
    A2[Scan orders_fact_dt<br/>PartitionFilters: dt=...] --> E2a[Exchange: HashPartition customer_id] --> S2a[Sort]
    B2[Scan customers_dim] --> E2b[Exchange: HashPartition customer_id] --> S2b[Sort]
    S2a --> JM[SortMergeJoin] <-- S2b
    JM --> AG3[HashAggregate] --> SH2[Exchange] --> AG4[Final Aggregate]
  end

```

## 6. S∆° ƒë·ªì pipeline query 1 v√† 2:

```mermaid
flowchart TD
  subgraph Q1[Q1: Prune + Broadcast Join]
    A1[Scan orders_fact_dt<br/>PartitionFilters dt=...] --> J1[BroadcastHashJoin]
    B1[Scan customers_dim] --> BE[BroadcastExchange] --> J1
    J1 --> AG1[HashAggregate] --> SH1[Exchange shuffle by group keys] --> AG2[Final Aggregate]
  end

  subgraph Q2[Q2: Prune + Shuffle Join]
    A2[Scan orders_fact_dt<br/>PartitionFilters dt=...] --> E2a[Exchange hashpartition customer_id] --> S2a[Sort]
    B2[Scan customers_dim] --> E2b[Exchange hashpartition customer_id] --> S2b[Sort]
    S2a --> JM[SortMergeJoin] <-- S2b
    JM --> AG3[HashAggregate] --> SH2[Exchange] --> AG4[Final Aggregate]
  end
```

---

## 7. ƒê·ªçc explain("formatted"):

### 1) Pruning c√≥ x·∫£y ra kh√¥ng?

#### T√¨m d√≤ng ki·ªÉu:
##### ‚úÖ C√≥ pruning:
-	PartitionFilters: [ ... (dt = 2026-01-10) ]
##### ‚ùå Kh√¥ng pruning:
-	kh√¥ng c√≥ PartitionFilters
-	ho·∫∑c filter xu·∫•t hi·ªán sau scan (Filter node)

### 2) Join type l√† g√¨?

#### B·∫°n s·∫Ω th·∫•y 1 trong 2:

##### Broadcast join
-	BroadcastExchange
-	BroadcastHashJoin ... BuildRight

##### Shuffle join (SortMergeJoin)
-	Exchange hashpartitioning(...) ·ªü c·∫£ 2 nh√°nh
-	Sort
-	SortMergeJoin

### 3) Shuffle ·ªü ƒë√¢u l√† ‚Äúb·∫Øt bu·ªôc‚Äù?
#####	Aggregate g·∫ßn nh∆∞ lu√¥n c√≥:
-	HashAggregate ‚Üí Exchange ‚Üí HashAggregate
-	C√°i n√†y l√† shuffle ‚Äúƒë√∫ng vi·ªác‚Äù, kh√¥ng ph·∫£i do join.

---

## 8. H∆∞·ªõng d·∫´n ch·∫°y Lab

Th·ª±c hi·ªán theo ƒë√∫ng th·ª© t·ª± sau trong terminal:

1. **T·∫°o d·ªØ li·ªáu**:
```bash
python spark/lab/lab3_1_prepare_fact_partitioned.py

```


2. **Ch·∫°y Query t·ªëi ∆∞u (Broadcast)**:
```bash
python spark/lab/lab3_1_q1_prune_broadcast.py

```


3. **Ch·∫°y Query Shuffle (ƒê·ªÉ so s√°nh)**:
```bash
python spark/lab/lab3_1_q2_prune_shuffle_join.py

```



---

## 9. Checklist ƒë√°nh gi√° k·∫øt qu·∫£

Sau khi ch·∫°y, T√¢n h√£y ki·ªÉm tra log `explain("formatted")` v√† x√°c nh·∫≠n:

* [ ] **PartitionFilters**: C√≥ xu·∫•t hi·ªán `(dt = '2026-01-10')` trong ph·∫ßn `Scan parquet`.
* [ ] **Join Type**: Q1 ph·∫£i l√† `BroadcastHashJoin`, Q2 ph·∫£i l√† `SortMergeJoin`.
* [ ] **Th·ªùi gian**: Q1 th∆∞·ªùng s·∫Ω nhanh h∆°n Q2 r√µ r·ªát d√π ch·∫°y local.

