# üöÄ LAB 3.2: Th·ª±c t·∫ø x·ª≠ l√Ω Skew Join v·ªõi AQE

> **M·ª•c ti√™u:** Gi·∫£ l·∫≠p t√¨nh hu·ªëng d·ªØ li·ªáu b·ªã l·ªách (Skew) c·ª±c n·∫∑ng t·∫°i kh√°ch h√†ng `customer_id='1'` v√† quan s√°t c√°ch **Adaptive Query Execution (AQE)** c·ªßa Spark 3.x/4.x t·ª± ƒë·ªông "c·ª©u" h·ªá th·ªëng m√† kh√¥ng c·∫ßn s·ª≠a code.

#### T·∫°o t√¨nh hu·ªëng th·ª±c t·∫ø:
-	orders_fact_dt r·∫•t l·ªõn, skew n·∫∑ng ·ªü customer_id='1'
-	join v·ªõi customers_dim (nh·ªè h∆°n) theo customer_id
-	sau join, group KPI theo dt, country, segment, risk_tier

#### Ta s·∫Ω so:
1.	AQE OFF ‚Üí th·∫•y join b·ªã k√©o b·ªüi 1 v√†i task (skew)
2.	AQE ON + skewJoin enabled ‚Üí Spark t·ª± ‚Äúsplit‚Äù partition b·ªã skew ƒë·ªÉ gi·∫£m straggler

---

## 0. Chu·∫©n b·ªã d·ªØ li·ªáu (Prepare Big Data)

B∆∞·ªõc n√†y t·∫°o ra 10 tri·ªáu d√≤ng giao d·ªãch (Fact) v√† 200 ngh√¨n kh√°ch h√†ng (Dim).

### File: `lab3_2a_prepare_big_data.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, expr, rand, floor, pmod, element_at, to_timestamp, to_date
)

# =========================
# CONFIG / PATHS
# =========================
BASE = "data/silver_lab32"
ORDERS_PATH = f"{BASE}/orders_fact_dt"
CUSTOMERS_PATH = f"{BASE}/customers_dim"

N_CUSTOMERS = 200_000        # S·ªë l∆∞·ª£ng kh√°ch h√†ng (Dimension)
N_ORDERS = 10_000_000        # S·ªë l∆∞·ª£ng ƒë∆°n h√†ng (Fact)
N_DAYS = 30                  # Chia d·ªØ li·ªáu trong 30 ng√†y

# T·ª∑ l·ªá Skew: 30% ƒë∆°n h√†ng s·∫Ω thu·ªôc v·ªÅ duy nh·∫•t customer_id = '1'
SKEW_RATIO = 0.30            

spark = (
    SparkSession.builder
    .appName("lab3_2_prepare_big_data")
    .config("spark.sql.shuffle.partitions", "50") # Gi·∫£m partition ƒë·ªÉ ph√π h·ª£p m√°y local
    .getOrCreate()
)

# =========================
# 1) CUSTOMERS DIM
# =========================
customers = (
    spark.range(0, N_CUSTOMERS)
    .select(
        # Chuy·ªÉn ID th√†nh String ƒë·ªÉ gi·ªëng th·ª±c t·∫ø h·ªá th·ªëng Core Banking
        (col("id") + 1).cast("string").alias("customer_id"),

        # S·ª≠ d·ª•ng element_at v√† pmod ƒë·ªÉ ph√¢n b·ªï ƒë·ªÅu c√°c thu·ªôc t√≠nh
        element_at(
            expr("array('MASS','AFFLUENT','SME','CORP')"),
            (pmod(col("id"), 4) + 1).cast("int")
        ).alias("segment"),

        element_at(
            expr("array('LOW','MED','HIGH')"),
            (pmod(col("id"), 3) + 1).cast("int")
        ).alias("risk_tier"),

        element_at(
            expr("array('VN','SG','TH','ID','MY','PH')"),
            (pmod(col("id"), 6) + 1).cast("int")
        ).alias("home_country"),

        expr("CASE WHEN pmod(id, 10)=0 THEN 'INACTIVE' ELSE 'ACTIVE' END").alias("cust_status")
    )
)

customers.write.mode("overwrite").parquet(CUSTOMERS_PATH)
print("‚úî customers_dim written")

# =========================
# 2) ORDERS FACT (SKEW + PARTITION dt)
# =========================
orders = (
    spark.range(0, N_ORDERS)
    .select(
        (col("id") + 1).cast("string").alias("order_id"),

        # T·∫†O SKEW CHI·∫æN THU·∫¨T:
        # N·∫øu s·ªë ng·∫´u nhi√™n < 0.30 th√¨ g√°n ID = '1', c√≤n l·∫°i tr·∫£i ƒë·ªÅu.
        expr(f"""
            CASE
              WHEN rand(7) < {SKEW_RATIO} THEN '1'
              ELSE cast(pmod(id * 17, {N_CUSTOMERS - 2}) + 2 as string)
            END
        """).alias("customer_id"),

        (rand(11) * 5000).alias("amount"),

        element_at(
            expr("array('VN','SG','TH','ID','MY','PH')"),
            (pmod(col("id"), 6) + 1).cast("int")
        ).alias("country"),

        element_at(
            expr("array('POS','ECOM','ATM','QR')"),
            (pmod(col("id"), 4) + 1).cast("int")
        ).alias("channel"),

        element_at(
            expr("array('SUCCESS','FAILED','REVERSED')"),
            (pmod(col("id"), 3) + 1).cast("int")
        ).alias("status"),

        # T√≠nh to√°n th·ªùi gian l√πi d·∫ßn 30 ng√†y t·ª´ m·ªëc 2026-01-12
        expr(f"timestamp('2026-01-12 10:00:00') - make_interval(0,0,0, cast(pmod(id,{N_DAYS}) as int), 0,0,0)")
        .alias("order_ts")
    )
    .withColumn("dt", to_date(col("order_ts")))
)

# Ghi d·ªØ li·ªáu v√† Partition theo c·ªôt 'dt' (Ng√†y)
orders.write.mode("overwrite").partitionBy("dt").parquet(ORDERS_PATH)
print("‚úî orders_fact_dt written")

spark.stop()

```

### Gi·∫£i th√≠ch code:

*	`spark.range(0, N)` t·∫°o DF r·∫•t nhanh (kh√¥ng c·∫ßn data file)
*	`expr(""" SQL """)` d√πng SQL expression ngay trong DataFrame API (r·∫•t ph·ªï bi·∫øn)
*	`rand(seed)` t·∫°o random ·ªïn ƒë·ªãnh theo seed (gi√∫p b·∫°n reproducible)
*	`pmod(a,b)` l√† modulo ‚Äúan to√†n‚Äù (kh√¥ng √¢m)
*	`element_at(array, idx)` l·∫•y ph·∫ßn t·ª≠ theo index (Spark index t·ª´ 1)
*	`make_interval(...)` gi√∫p tr·ª´ ng√†y m√† kh√¥ng c·∫ßn interval keyword (tr√°nh l·ªói b·∫°n g·∫∑p)


---

## 1. Ki·ªÉm tra ƒë·ªô l·ªách (Check Skew)

Tr∆∞·ªõc khi Join, ta c·∫ßn x√°c nh·∫≠n xem d·ªØ li·ªáu c√≥ th·ª±c s·ª± b·ªã l·ªách hay kh√¥ng.

### File: `lab3_2b_check_skew.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count as _count

ORDERS_PATH = "data/silver_lab32/orders_fact_dt"

spark = (
    SparkSession.builder
    .appName("lab3_2_check_skew")
    .config("spark.sql.shuffle.partitions", "50")
    .getOrCreate()
)

df = spark.read.parquet(ORDERS_PATH).select("customer_id")

# Group theo ID kh√°ch h√†ng v√† ƒë·∫øm s·ªë l∆∞·ª£ng ƒë∆°n h√†ng
res = (df.groupBy("customer_id")
         .agg(_count("*").alias("cnt"))
         .orderBy(col("cnt").desc())
         .limit(20))

res.explain("formatted") # Xem k·∫ø ho·∫°ch th·ª±c thi d·∫°ng format
res.show(truncate=False)

spark.stop()

```

**K·∫øt qu·∫£ mong ƒë·ª£i:** B·∫°n s·∫Ω th·∫•y `customer_id=1` c√≥ s·ªë l∆∞·ª£ng (`cnt`) v∆∞·ª£t tr·ªôi ho√†n to√†n so v·ªõi c√°c ID kh√°c.

```code
== Physical Plan ==
AdaptiveSparkPlan (7)
+- TakeOrderedAndProject (6)
   +- HashAggregate (5)
      +- Exchange (4)
         +- HashAggregate (3)
            +- Project (2)
               +- Scan parquet  (1)


(1) Scan parquet 
Output [2]: [customer_id#1, dt#7]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/orders_fact_dt]
ReadSchema: struct<customer_id:string>

(2) Project
Output [1]: [customer_id#1]
Input [2]: [customer_id#1, dt#7]

(3) HashAggregate
Input [1]: [customer_id#1]
Keys [1]: [customer_id#1]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#12L]
Results [2]: [customer_id#1, count#13L]

(4) Exchange
Input [2]: [customer_id#1, count#13L]
Arguments: hashpartitioning(customer_id#1, 50), ENSURE_REQUIREMENTS, [plan_id=16]

(5) HashAggregate
Input [2]: [customer_id#1, count#13L]
Keys [1]: [customer_id#1]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#11L]
Results [2]: [customer_id#1, count(1)#11L AS cnt#9L]

(6) TakeOrderedAndProject
Input [2]: [customer_id#1, cnt#9L]
Arguments: 20, [cnt#9L DESC NULLS LAST], [customer_id#1, cnt#9L]

(7) AdaptiveSparkPlan
Output [2]: [customer_id#1, cnt#9L]
Arguments: isFinalPlan=false


+-----------+-------+                                                           
|customer_id|cnt    |
+-----------+-------+
|1          |3000825|
|148288     |48     |
|132241     |48     |
|154761     |47     |
|75517      |47     |
|128199     |47     |
|192685     |47     |
|97419      |47     |
|55479      |47     |
|103271     |46     |
|49056      |46     |
|18342      |46     |
|63258      |46     |
|11967      |46     |
|36592      |46     |
|119260     |46     |
|80077      |46     |
|78156      |46     |
|162671     |46     |
|129109     |46     |
+-----------+-------+
```

### Gi·∫£i th√≠ch chi ti·∫øt:

#### 1. Gi·∫£i th√≠ch k√Ω hi·ªáu `customer_id#1`, `dt#7` v√† `#13L`

ƒê√¢y l√† c√°ch Spark ƒë√°nh d·∫•u ƒë·ªãnh danh cho c√°c c·ªôt (Internal Column ID) ƒë·ªÉ qu·∫£n l√Ω trong su·ªët qu√° tr√¨nh t·ªëi ∆∞u h√≥a truy v·∫•n.

* **`#1`, `#7`, `#13**`: L√† s·ªë ID duy nh·∫•t m√† Spark g√°n cho c·ªôt ƒë√≥. D√π b·∫°n c√≥ ƒë·ªïi t√™n c·ªôt (alias) nhi·ªÅu l·∫ßn, Spark v·∫´n d√πng s·ªë ID n√†y ƒë·ªÉ bi·∫øt ƒë√≥ l√† c√πng m·ªôt d·ªØ li·ªáu g·ªëc.
* **`L` (trong `13L`)**: K√Ω hi·ªáu ki·ªÉu d·ªØ li·ªáu **Long** (S·ªë nguy√™n 64-bit). C√°c k√Ω hi·ªáu kh√°c b·∫°n c√≥ th·ªÉ g·∫∑p: `S` (String), `I` (Integer), `D` (Double).
* **`Input [2]: [customer_id#1, dt#7]`**: Nghƒ©a l√† ·ªü b∆∞·ªõc Scan (ƒë·ªçc file), Spark l·∫•y ra 2 c·ªôt t·ª´ file Parquet: `customer_id` (ƒë∆∞·ª£c g√°n ID 1) v√† `dt` (ƒë∆∞·ª£c g√°n ID 7).

---

#### 2. Gi·∫£i th√≠ch c√°c b∆∞·ªõc Aggregate (Gom nh√≥m)

Spark th·ª±c hi·ªán gom nh√≥m qua 2 giai ƒëo·∫°n ƒë·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng:

##### B∆∞·ªõc (3): HashAggregate (Partial)

* **`Functions [1]: [partial_count(1)]`**: ƒê√¢y l√† b∆∞·ªõc ƒë·∫øm c·ª•c b·ªô t·∫°i t·ª´ng m√°y (Worker). M·ªói m√°y s·∫Ω ƒë·∫øm xem c√°c kh√°ch h√†ng m√¨nh ƒëang gi·ªØ xu·∫•t hi·ªán bao nhi√™u l·∫ßn.
* **`Results [2]: [customer_id#1, count#13L]`**: K·∫øt qu·∫£ ƒë·∫ßu ra c·ªßa b∆∞·ªõc n√†y g·ªìm m√£ kh√°ch h√†ng v√† m·ªôt con s·ªë t·∫°m th·ªùi (`count#13L`).
* **Nguy√™n t·∫Øc**: Thay v√¨ g·ª≠i 10 tri·ªáu d√≤ng qua m·∫°ng, Spark gom ch√∫ng l·∫°i th√†nh c√°c nh√≥m nh·ªè ngay t·∫°i ngu·ªìn.

##### B∆∞·ªõc (4): Exchange (Shuffle)

* **`hashpartitioning(customer_id#1, 50)`**: Spark bƒÉm (hash) ID kh√°ch h√†ng v√† chia v√†o 50 "gi·ªè" (partition).
* **T·∫°i sao l√† 50?**: V√¨ b·∫°n ƒë√£ c·∫•u h√¨nh `.config("spark.sql.shuffle.partitions", "50")`.

---

#### 3. Tham s·ªë n√†o cho th·∫•y Skew (L·ªách d·ªØ li·ªáu)?

**L∆∞u √Ω quan tr·ªçng:** Trong vƒÉn b·∫£n "Explain" thu·∫ßn t√∫y, Spark **kh√¥ng** ghi ch·ªØ "SKEW" m·ªôt c√°ch tr·ª±c ti·∫øp ·ªü b∆∞·ªõc Exchange. B·∫°n ph·∫£i nh·∫≠n di·ªán Skew qua 2 d·∫•u hi·ªáu:

##### D·∫•u hi·ªáu 1: K·∫øt qu·∫£ Output (R√µ r√†ng nh·∫•t)

Nh√¨n v√†o b·∫£ng k·∫øt qu·∫£ b·∫°n g·ª≠i:

* `customer_id = 1` c√≥ **3,000,825** d√≤ng.
* C√°c kh√°ch h√†ng kh√°c ch·ªâ c√≥ kho·∫£ng **46 - 48** d√≤ng.
* **ƒê·ªô l·ªách:** G·∫•p kho·∫£ng **65,000 l·∫ßn**. ƒê√¢y l√† minh ch·ª©ng h√πng h·ªìn cho Data Skew.

#### D·∫•u hi·ªáu 2: Trong Physical Plan (B∆∞·ªõc s·ªë 7)

* **`(7) AdaptiveSparkPlan ... isFinalPlan=false`**:
Th√¥ng s·ªë `isFinalPlan=false` k·∫øt h·ª£p v·ªõi `AdaptiveSparkPlan` cho th·∫•y Spark ƒëang ch·∫°y ·ªü ch·∫ø ƒë·ªô **AQE (Adaptive Query Execution)**.
Khi b·∫°n th·ª±c hi·ªán l·ªánh Join sau n√†y, AQE s·∫Ω nh√¨n v√†o con s·ªë `3,000,825` kia ·ªü b∆∞·ªõc Runtime. Khi ƒë√≥, ·ªü Physical Plan c·ªßa l·ªánh Join, b·∫°n s·∫Ω th·∫•y xu·∫•t hi·ªán th√™m c√°c node nh∆∞:
`CustomShuffleReader` ho·∫∑c `CoalescedShuffleReader`.

---

#### 4. T√≥m t·∫Øt nguy√™n t·∫Øc ƒë·ªçc Physical Plan

1. **ƒê·ªçc t·ª´ d∆∞·ªõi l√™n tr√™n**: D·ªØ li·ªáu ƒëi t·ª´ Scan (1) ‚Üí Gom nh√≥m s∆° b·ªô (3) ‚Üí G·ª≠i qua m·∫°ng (4) ‚Üí T·ªïng h·ª£p cu·ªëi (5) ‚Üí S·∫Øp x·∫øp & L·∫•y k·∫øt qu·∫£ (6).
2. **Nh√¨n v√†o d·∫•u `#**`: ƒê·ªÉ b√°m s√°t m·ªôt c·ªôt d·ªØ li·ªáu qua nhi·ªÅu t·∫ßng bi·∫øn ƒë·ªïi.
3. **Nh√¨n v√†o `Exchange**`: ƒê√¢y lu√¥n l√† "n√∫t th·∫Øt c·ªï chai". N·∫øu b·∫°n th·∫•y `hashpartitioning` m√† d·ªØ li·ªáu ƒë·∫ßu ra l·∫°i l·ªách nh∆∞ b·∫£ng tr√™n, ch·∫Øc ch·∫Øn Task x·ª≠ l√Ω partition ch·ª©a `customer_id=1` s·∫Ω b·ªã **Spill Disk** ho·∫∑c ch·∫°y c·ª±c l√¢u.

---

### Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc:


#### B∆∞·ªõc (1): Scan parquet

ƒê√¢y l√† b∆∞·ªõc ƒë·ªçc d·ªØ li·ªáu t·ª´ ·ªï c·ª©ng v√†o b·ªô nh·ªõ RAM.

* **`Output [2]: [customer_id#1, dt#7]`**: Spark ch·ªâ l·∫•y ƒë√∫ng 2 c·ªôt c·∫ßn thi·∫øt ƒë·ªÉ t·ªëi ∆∞u b·ªô nh·ªõ. `#1` v√† `#7` l√† ƒë·ªãnh danh c·ªôt su·ªët v√≤ng ƒë·ªùi query n√†y.
* **`Batched: true`**: Spark ƒë·ªçc d·ªØ li·ªáu theo t·ª´ng kh·ªëi (Batch) thay v√¨ t·ª´ng d√≤ng ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô.
* **`ReadSchema`**: Cho b·∫°n bi·∫øt c·∫•u tr√∫c c·ªôt ƒëang ƒë·ªçc l√† `string`.

#### B∆∞·ªõc (2): Project (Chi·∫øu)

B∆∞·ªõc n√†y gi·ªëng nh∆∞ l·ªánh `SELECT` trong SQL.

* **`Input [2]: [customer_id#1, dt#7]`**
* **`Output [1]: [customer_id#1]`**
* **T·∫°i sao m·∫•t c·ªôt `dt#7`?**: V√¨ sau b∆∞·ªõc Scan, Spark nh·∫≠n ra c√°c b∆∞·ªõc ti·∫øp theo (Aggregate) kh√¥ng c·∫ßn d√πng ƒë·∫øn c·ªôt `dt` n·ªØa, n√™n n√≥ lo·∫°i b·ªè lu√¥n ƒë·ªÉ ti·∫øt ki·ªám RAM.

#### B∆∞·ªõc (3): HashAggregate (Giai ƒëo·∫°n Partial - C·ª•c b·ªô)

ƒê√¢y l√† giai ƒëo·∫°n "Ti·ªÅn t·ªïng h·ª£p" di·ªÖn ra ngay t·∫°i m·ªói Task (Worker).

* **`Keys [1]: [customer_id#1]`**: Gom nh√≥m theo c·ªôt kh√°ch h√†ng.
* **`Functions [1]: [partial_count(1)]`**: M·ªói m√°y t·ª± ƒë·∫øm s·ªë d√≤ng c·ªßa c√°c kh√°ch h√†ng m√† n√≥ ƒëang gi·ªØ.
* **`count#12L` & `count#13L**`: `L` nghƒ©a l√† **Long** (s·ªë nguy√™n l·ªõn). Spark g√°n ID m·ªõi cho c·ªôt k·∫øt qu·∫£ c·ªßa h√†m ƒë·∫øm.
* **M·ª•c ƒë√≠ch**: N·∫øu m√°y A c√≥ 1 tri·ªáu d√≤ng c·ªßa `customer_id=1`, n√≥ s·∫Ω r√∫t g·ªçn th√†nh ƒë√∫ng 1 d√≤ng: `(1, 1000000)` tr∆∞·ªõc khi g·ª≠i qua m·∫°ng.

#### B∆∞·ªõc (4): Exchange (X√°o tr·ªôn d·ªØ li·ªáu - Shuffle)

ƒê√¢y l√† b∆∞·ªõc **nguy hi·ªÉm nh·∫•t** v√† t·ªën t√†i nguy√™n nh·∫•t.

* **`Arguments: hashpartitioning(customer_id#1, 50)`**: Spark d√πng thu·∫≠t to√°n Hash ƒë·ªÉ quy·∫øt ƒë·ªãnh: "D√≤ng d·ªØ li·ªáu n√†y s·∫Ω bay sang m√°y n√†o?".
* V√≠ d·ª•: `hash('1') % 50 = 10`. V·∫≠y t·∫•t c·∫£ d·ªØ li·ªáu c·ªßa kh√°ch h√†ng `1` tr√™n to√†n b·ªô c√°c m√°y Worker s·∫Ω b·ªã √©p d·ªìn v·ªÅ **Partition s·ªë 10**.


* **`ENSURE_REQUIREMENTS`**: ƒê·∫£m b·∫£o r·∫±ng d·ªØ li·ªáu c√≥ c√πng Key ph·∫£i n·∫±m tr√™n c√πng m·ªôt m√°y th√¨ m·ªõi `Group By` ch√≠nh x√°c ƒë∆∞·ª£c.

#### B∆∞·ªõc (5): HashAggregate (Giai ƒëo·∫°n Final - T·ªïng h·ª£p cu·ªëi)

Di·ªÖn ra sau khi Shuffle xong.

* **Logic**: M√°y nh·∫≠n ƒë∆∞·ª£c Partition s·ªë 10 s·∫Ω c·ªông d·ªìn t·∫•t c·∫£ c√°c `partial_count` t·ª´ c√°c m√°y kh√°c g·ª≠i t·ªõi.
* **`Results [2]: [customer_id#1, cnt#9L]`**: Tr·∫£ v·ªÅ k·∫øt qu·∫£ cu·ªëi c√πng: Kh√°ch h√†ng n√†o c√≥ t·ªïng bao nhi√™u ƒë∆°n h√†ng.

#### B∆∞·ªõc (6): TakeOrderedAndProject

T∆∞∆°ng ·ª©ng v·ªõi l·ªánh `ORDER BY cnt DESC LIMIT 20`.

* **`Arguments: 20, [cnt#9L DESC NULLS LAST]`**: L·∫•y 20 d√≤ng ƒë·∫ßu ti√™n c√≥ `cnt` cao nh·∫•t.
* B∆∞·ªõc n√†y th∆∞·ªùng ch·∫°y tr√™n Driver v√¨ d·ªØ li·ªáu l√∫c n√†y ƒë√£ r·∫•t nh·ªè (ch·ªâ c√≤n 20 d√≤ng).

#### B∆∞·ªõc (7): AdaptiveSparkPlan (AQE)

* **`isFinalPlan=false`**: ƒê√¢y l√† chi ti·∫øt c·ª±c k·ª≥ ƒë·∫Øt gi√°. N√≥ c√≥ nghƒ©a l√†: "ƒê√¢y m·ªõi ch·ªâ l√† k·∫ø ho·∫°ch d·ª± ki·∫øn".
* **C∆° ch·∫ø**: V√¨ b·∫°n b·∫≠t AQE, Spark s·∫Ω ch·∫°y c√°c b∆∞·ªõc tr√™n, sau ƒë√≥ n√≥ d·ª´ng l·∫°i nh√¨n v√†o k·∫øt qu·∫£ ·ªü B∆∞·ªõc (5).
* **Ph√°t hi·ªán Skew**: N√≥ th·∫•y `customer_id=1` c√≥ t·∫≠n **3,000,825** d√≤ng trong khi trung b√¨nh ch·ªâ c√≥ **46**.
* **H√†nh ƒë·ªông**: N·∫øu b∆∞·ªõc ti·∫øp theo l√† m·ªôt l·ªánh JOIN, Spark s·∫Ω t·ª± ƒë·ªông ƒë·ªïi k·∫ø ho·∫°ch (Final Plan) ƒë·ªÉ ch·∫ª nh·ªè partition c·ªßa kh√°ch h√†ng `1` ra.

---

#### üîç Tham s·ªë n√†o cho th·∫•y Skew "ng·∫ßm"?

Trong vƒÉn b·∫£n gi·∫£i th√≠ch n√†y, tham s·ªë cho th·∫•y Skew kh√¥ng n·∫±m ·ªü ch·ªØ vi·∫øt, m√† n·∫±m ·ªü **th·ªëng k√™ d·ªØ li·ªáu (Statistics)** m√† Spark thu th·∫≠p ƒë∆∞·ª£c gi·ªØa ch·ª´ng:

1. **·ªû b∆∞·ªõc (4) Exchange**: Spark thu th·∫≠p `MapOutputStatistics`.
2. N·∫øu b·∫°n xem tr√™n Spark UI ·ªü tab **SQL**, click v√†o node Exchange n√†y, b·∫°n s·∫Ω th·∫•y k√≠ch th∆∞·ªõc c√°c Partition. N·∫øu 49 Partition c·ª° **KB**, c√≤n 1 Partition c·ª° **MB/GB**, ƒë√≥ ch√≠nh l√† Skew.
3. **K·∫øt qu·∫£ `cnt = 3,000,825**`: ƒê√¢y ch√≠nh l√† b·∫±ng ch·ª©ng x√°c th·ª±c nh·∫•t. N√≥ cho bi·∫øt 1 Task ƒëang ph·∫£i x·ª≠ l√Ω kh·ªëi l∆∞·ª£ng c√¥ng vi·ªác g·∫•p **65,000 l·∫ßn** c√°c Task kh√°c.

---

#### üí° M·∫πo ƒë·ªçc Plan:

* Nh√¨n v√†o **`#s·ªë`** ƒë·ªÉ kh√¥ng b·ªã l·∫°c khi Spark ƒë·ªïi t√™n c·ªôt.
* C·ª© th·∫•y **`Exchange`** l√† bi·∫øt c√≥ d·ªØ li·ªáu bay qua m·∫°ng (Shuffle).
* C·ª© th·∫•y **`HashAggregate`** l√† bi·∫øt ƒëang c√≥ gom nh√≥m (Group By).


---

## 2. So s√°nh AQE OFF vs AQE ON

### 2.1 K·ªãch b·∫£n 1: AQE OFF (Th·∫•y v·∫•n ƒë·ªÅ)

Khi t·∫Øt AQE, Spark s·∫Ω b·ªã "k·∫πt" ·ªü c√°c task x·ª≠ l√Ω `customer_id=1`.

### File: `lab3_2c_join_kpi_aqe_off.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, avg as _avg, count as _count

ORDERS_PATH = "data/silver_lab32/orders_fact_dt"
CUSTOMERS_PATH = "data/silver_lab32/customers_dim"

spark = (
    SparkSession.builder
    .appName("lab3_2_join_kpi_aqe_off")
    .config("spark.sql.shuffle.partitions", "50")
    .config("spark.sql.adaptive.enabled", "false")        # T·∫ÆT AQE
    .config("spark.sql.autoBroadcastJoinThreshold", "-1") # T·∫Øt Broadcast ƒë·ªÉ √©p Shuffle Join
    .getOrCreate()
)

o = spark.read.parquet(ORDERS_PATH).alias("o")
c = spark.read.parquet(CUSTOMERS_PATH).select(
    col("customer_id").alias("c_customer_id"),
    col("segment").alias("c_segment"),
    col("risk_tier").alias("c_risk_tier")
).alias("c")

# Filter theo ng√†y ƒë·ªÉ k√≠ch ho·∫°t Partition Pruning
filtered = o.where(col("dt") == "2026-01-10")

j = filtered.join(c, col("o.customer_id") == col("c.c_customer_id"), "left")

# T√≠nh to√°n KPI
kpi = (j.groupBy("dt", "country", "c_segment", "c_risk_tier")
         .agg(
             _count("*").alias("txns"),
             _sum("amount").alias("total_amount"),
             _avg("amount").alias("avg_amount")
         )
         .orderBy("country", "c_segment", "c_risk_tier"))

kpi.show(50, truncate=False)
spark.stop()

```

#### K·∫øt qu·∫£:

```code
== Physical Plan ==
* Sort (17)
+- Exchange (16)
   +- * HashAggregate (15)
      +- Exchange (14)
         +- * HashAggregate (13)
            +- * Project (12)
               +- * SortMergeJoin LeftOuter (11)
                  :- * Sort (4)
                  :  +- Exchange (3)
                  :     +- * ColumnarToRow (2)
                  :        +- Scan parquet  (1)
                  +- * Sort (10)
                     +- Exchange (9)
                        +- * Project (8)
                           +- * Filter (7)
                              +- * ColumnarToRow (6)
                                 +- Scan parquet  (5)


(1) Scan parquet 
Output [4]: [customer_id#1, amount#2, country#3, dt#7]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/orders_fact_dt]
PartitionFilters: [isnotnull(dt#7), (dt#7 = 2026-01-10)]
ReadSchema: struct<customer_id:string,amount:double,country:string>

(2) ColumnarToRow [codegen id : 1]
Input [4]: [customer_id#1, amount#2, country#3, dt#7]

(3) Exchange
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: hashpartitioning(customer_id#1, 50), ENSURE_REQUIREMENTS, [plan_id=62]

(4) Sort [codegen id : 2]
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: [customer_id#1 ASC NULLS FIRST], false, 0

(5) Scan parquet 
Output [3]: [customer_id#8, segment#9, risk_tier#10]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/customers_dim]
PushedFilters: [IsNotNull(customer_id)]
ReadSchema: struct<customer_id:string,segment:string,risk_tier:string>

(6) ColumnarToRow [codegen id : 3]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(7) Filter [codegen id : 3]
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Condition : isnotnull(customer_id#8)

(8) Project [codegen id : 3]
Output [3]: [customer_id#8 AS c_customer_id#13, segment#9 AS c_segment#14, risk_tier#10 AS c_risk_tier#15]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(9) Exchange
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: hashpartitioning(c_customer_id#13, 50), ENSURE_REQUIREMENTS, [plan_id=72]

(10) Sort [codegen id : 4]
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: [c_customer_id#13 ASC NULLS FIRST], false, 0

(11) SortMergeJoin [codegen id : 5]
Left keys [1]: [customer_id#1]
Right keys [1]: [c_customer_id#13]
Join type: LeftOuter
Join condition: None

(12) Project [codegen id : 5]
Output [5]: [amount#2, country#3, dt#7, c_segment#14, c_risk_tier#15]
Input [7]: [customer_id#1, amount#2, country#3, dt#7, c_customer_id#13, c_segment#14, c_risk_tier#15]

(13) HashAggregate [codegen id : 5]
Input [5]: [amount#2, country#3, dt#7, c_segment#14, c_risk_tier#15]
Keys [4]: [dt#7, country#3, c_segment#14, c_risk_tier#15]
Functions [3]: [partial_count(1), partial_sum(amount#2), partial_avg(amount#2)]
Aggregate Attributes [4]: [count#35L, sum#36, sum#37, count#38L]
Results [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]

(14) Exchange
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Arguments: hashpartitioning(dt#7, country#3, c_segment#14, c_risk_tier#15, 50), ENSURE_REQUIREMENTS, [plan_id=81]

(15) HashAggregate [codegen id : 6]
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Keys [4]: [dt#7, country#3, c_segment#14, c_risk_tier#15]
Functions [3]: [count(1), sum(amount#2), avg(amount#2)]
Aggregate Attributes [3]: [count(1)#32L, sum(amount#2)#33, avg(amount#2)#34]
Results [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count(1)#32L AS txns#18L, sum(amount#2)#33 AS total_amount#19, avg(amount#2)#34 AS avg_amount#20]

(16) Exchange
Input [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, txns#18L, total_amount#19, avg_amount#20]
Arguments: rangepartitioning(country#3 ASC NULLS FIRST, c_segment#14 ASC NULLS FIRST, c_risk_tier#15 ASC NULLS FIRST, 50), ENSURE_REQUIREMENTS, [plan_id=85]

(17) Sort [codegen id : 7]
Input [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, txns#18L, total_amount#19, avg_amount#20]
Arguments: [country#3 ASC NULLS FIRST, c_segment#14 ASC NULLS FIRST, c_risk_tier#15 ASC NULLS FIRST], true, 0


+----------+-------+---------+-----------+------+--------------------+------------------+
|dt        |country|c_segment|c_risk_tier|txns  |total_amount        |avg_amount        |
+----------+-------+---------+-----------+------+--------------------+------------------+
|2026-01-10|TH     |AFFLUENT |HIGH       |116662|2.916220041084458E8 |2499.7171667590633|
|2026-01-10|TH     |CORP     |HIGH       |116766|2.919576008125071E8 |2500.3648391869815|
|2026-01-10|TH     |MASS     |LOW        |99906 |2.5031095210550228E8|2505.4646578333864|
+----------+-------+---------+-----------+------+--------------------+------------------+
```

---

#### Gi·∫£i th√≠ch:

B·∫£n plan n√†y l√† m·ªôt v√≠ d·ª• "kinh ƒëi·ªÉn" v·ªÅ c√°ch Spark x·ª≠ l√Ω m·ªôt b√†i to√°n Data Warehouse th·ª±c t·∫ø: **Filter -> Join -> Aggregate -> Sort**.

H√£y c√πng "gi·∫£i ph·∫´u" t·ª´ng giai ƒëo·∫°n ƒë·ªÉ th·∫•y c√°ch d·ªØ li·ªáu 10 tri·ªáu d√≤ng c·ªßa b·∫°n ƒë∆∞·ª£c lu√¢n chuy·ªÉn nh∆∞ th·∫ø n√†o:

---

##### Giai ƒëo·∫°n 1: Chu·∫©n b·ªã d·ªØ li·ªáu (Scan & Filter)

·ªû b∆∞·ªõc n√†y, Spark ƒë·ªçc song song hai b·∫£ng t·ª´ ·ªï c·ª©ng.

* **B·∫£ng Orders (1) & (2):**
* **`PartitionFilters`**: R·∫•t quan tr·ªçng! Spark th·ª±c hi·ªán **Partition Pruning**. Thay v√¨ qu√©t to√†n b·ªô 10 tri·ªáu d√≤ng, n√≥ ch·ªâ nh·∫£y v√†o ƒë√∫ng th∆∞ m·ª•c `dt=2026-01-10`. ƒêi·ªÅu n√†y gi√∫p gi·∫£m 90% l∆∞·ª£ng d·ªØ li·ªáu c·∫ßn ƒë·ªçc ngay t·ª´ ƒë·∫ßu.
* **`ColumnarToRow`**: Do d·ªØ li·ªáu Parquet l∆∞u d·∫°ng c·ªôt (columnar), Spark chuy·ªÉn n√≥ sang d·∫°ng d√≤ng (row) ƒë·ªÉ th·ª±c hi·ªán c√°c ph√©p t√≠nh to√°n ti·∫øp theo.


* **B·∫£ng Customers (5), (7) & (8):**
* **`Filter (isnotnull(customer_id#8))`**: Spark t·ª± ƒë·ªông th√™m b·ªô l·ªçc n√†y v√¨ b·∫°n th·ª±c hi·ªán Join tr√™n c·ªôt n√†y (Inner/Left Join key kh√¥ng th·ªÉ null).
* **`Project (8)`**: Th·ª±c hi·ªán Alias (ƒë·ªïi t√™n) c·ªôt th√†nh `c_customer_id`, `c_segment`... ƒë·ªÉ tr√°nh tr√πng t√™n v·ªõi b·∫£ng Orders.



---

##### Giai ƒëo·∫°n 2: Chu·∫©n b·ªã cho Shuffle Join (Exchange & Sort)

V√¨ b·∫°n t·∫Øt AQE v√† t·∫Øt Broadcast, Spark b·∫Øt bu·ªôc ph·∫£i d√πng **SortMergeJoin**. ƒê√¢y l√† l√∫c "n·ªói ƒëau" Skew b·∫Øt ƒë·∫ßu.

* **Exchange (3) & (9)**: Spark th·ª±c hi·ªán Shuffle. N√≥ bƒÉm (`hash`) `customer_id` c·ªßa c·∫£ 2 b·∫£ng ƒë·ªÉ ƒë∆∞a c√°c ID gi·ªëng nhau v·ªÅ c√πng m·ªôt m√°y.
* **V·∫•n ƒë·ªÅ Skew**: T·∫°i b∆∞·ªõc (3), 30% d·ªØ li·ªáu c·ªßa b·∫£ng Orders (kho·∫£ng h∆°n 300k d√≤ng cho 1 ng√†y) c√≥ chung `customer_id=1`. T·∫•t c·∫£ ch√∫ng s·∫Ω b·ªã d·ªìn v√†o **duy nh·∫•t 1 m√°y** x·ª≠ l√Ω.


* **Sort (4) & (10)**: Sau khi d·ªØ li·ªáu v·ªÅ chung m·ªôt m√°y, Spark s·∫Øp x·∫øp ch√∫ng theo th·ª© t·ª± tƒÉng d·∫ßn c·ªßa `customer_id`. Vi·ªác s·∫Øp x·∫øp gi√∫p Spark ch·ªâ c·∫ßn qu√©t qua d·ªØ li·ªáu m·ªôt l·∫ßn duy nh·∫•t ƒë·ªÉ t√¨m c√°c c·∫∑p kh·ªõp nhau.

---

##### Giai ƒëo·∫°n 3: Join & Aggregate s∆° b·ªô (11, 12 & 13)

* **SortMergeJoin (11)**: Spark th·ª±c hi·ªán kh·ªõp n·ªëi hai b·∫£ng. Task gi·ªØ `customer_id=1` s·∫Ω ph·∫£i th·ª±c hi·ªán h√†ng trƒÉm ng√†n ph√©p so kh·ªõp, trong khi c√°c Task kh√°c ch·ªâ l√†m v√†i ch·ª•c ph√©p. ƒê√¢y l√† l√Ω do Task ƒë√≥ tr·ªü th√†nh **Straggler** (k·∫ª k√©o ƒëu√¥i).
* **Project (12)**: Lo·∫°i b·ªè c√°c c·ªôt kh√¥ng c·∫ßn thi·∫øt sau Join, ch·ªâ gi·ªØ l·∫°i `amount`, `country`, `segment`...
* **HashAggregate (13) - Partial**: ƒê√¢y l√† b∆∞·ªõc t·ªëi ∆∞u. Spark t√≠nh t·ªïng v√† trung b√¨nh t·∫°m th·ªùi ngay t·∫°i ch·ªó.
* `partial_sum(amount#2)`: C·ªông d·ªìn s·ªë ti·ªÅn.
* `partial_avg(amount#2)`: L∆∞u l·∫°i c·∫£ t·ªïng v√† s·ªë l∆∞·ª£ng ƒë·ªÉ sau n√†y t√≠nh trung b√¨nh ch√≠nh x√°c.



---

##### Giai ƒëo·∫°n 4: T·ªïng h·ª£p cu·ªëi & S·∫Øp x·∫øp k·∫øt qu·∫£ (14, 15, 16 & 17)

* **Exchange (14)**: Shuffle l·∫ßn 2 ƒë·ªÉ gom c√°c d√≤ng c√≥ c√πng `dt, country, segment, risk_tier` v·ªÅ m·ªôt ch·ªó ƒë·ªÉ t√≠nh KPI cu·ªëi c√πng.
* **HashAggregate (15) - Final**: T√≠nh ra con s·ªë cu·ªëi c√πng b·∫°n th·∫•y trong b·∫£ng k·∫øt qu·∫£: `txns`, `total_amount`, `avg_amount`.
* **Exchange (16) & Sort (17)**: ƒê√¢y l√† k·∫øt qu·∫£ c·ªßa l·ªánh `.orderBy()`. Spark chia d·ªØ li·ªáu theo d·∫£i (`rangepartitioning`) d·ª±a tr√™n t√™n qu·ªëc gia ƒë·ªÉ s·∫Øp x·∫øp to√†n c·ª•c tr∆∞·ªõc khi hi·ªÉn th·ªã.

---

##### Ph√¢n t√≠ch k·∫øt qu·∫£ Output:

| dt | country | c_segment | c_risk_tier | txns | total_amount |
| --- | --- | --- | --- | --- | --- |
| 2026-01-10 | TH | AFFLUENT | HIGH | **116,662** | 2.91E8 |

**Nh·∫≠n x√©t v·ªÅ "Skew" qua con s·ªë:**

1. **S·ªë l∆∞·ª£ng Txns (116,662)**: Con s·ªë n√†y c·ª±c k·ª≥ l·ªõn cho m·ªôt ph√¢n kh√∫c c·ª• th·ªÉ trong m·ªôt ng√†y. ƒêi·ªÅu n√†y ph·∫£n √°nh ƒë√∫ng c√°i "Skew" b·∫°n ƒë√£ t·∫°o ra.
2. **K√Ω hi·ªáu Khoa h·ªçc (2.91E8)**: ƒê√¢y l√† , t·ª©c l√† kho·∫£ng 291 tri·ªáu ƒë∆°n v·ªã ti·ªÅn t·ªá. Do b·∫°n d√πng ki·ªÉu d·ªØ li·ªáu `Double`, khi con s·ªë qu√° l·ªõn Spark s·∫Ω hi·ªÉn th·ªã d·∫°ng E ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian.
3. **Chi ph√≠ hi·ªáu nƒÉng**: Trong Physical Plan, b∆∞·ªõc **(11) SortMergeJoin** ch√≠nh l√† n∆°i "ng·ªën" th·ªùi gian nh·∫•t. Task x·ª≠ l√Ω kh√°ch h√†ng ID=1 s·∫Ω ph·∫£i g√°nh 116,662 d√≤ng n√†y c·ªông v·ªõi 30% d·ªØ li·ªáu Skew c·ªßa ng√†y ƒë√≥.


---

### 2.2 K·ªãch b·∫£n 2: AQE ON (X·ª≠ l√Ω Skew Join)

ƒê√¢y l√† c√°ch d√πng trong Production. Spark t·ª± nh·∫≠n di·ªán partition b·ªã "ph√¨nh to" v√† x√© nh·ªè n√≥ ra.

### File: `lab3_2d_join_kpi_aqe_on_skew.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum as _sum, avg as _avg, count as _count

spark = (
    SparkSession.builder
    .appName("lab3_2_join_kpi_aqe_on_skew")
    .config("spark.sql.shuffle.partitions", "50")
    .config("spark.sql.adaptive.enabled", "true")           # B·∫¨T AQE
    .config("spark.sql.adaptive.skewJoin.enabled", "true")  # B·∫¨T T√çNH NƒÇNG SKEW JOIN
    .config("spark.sql.autoBroadcastJoinThreshold", "-1")    # √âp Shuffle Join ƒë·ªÉ th·∫•y t√°c d·ª•ng AQE
    .getOrCreate()
)

# ... (Ph·∫ßn code Join v√† Aggregation gi·ªëng h·ªát Version 1) ...

```

#### K·∫øt qu·∫£:

```code
== Physical Plan ==
AdaptiveSparkPlan (36)
+- == Final Plan ==
   ResultQueryStage (23), Statistics(sizeInBytes=8.0 EiB)
   +- TakeOrderedAndProject (22)
      +- * HashAggregate (21)
         +- AQEShuffleRead (20)
            +- ShuffleQueryStage (19), Statistics(sizeInBytes=1248.0 B, rowCount=13)
               +- Exchange (18)
                  +- * HashAggregate (17)
                     +- * Project (16)
                        +- * SortMergeJoin LeftOuter (15)
                           :- * Sort (6)
                           :  +- AQEShuffleRead (5)
                           :     +- ShuffleQueryStage (4), Statistics(sizeInBytes=17.8 MiB, rowCount=3.33E+5)
                           :        +- Exchange (3)
                           :           +- * ColumnarToRow (2)
                           :              +- Scan parquet  (1)
                           +- * Sort (14)
                              +- AQEShuffleRead (13)
                                 +- ShuffleQueryStage (12), Statistics(sizeInBytes=10.7 MiB, rowCount=2.00E+5)
                                    +- Exchange (11)
                                       +- * Project (10)
                                          +- * Filter (9)
                                             +- * ColumnarToRow (8)
                                                +- Scan parquet  (7)
+- == Initial Plan ==
   TakeOrderedAndProject (35)
   +- HashAggregate (34)
      +- Exchange (33)
         +- HashAggregate (32)
            +- Project (31)
               +- SortMergeJoin LeftOuter (30)
                  :- Sort (25)
                  :  +- Exchange (24)
                  :     +- Scan parquet  (1)
                  +- Sort (29)
                     +- Exchange (28)
                        +- Project (27)
                           +- Filter (26)
                              +- Scan parquet  (7)


(1) Scan parquet 
Output [4]: [customer_id#1, amount#2, country#3, dt#7]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/orders_fact_dt]
PartitionFilters: [isnotnull(dt#7), (dt#7 = 2026-01-10)]
ReadSchema: struct<customer_id:string,amount:double,country:string>

(2) ColumnarToRow [codegen id : 1]
Input [4]: [customer_id#1, amount#2, country#3, dt#7]

(3) Exchange
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: hashpartitioning(customer_id#1, 50), ENSURE_REQUIREMENTS, [plan_id=114]

(4) ShuffleQueryStage
Output [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: 0

(5) AQEShuffleRead
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: coalesced

(6) Sort [codegen id : 3]
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: [customer_id#1 ASC NULLS FIRST], false, 0

(7) Scan parquet 
Output [3]: [customer_id#8, segment#9, risk_tier#10]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/customers_dim]
PushedFilters: [IsNotNull(customer_id)]
ReadSchema: struct<customer_id:string,segment:string,risk_tier:string>

(8) ColumnarToRow [codegen id : 2]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(9) Filter [codegen id : 2]
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Condition : isnotnull(customer_id#8)

(10) Project [codegen id : 2]
Output [3]: [customer_id#8 AS c_customer_id#13, segment#9 AS c_segment#14, risk_tier#10 AS c_risk_tier#15]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(11) Exchange
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: hashpartitioning(c_customer_id#13, 50), ENSURE_REQUIREMENTS, [plan_id=131]

(12) ShuffleQueryStage
Output [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: 1

(13) AQEShuffleRead
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: coalesced

(14) Sort [codegen id : 4]
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: [c_customer_id#13 ASC NULLS FIRST], false, 0

(15) SortMergeJoin [codegen id : 5]
Left keys [1]: [customer_id#1]
Right keys [1]: [c_customer_id#13]
Join type: LeftOuter
Join condition: None

(16) Project [codegen id : 5]
Output [5]: [amount#2, country#3, dt#7, c_segment#14, c_risk_tier#15]
Input [7]: [customer_id#1, amount#2, country#3, dt#7, c_customer_id#13, c_segment#14, c_risk_tier#15]

(17) HashAggregate [codegen id : 5]
Input [5]: [amount#2, country#3, dt#7, c_segment#14, c_risk_tier#15]
Keys [4]: [dt#7, country#3, c_segment#14, c_risk_tier#15]
Functions [3]: [partial_count(1), partial_sum(amount#2), partial_avg(amount#2)]
Aggregate Attributes [4]: [count#35L, sum#36, sum#37, count#38L]
Results [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]

(18) Exchange
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Arguments: hashpartitioning(dt#7, country#3, c_segment#14, c_risk_tier#15, 50), ENSURE_REQUIREMENTS, [plan_id=233]

(19) ShuffleQueryStage
Output [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Arguments: 2

(20) AQEShuffleRead
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Arguments: coalesced

(21) HashAggregate [codegen id : 6]
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Keys [4]: [dt#7, country#3, c_segment#14, c_risk_tier#15]
Functions [3]: [count(1), sum(amount#2), avg(amount#2)]
Aggregate Attributes [3]: [count(1)#32L, sum(amount#2)#33, avg(amount#2)#34]
Results [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count(1)#32L AS txns#18L, sum(amount#2)#33 AS total_amount#19, avg(amount#2)#34 AS avg_amount#20]

(22) TakeOrderedAndProject
Input [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, txns#18L, total_amount#19, avg_amount#20]
Arguments: 51, [country#3 ASC NULLS FIRST, c_segment#14 ASC NULLS FIRST, c_risk_tier#15 ASC NULLS FIRST], [toprettystring(dt#7, Some(Asia/Ho_Chi_Minh)) AS dt#43, toprettystring(country#3, Some(Asia/Ho_Chi_Minh)) AS country#44, toprettystring(c_segment#14, Some(Asia/Ho_Chi_Minh)) AS c_segment#45, toprettystring(c_risk_tier#15, Some(Asia/Ho_Chi_Minh)) AS c_risk_tier#46, toprettystring(txns#18L, Some(Asia/Ho_Chi_Minh)) AS txns#47, toprettystring(total_amount#19, Some(Asia/Ho_Chi_Minh)) AS total_amount#48, toprettystring(avg_amount#20, Some(Asia/Ho_Chi_Minh)) AS avg_amount#49]

(23) ResultQueryStage
Output [7]: [dt#43, country#44, c_segment#45, c_risk_tier#46, txns#47, total_amount#48, avg_amount#49]
Arguments: 3

(24) Exchange
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: hashpartitioning(customer_id#1, 50), ENSURE_REQUIREMENTS, [plan_id=76]

(25) Sort
Input [4]: [customer_id#1, amount#2, country#3, dt#7]
Arguments: [customer_id#1 ASC NULLS FIRST], false, 0

(26) Filter
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Condition : isnotnull(customer_id#8)

(27) Project
Output [3]: [customer_id#8 AS c_customer_id#13, segment#9 AS c_segment#14, risk_tier#10 AS c_risk_tier#15]
Input [3]: [customer_id#8, segment#9, risk_tier#10]

(28) Exchange
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: hashpartitioning(c_customer_id#13, 50), ENSURE_REQUIREMENTS, [plan_id=77]

(29) Sort
Input [3]: [c_customer_id#13, c_segment#14, c_risk_tier#15]
Arguments: [c_customer_id#13 ASC NULLS FIRST], false, 0

(30) SortMergeJoin
Left keys [1]: [customer_id#1]
Right keys [1]: [c_customer_id#13]
Join type: LeftOuter
Join condition: None

(31) Project
Output [5]: [amount#2, country#3, dt#7, c_segment#14, c_risk_tier#15]
Input [7]: [customer_id#1, amount#2, country#3, dt#7, c_customer_id#13, c_segment#14, c_risk_tier#15]

(32) HashAggregate
Input [5]: [amount#2, country#3, dt#7, c_segment#14, c_risk_tier#15]
Keys [4]: [dt#7, country#3, c_segment#14, c_risk_tier#15]
Functions [3]: [partial_count(1), partial_sum(amount#2), partial_avg(amount#2)]
Aggregate Attributes [4]: [count#35L, sum#36, sum#37, count#38L]
Results [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]

(33) Exchange
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Arguments: hashpartitioning(dt#7, country#3, c_segment#14, c_risk_tier#15, 50), ENSURE_REQUIREMENTS, [plan_id=84]

(34) HashAggregate
Input [8]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count#39L, sum#40, sum#41, count#42L]
Keys [4]: [dt#7, country#3, c_segment#14, c_risk_tier#15]
Functions [3]: [count(1), sum(amount#2), avg(amount#2)]
Aggregate Attributes [3]: [count(1)#32L, sum(amount#2)#33, avg(amount#2)#34]
Results [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, count(1)#32L AS txns#18L, sum(amount#2)#33 AS total_amount#19, avg(amount#2)#34 AS avg_amount#20]

(35) TakeOrderedAndProject
Input [7]: [dt#7, country#3, c_segment#14, c_risk_tier#15, txns#18L, total_amount#19, avg_amount#20]
Arguments: 51, [country#3 ASC NULLS FIRST, c_segment#14 ASC NULLS FIRST, c_risk_tier#15 ASC NULLS FIRST], [toprettystring(dt#7, Some(Asia/Ho_Chi_Minh)) AS dt#43, toprettystring(country#3, Some(Asia/Ho_Chi_Minh)) AS country#44, toprettystring(c_segment#14, Some(Asia/Ho_Chi_Minh)) AS c_segment#45, toprettystring(c_risk_tier#15, Some(Asia/Ho_Chi_Minh)) AS c_risk_tier#46, toprettystring(txns#18L, Some(Asia/Ho_Chi_Minh)) AS txns#47, toprettystring(total_amount#19, Some(Asia/Ho_Chi_Minh)) AS total_amount#48, toprettystring(avg_amount#20, Some(Asia/Ho_Chi_Minh)) AS avg_amount#49]

(36) AdaptiveSparkPlan
Output [7]: [dt#43, country#44, c_segment#45, c_risk_tier#46, txns#47, total_amount#48, avg_amount#49]
Arguments: isFinalPlan=true
```

#### Gi·∫£i th√≠ch:


##### 1. S·ª± kh√°c bi·ªát gi·ªØa Initial Plan v√† Final Plan

* **Initial Plan (35):** ƒê√¢y l√† d·ª± t√≠nh c·ªßa Spark d·ª±a tr√™n c√°c con s·ªë ∆∞·ªõc l∆∞·ª£ng (statistics) c≈©. B·∫°n c√≥ th·ªÉ th·∫•y n√≥ kh√° gi·ªëng v·ªõi b·∫£n Plan c≈© m√† b·∫°n g·ª≠i tr∆∞·ªõc ƒë√≥.
* **Final Plan (23):** ƒê√¢y l√† k·∫ø ho·∫°ch Spark **th·ª±c s·ª± ch·∫°y**. Nh√¨n v√†o c·ªôt s·ªë th·ª© t·ª±, b·∫°n th·∫•y Spark b·∫Øt ƒë·∫ßu d√πng c√°c k√Ω hi·ªáu nh∆∞ `AQEShuffleRead` v√† `QueryStage`.

---

### 2. C√°c tham s·ªë "AQE" ƒë·∫∑c bi·ªát trong Plan n√†y

B·∫°n c·∫ßn ch√∫ √Ω c√°c th√†nh ph·∫ßn m√† b·∫£n c≈© kh√¥ng c√≥:

* **`AQEShuffleRead (5), (13), (20)`**: Thay v√¨ ƒë·ªçc tr·ª±c ti·∫øp t·ª´ Shuffle nh∆∞ c≈©, Spark ch√®n th√™m m·ªôt l·ªõp "Adaptive" v√†o gi·ªØa.
* **`Arguments: coalesced`**: ƒê√¢y l√† m·ªôt k·ªπ thu·∫≠t c·ª±c hay c·ªßa AQE. N·∫øu d·ªØ li·ªáu c·ªßa b·∫°n sau khi Shuffle b·ªã chia th√†nh qu√° nhi·ªÅu partition nh·ªè x√≠u (r√°c), Spark s·∫Ω t·ª± ƒë·ªông **gom (coalesce)** ch√∫ng l·∫°i th√†nh √≠t partition h∆°n ƒë·ªÉ tr√°nh t·ªën t√†i nguy√™n qu·∫£n l√Ω.


* **`ShuffleQueryStage`**: ƒê√¢y l√† m·ªôt "ƒëi·ªÉm d·ª´ng" (Checkpoint). Spark ch·∫°y xong b∆∞·ªõc n√†y, n√≥ s·∫Ω d·ª´ng l·∫°i m·ªôt nh·ªãp ƒë·ªÉ nh√¨n xem d·ªØ li·ªáu th·ª±c t·∫ø l·ªõn bao nhi√™u (Statistics) r·ªìi m·ªõi quy·∫øt ƒë·ªãnh b∆∞·ªõc ti·∫øp theo ch·∫°y nh∆∞ th·∫ø n√†o.

---

##### 3. Gi·∫£i th√≠ch c√°c con s·ªë th·ªëng k√™ (Statistics)

Nh√¨n v√†o Final Plan, b·∫°n s·∫Ω th·∫•y c√°c con s·ªë th·ª±c t·∫ø m√† Spark ƒëo ƒë∆∞·ª£c:

* **`Statistics(sizeInBytes=17.8 MiB, rowCount=3.33E+5)` (t·∫°i b∆∞·ªõc 4)**:
* `3.33E+5` ch√≠nh l√†  d√≤ng. ƒê√¢y ch√≠nh l√† s·ªë ƒë∆°n h√†ng c·ªßa 1 ng√†y m√† Spark v·ª´a ƒë·ªçc ƒë∆∞·ª£c t·ª´ b·∫£ng Orders.


* **`Statistics(sizeInBytes=1248.0 B, rowCount=13)` (t·∫°i b∆∞·ªõc 19)**:
* Sau khi Join v√† Aggregate, d·ªØ li·ªáu t·ª´ 333,000 d√≤ng ch·ªâ c√≤n l·∫°i **13 d√≤ng** k·∫øt qu·∫£ KPI. Spark th·∫•y n√≥ qu√° nh·ªè n√™n b∆∞·ªõc cu·ªëi c√πng n√≥ x·ª≠ l√Ω c·ª±c nhanh.


* **`sizeInBytes=8.0 EiB` (t·∫°i b∆∞·ªõc 23)**:
* ƒê√¢y l√† m·ªôt l·ªói hi·ªÉn th·ªã "tr√†n s·ªë" (overflow) th∆∞·ªùng g·∫∑p trong Spark UI khi t√≠nh to√°n c√°c gi√° tr·ªã v√¥ c·ª±c ho·∫∑c ch∆∞a x√°c ƒë·ªãnh r√µ ·ªü t·∫ßng Root, nh∆∞ng ƒë·ª´ng lo, n√≥ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn k·∫øt qu·∫£ ch·∫°y.



---

##### 4. T·∫°i sao v·∫´n l√† `SortMergeJoin` (15) m√† kh√¥ng ph·∫£i `skewed=true`?

T·∫°i sao m√¨nh ƒë√£ b·∫≠t `skewJoin.enabled` m√† Plan v·∫´n hi·ªán `SortMergeJoin` b√¨nh th∆∞·ªùng kh√¥ng?

**L√Ω do:** D·ªØ li·ªáu b·∫°n ƒëang filter l√† **1 ng√†y** (`dt = 2026-01-10`).

* Trong ng√†y n√†y, kh√°ch h√†ng `customer_id=1` c√≥ kho·∫£ng h∆°n 100,000 d√≤ng (d·ª±a tr√™n t·ª∑ l·ªá 30%).
* K√≠ch th∆∞·ªõc partition n√†y r∆°i v√†o kho·∫£ng **17.8 MiB** (nh∆∞ th·ªëng k√™ ·ªü b∆∞·ªõc 4).
* Trong c·∫•u h√¨nh m·∫∑c ƒë·ªãnh c·ªßa Spark, m·ªôt partition ch·ªâ ƒë∆∞·ª£c coi l√† "Skew" khi n√≥ l·ªõn h∆°n nhi·ªÅu so v·ªõi trung b√¨nh **V√Ä** ph·∫£i l·ªõn h∆°n m·ªôt ng∆∞·ª°ng nh·∫•t ƒë·ªãnh (th∆∞·ªùng l√† `64MB` ho·∫∑c `128MB`).

**K·∫øt lu·∫≠n:** V·ªõi Spark, c·ª•c 17.8 MiB v·∫´n l√† "mu·ªói", n√≥ c·∫£m th·∫•y d√πng `SortMergeJoin` truy·ªÅn th·ªëng v·∫´n x·ª≠ l√Ω nhanh ch√°n, ch∆∞a c·∫ßn k√≠ch ho·∫°t ch·∫ø ƒë·ªô "ch·∫ª nh·ªè" (Split skew).

---

##### 5. Nh·ªØng k√Ω hi·ªáu "L·∫°" kh√°c

* **`toprettystring(...)` (b∆∞·ªõc 22)**: Spark t·ª± ƒë·ªông chuy·ªÉn c√°c ƒë·ªãnh d·∫°ng ng√†y th√°ng v√† s·ªë v·ªÅ d·∫°ng chu·ªói "ƒë·∫πp" ƒë·ªÉ hi·ªÉn th·ªã l√™n m√†n h√¨nh cho b·∫°n xem.
* **`isFinalPlan=true` (b∆∞·ªõc 36)**: Ch·ªët h·∫°! ƒê√¢y l√† k·∫ø ho·∫°ch t·ªëi ∆∞u nh·∫•t m√† Spark ƒë√£ th·ª±c thi xong.

##### üí° M·∫πo th·ª±c t·∫ø cho T√¢n:

ƒê·ªÉ √©p Spark ph·∫£i hi·ªán `skewed=true` trong Plan nh·∫±m m·ª•c ƒë√≠ch h·ªçc t·∫≠p, b·∫°n c√≥ th·ªÉ:

1. TƒÉng `N_ORDERS` l√™n g·∫•p 10 l·∫ßn.
2. Ho·∫∑c h·∫° th·∫•p ng∆∞·ª°ng nh·∫≠n di·ªán skew c·ªßa Spark xu·ªëng b·∫±ng l·ªánh:
`spark.conf.set("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes", "1mb")`

**Th·ª≠ h·∫° ng∆∞·ª°ng n√†y xu·ªëng ƒë·ªÉ b·∫°n ch·∫°y l·∫°i v√† t·∫≠n m·∫Øt th·∫•y ch·ªØ "skewed=true" xu·∫•t hi·ªán trong Plan kh√¥ng?**

---

## 3. S∆° ƒë·ªì minh h·ªça AQE

```mermaid
graph TD
    subgraph OFF [K·ªãch b·∫£n AQE OFF]
    O1[Scan Orders dt=10] --> S1[Shuffle by customer_id]
    C1[Scan Customers] --> S2[Shuffle by customer_id]
    S1 --> J1[SortMergeJoin]
    S2 --> J1
    J1 --> K1[1 Task g√°nh 30% Data - Ch·∫°y c·ª±c l√¢u]
    end

    subgraph ON [K·ªãch b·∫£n AQE ON]
    O2[Scan Orders dt=10] --> S3[Shuffle by customer_id]
    C2[Scan Customers] --> S4[Shuffle by customer_id]
    S3 --> SKEW{Ph√°t hi·ªán Skew?}
    SKEW -- Yes --> SPLIT[Split Skew Partition th√†nh N Task nh·ªè]
    SPLIT --> J2[SortMergeJoin Parallel]
    S4 --> J2
    J2 --> K2[C√°c Task ch·∫°y ƒë·ªÅu nhau - Ho√†n th√†nh s·ªõm]
    end

```

---

## 4. Gi·∫£i th√≠ch chi ti·∫øt c√°c k·ªπ thu·∫≠t PySpark s·ª≠ d·ª•ng

| K·ªπ thu·∫≠t | Gi·∫£i th√≠ch |
| --- | --- |
| `spark.range(0, N)` | T·∫°o DataFrame ch·ªâ c√≥ 1 c·ªôt `id`. ƒê√¢y l√† c√°ch nhanh nh·∫•t ƒë·ªÉ t·∫°o data gi·∫£ l·∫≠p m√† kh√¥ng c·∫ßn ƒë·ªçc file v·∫≠t l√Ω. |
| `expr(f"CASE ...")` | S·ª≠ d·ª•ng SQL string ngay trong Python. Gi√∫p vi·∫øt logic ph·ª©c t·∫°p (nh∆∞ t·∫°o Skew ng·∫´u nhi√™n) d·ªÖ ƒë·ªçc h∆°n. |
| `rand(seed)` | H√†m t·∫°o s·ªë ng·∫´u nhi√™n. Vi·ªác truy·ªÅn `seed` (v√≠ d·ª• `rand(7)`) gi√∫p d·ªØ li·ªáu sinh ra gi·ªëng h·ªát nhau ·ªü m·ªói l·∫ßn ch·∫°y (Reproducible). |
| `make_interval(...)` | H√†m t·∫°o kho·∫£ng th·ªùi gian (nƒÉm, th√°ng, ng√†y...). D√πng c√°i n√†y thay cho t·ª´ kh√≥a `INTERVAL` trong SQL ƒë·ªÉ tr√°nh c√°c l·ªói kh√¥ng ƒë·ªãnh nghƒ©a ƒë∆∞·ª£c Routine tr√™n m·ªôt s·ªë phi√™n b·∫£n Spark. |
| `partitionBy("dt")` | Khi ghi d·ªØ li·ªáu, Spark s·∫Ω t·∫°o c√°c th∆∞ m·ª•c ri√™ng cho t·ª´ng ng√†y. Gi√∫p c√°c truy v·∫•n sau n√†y ch·ªâ c·∫ßn ƒë·ªçc ƒë√∫ng th∆∞ m·ª•c ng√†y ƒë√≥ (**Partition Pruning**), kh√¥ng c·∫ßn qu√©t to√†n b·ªô 10 tri·ªáu d√≤ng. |

---

## 5. B·∫°n n√™n quan s√°t g√¨ tr√™n Spark UI?

Khi ch·∫°y b·∫£n **AQE ON**, b·∫°n h√£y m·ªü tr√¨nh duy·ªát v√†o c·ªïng `4040`:

1. V√†o tab **SQL**.
2. Nh·∫•n v√†o query Join g·∫ßn nh·∫•t.
3. Trong s∆° ƒë·ªì (Plan), t√¨m c√°c node c√≥ t√™n l√† **"SortMergeJoin (skewed=true)"**.
4. N·∫øu th·∫•y ch·ªØ `skewed=true`, nghƒ©a l√† Spark ƒë√£ th√†nh c√¥ng trong vi·ªác nh·∫≠n di·ªán v√† x·ª≠ l√Ω "c·ª•c n·ª£" mang t√™n `customer_id=1` cho b·∫°n!

**B∆∞·ªõc ti·∫øp theo:** B·∫°n h√£y ch·∫°y l·∫ßn l∆∞·ª£t c√°c script tr√™n m√°y c·ªßa m√¨nh. N·∫øu g·∫∑p l·ªói ·ªü b·∫•t k·ª≥ b∆∞·ªõc n√†o ho·∫∑c mu·ªën m√¨nh gi·∫£i th√≠ch k·ªπ h∆°n v·ªÅ b·∫£ng log c·ªßa Spark UI, h√£y cho m√¨nh bi·∫øt nh√©! Would you like me to explain how to read the **Timeline View** in Spark UI to prove the stragglers are gone?