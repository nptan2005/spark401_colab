# LAB 3.2 ‚Äì Fact‚ÄìDim Join ·ªü quy m√¥ l·ªõn  
## Chi·∫øn l∆∞·ª£c t·ªëi ∆∞u TH·ª∞C T·∫æ (Bank / Payment / Analytics)

> M·ª•c ti√™u lab n√†y:
> - L√†m **fact‚Äìdim join quy m√¥ l·ªõn**
> - C√≥ **skew, partition, pruning, join strategy**
> - ƒê·ªçc ƒë∆∞·ª£c **Spark UI / Explain**
> - G·∫ßn v·ªõi **b√†i to√°n ng√¢n h√†ng ‚Äì thanh to√°n ‚Äì KPI**

---

## 0Ô∏è‚É£ B·ªëi c·∫£nh th·ª±c t·∫ø (Real-world context)

Trong ng√¢n h√†ng / payment / fintech, ta th∆∞·ªùng c√≥:

### FACT TABLE (l·ªõn ‚Äì h√†ng tri·ªáu ƒë·∫øn t·ª∑)
- `orders / transactions / payments`
- Append-only
- Partition theo `dt`

### DIM TABLE (nh·ªè h∆°n nh∆∞ng v·∫´n l·ªõn)
- `customers`
- `merchants`
- `accounts`

üëâ B√†i to√°n ph·ªï bi·∫øn:

> **T√≠nh KPI giao d·ªãch theo ng√†y, qu·ªëc gia, ph√¢n kh√∫c kh√°ch h√†ng**

---

## 1Ô∏è‚É£ Thi·∫øt k·∫ø d·ªØ li·ªáu cho LAB 3.2

### 1.1 Fact: `orders_fact_dt`
- ~ **10 tri·ªáu rows**
- Partition theo `dt`
- C√≥ skew customer_id = `"1"` (VIP / Merchant l·ªõn)

| column | √Ω nghƒ©a |
|------|-------|
| order_id | id giao d·ªãch |
| customer_id | kh√°ch |
| amount | s·ªë ti·ªÅn |
| country | VN / TH / SG |
| dt | ng√†y giao d·ªãch |

---

### 1.2 Dim: `customers_dim`
- ~ **200k rows**
- Kh√¥ng partition
- Join key: `customer_id`

| column | √Ω nghƒ©a |
|------|-------|
| customer_id | PK |
| segment | MASS / AFFLUENT / SME |
| risk_tier | LOW / MED / HIGH |

---

## 2Ô∏è‚É£ Chu·∫©n b·ªã data (ƒë√£ ch·∫°y xong ph√≠a b·∫°n)

B·∫°n **ƒë√£ l√†m ƒë√∫ng** ph·∫ßn n√†y üëç  
‚Üí Ch√∫ng ta **t·∫≠p trung JOIN & t·ªëi ∆∞u**

---

## 3Ô∏è‚É£ Query baseline (ch∆∞a t·ªëi ∆∞u)

### 3.1 Code

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, sum, avg, count

spark = (
    SparkSession.builder
    .appName("lab3_2_baseline")
    .config("spark.sql.shuffle.partitions", "50")
    .getOrCreate()
)

orders = spark.read.parquet("data/silver_lab32/orders_fact_dt")
customers = spark.read.parquet("data/silver_lab32/customers_dim")

res = (
    orders
    .where(col("dt") == "2026-01-10")
    .join(customers, "customer_id", "left")
    .groupBy("dt", "country", "segment", "risk_tier")
    .agg(
        count("*").alias("txns"),
        sum("amount").alias("total_amount"),
        avg("amount").alias("avg_amount")
    )
)

res.explain("formatted")
res.show(5)

spark.stop()

```

### 3.2 K·∫øt qu·∫£

```code
== Physical Plan ==
AdaptiveSparkPlan (10)
+- HashAggregate (9)
   +- Exchange (8)
      +- HashAggregate (7)
         +- Project (6)
            +- BroadcastHashJoin LeftOuter BuildRight (5)
               :- Scan parquet  (1)
               +- BroadcastExchange (4)
                  +- Filter (3)
                     +- Scan parquet  (2)


(1) Scan parquet 
Output [4]: [customer_id#1, amount#2, country#3, dt#7]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/orders_fact_dt]
PartitionFilters: [isnotnull(dt#7), (dt#7 = 2026-01-10)]
ReadSchema: struct<customer_id:string,amount:double,country:string>

(2) Scan parquet 
Output [3]: [customer_id#8, segment#9, risk_tier#10]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_lab32/customers_dim]
PushedFilters: [IsNotNull(customer_id)]
ReadSchema: struct<customer_id:string,segment:string,risk_tier:string>

(3) Filter
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Condition : isnotnull(customer_id#8)

(4) BroadcastExchange
Input [3]: [customer_id#8, segment#9, risk_tier#10]
Arguments: HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=31]

(5) BroadcastHashJoin
Left keys [1]: [customer_id#1]
Right keys [1]: [customer_id#8]
Join type: LeftOuter
Join condition: None

(6) Project
Output [5]: [amount#2, country#3, dt#7, segment#9, risk_tier#10]
Input [7]: [customer_id#1, amount#2, country#3, dt#7, customer_id#8, segment#9, risk_tier#10]

(7) HashAggregate
Input [5]: [amount#2, country#3, dt#7, segment#9, risk_tier#10]
Keys [4]: [dt#7, country#3, segment#9, risk_tier#10]
Functions [3]: [partial_count(1), partial_sum(amount#2), partial_avg(amount#2)]
Aggregate Attributes [4]: [count#33L, sum#34, sum#35, count#36L]
Results [8]: [dt#7, country#3, segment#9, risk_tier#10, count#37L, sum#38, sum#39, count#40L]

(8) Exchange
Input [8]: [dt#7, country#3, segment#9, risk_tier#10, count#37L, sum#38, sum#39, count#40L]
Arguments: hashpartitioning(dt#7, country#3, segment#9, risk_tier#10, 50), ENSURE_REQUIREMENTS, [plan_id=36]

(9) HashAggregate
Input [8]: [dt#7, country#3, segment#9, risk_tier#10, count#37L, sum#38, sum#39, count#40L]
Keys [4]: [dt#7, country#3, segment#9, risk_tier#10]
Functions [3]: [count(1), sum(amount#2), avg(amount#2)]
Aggregate Attributes [3]: [count(1)#30L, sum(amount#2)#31, avg(amount#2)#32]
Results [7]: [dt#7, country#3, segment#9, risk_tier#10, count(1)#30L AS txns#15L, sum(amount#2)#31 AS total_amount#16, avg(amount#2)#32 AS avg_amount#17]

(10) AdaptiveSparkPlan
Output [7]: [dt#7, country#3, segment#9, risk_tier#10, txns#15L, total_amount#16, avg_amount#17]
Arguments: isFinalPlan=false
```

### Gi·∫£i th√≠ch:

Physical Plan n√†y c·ª±c k·ª≥ **·ªïn** v√† th∆∞·ªùng l√† tr·∫°ng th√°i **l√Ω t∆∞·ªüng nh·∫•t** cho c√°c truy v·∫•n Join gi·ªØa m·ªôt b·∫£ng Fact l·ªõn v√† m·ªôt b·∫£ng Dimension nh·ªè.

ƒê√¢y ch√≠nh l√† k·ªãch b·∫£n "Happy Path" m√† c√°c Data Engineer lu√¥n h∆∞·ªõng t·ªõi. H√£y ph√¢n t√≠ch xem t·∫°i sao n√≥ l·∫°i t·ªët:

#### 1. "V≈© kh√≠ h·∫°ng n·∫∑ng": BroadcastHashJoin (5)

Thay v√¨ ph·∫£i d√πng `SortMergeJoin` (t·ªën c√¥ng Shuffle d·ªØ li·ªáu c·∫£ hai b·∫£ng qua m·∫°ng v√† Sort l·∫°i), Spark ƒë√£ ch·ªçn **BroadcastHashJoin (BHJ)**.

* **C∆° ch·∫ø:** Spark l·∫•y b·∫£ng nh·ªè (Customers - b√™n ph·∫£i) g·ª≠i b·∫£n sao ƒë·∫øn **t·∫•t c·∫£** c√°c m√°y Worker ƒëang gi·ªØ c√°c ph·∫ßn c·ªßa b·∫£ng Orders.
* **T·∫°i sao ·ªïn?**:
* **Kh√¥ng Shuffle b·∫£ng to:** B·∫£ng Orders (10 tri·ªáu d√≤ng) ƒë·ª©ng y√™n t·∫°i ch·ªó, kh√¥ng ph·∫£i bay qua m·∫°ng. ƒêi·ªÅu n√†y ti·∫øt ki·ªám c·ª±c nhi·ªÅu bƒÉng th√¥ng v√† th·ªùi gian.
* **Ph√° gi·∫£i ho√†n to√†n Skew:** Trong BHJ, Skew kh√¥ng c√≤n l√† v·∫•n ƒë·ªÅ ƒë√°ng s·ª£ n·ªØa. D√π `customer_id=1` c√≥ 3 tri·ªáu d√≤ng n·∫±m ·ªü 1 m√°y, m√°y ƒë√≥ ch·ªâ vi·ªác l·∫•y b·∫£ng Customers (ƒë√£ n·∫±m s·∫µn trong RAM) ra ƒë·ªÉ "so kh·ªõp" (lookup) c·ª±c nhanh. Kh√¥ng c√≥ b∆∞·ªõc Shuffle d·ªìn c·ª•c n√™n kh√¥ng c√≥ Task n√†o b·ªã ngh·∫Ωn (Straggler).



---

#### 2. Ph√¢n t√≠ch c√°c b∆∞·ªõc then ch·ªët

* **(4) BroadcastExchange**: ƒê√¢y l√† b∆∞·ªõc g·ª≠i b·∫£ng Customers ƒëi "ph·ªß s√≥ng" to√†n c·ª•m m√°y ch·ªß. B·∫°n s·∫Ω th·∫•y `BuildRight`, nghƒ©a l√† b·∫£ng b√™n ph·∫£i (Customers) ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ broadcast.
* **(1) PartitionFilters [dt = 2026-01-10]**: V·∫´n gi·ªØ ƒë∆∞·ª£c k·ªπ thu·∫≠t **Partition Pruning**. Spark ch·ªâ ƒë·ªçc ƒë√∫ng file c·∫ßn thi·∫øt, gi√∫p t·ªëc ƒë·ªô c·ª±c nhanh.
* **(7) & (9) HashAggregate**: Qu√° tr√¨nh t√≠nh to√°n KPI (`txns`, `total_amount`) di·ªÖn ra m∆∞·ª£t m√† ngay sau khi Join xong.

---

#### 3. T·∫°i sao Plan n√†y l·∫°i xu·∫•t hi·ªán?

Tr∆∞·ªõc ƒë√≥ b·∫°n th·∫•y `SortMergeJoin` v√¨ ch√∫ng ta ƒë√£ ch·ªß ƒë·ªông t·∫Øt Broadcast b·∫±ng c·∫•u h√¨nh:
`spark.sql.autoBroadcastJoinThreshold = -1`.

Trong Plan n√†y, c√≥ v·∫ª b·∫°n ƒë√£ b·∫≠t l·∫°i n√≥ (m·∫∑c ƒë·ªãnh l√† 10MB) ho·∫∑c kh√¥ng c·∫•u h√¨nh t·∫Øt n√≥ n·ªØa. Spark th·∫•y b·∫£ng Customers ƒë·ªß nh·ªè ƒë·ªÉ nh√©t v·ª´a RAM c·ªßa c√°c m√°y Worker n√™n n√≥ t·ª± ƒë·ªông ch·ªçn BHJ.

---

#### 4. M·ªôt l∆∞u √Ω nh·ªè: `isFinalPlan=false` (10)

D√π Plan n√†y r·∫•t t·ªët, nh∆∞ng k√Ω hi·ªáu `isFinalPlan=false` ·ªü b∆∞·ªõc s·ªë (10) cho th·∫•y ƒë√¢y v·∫´n l√† **AdaptiveSparkPlan**.

* Spark ƒëang n√≥i: *"T√¥i d·ª± ƒë·ªãnh d√πng BroadcastHashJoin, nh∆∞ng t√¥i s·∫Ω v·ª´a ch·∫°y v·ª´a quan s√°t. N·∫øu sau khi ƒë·ªçc b·∫£ng Orders xong m√† th·∫•y n√≥ qu√° nh·ªè, ho·∫∑c b·∫£ng Customers th·ª±c t·∫ø l·∫°i to h∆°n t√¥i t∆∞·ªüng (v∆∞·ª£t ng∆∞·ª°ng 10MB), t√¥i c√≥ th·ªÉ s·∫Ω ƒë·ªïi √Ω ·ªü gi√¢y cu·ªëi c√πng."*

#### üí° So s√°nh nhanh cho T√¢n:

| ƒê·∫∑c ƒëi·ªÉm | SortMergeJoin (Lab tr∆∞·ªõc) | BroadcastHashJoin (Lab n√†y) |
| --- | --- | --- |
| **T·ªëc ƒë·ªô** | Ch·∫≠m (do Shuffle & Sort) | **C·ª±c nhanh** |
| **Skew** | B·ªã ngh·∫Ωn t·∫°i Key b·ªã l·ªách | **B·∫•t ch·∫•p Skew** |
| **BƒÉng th√¥ng m·∫°ng** | T·ªën nhi·ªÅu (g·ª≠i c·∫£ 2 b·∫£ng) | T·ªën √≠t (ch·ªâ g·ª≠i b·∫£ng nh·ªè) |
| **R·ªßi ro** | √çt r·ªßi ro OOM | D·ªÖ **OOM** n·∫øu b·∫£ng nh·ªè kh√¥ng th·ª±c s·ª± nh·ªè |

**K·∫øt lu·∫≠n:** N·∫øu b·∫£ng Dimension (Customers) c·ªßa b·∫°n d∆∞·ªõi v√†i trƒÉm MB, h√£y lu√¥n ∆∞u ti√™n ƒë·ªÉ Spark ch·∫°y **BroadcastHashJoin** nh∆∞ th·∫ø n√†y. ƒê√¢y l√† tr·∫°ng th√°i t·ªëi ∆∞u nh·∫•t!

---

## 4Ô∏è‚É£ Ph√¢n t√≠ch Explain ‚Äì Baseline

### 4.1 Nh·ªØng ƒëi·ªÉm quan tr·ªçng trong plan

#### B·∫°n s·∫Ω th·∫•y:

```code
Scan parquet (orders)
PartitionFilters: dt = '2026-01-10'   ‚úÖ PRUNING OK

SortMergeJoin ‚ùå
Exchange (shuffle) ·ªü c·∫£ 2 ph√≠a ‚ùå‚ùå
```

### 4.2 √ù nghƒ©a

|**Hi·ªán t∆∞·ª£ng**|**Nh·∫≠n x√©t**|
|--------------|------------|
|Partition pruning	|‚úÖ T·ªët|
|SortMergeJoin|	‚ùå ƒê·∫Øt|
|Shuffle tr∆∞·ªõc join	|‚ùå R·∫•t t·ªën IO|
|Shuffle sau agg|	‚ùå Kh√¥ng tr√°nh ƒë∆∞·ª£c|

> üëâ Baseline ch·∫°y ƒë∆∞·ª£c nh∆∞ng ch∆∞a t·ªëi ∆∞u

---

## 5Ô∏è‚É£ T·ªëi ∆∞u #1 ‚Äì Broadcast Dim (TH·ª∞C T·∫æ D√ôNG NHI·ªÄU NH·∫§T)


### 5.1 Khi n√†o d√πng?
-	Dim < ~100MB
-	√çt thay ƒë·ªïi
-	Join nhi·ªÅu l·∫ßn

### 5.2 Code

```python
from pyspark.sql.functions import broadcast

res = (
    orders
    .where(col("dt") == "2026-01-10")
    .join(broadcast(customers), "customer_id", "left")
    .groupBy("dt", "country", "segment", "risk_tier")
    .agg(
        count("*").alias("txns"),
        sum("amount").alias("total_amount"),
        avg("amount").alias("avg_amount")
    )
)

res.explain("formatted")
```

---

### 5.3 Explain ‚Äì Broadcast Join

#### B·∫°n s·∫Ω th·∫•y:

```code
BroadcastHashJoin ‚úÖ
NO shuffle ph√≠a dim ‚úÖ
Shuffle ch·ªâ c√≤n ·ªü aggregation
```

#### 5.4 K·∫øt lu·∫≠n

|**Ti√™u ch√≠**|**Baseline**|**Broadcast**|
|------------|------------|-------------|
|Join|SortMergeJoin|BroadcastHashJoin|
|Shuffle|2 ph√≠a|1 ph√≠a|
|Runtime|Cao|Th·∫•p h∆°n r√µ|

> üëâ 90% job analytics production d√πng c√°ch n√†y

‚∏ª

## 6Ô∏è‚É£ V·∫•n ƒë·ªÅ TH·ª∞C T·∫æ: Skew (customer_id = ‚Äò1‚Äô)

### 6.1 D·∫•u hi·ªáu tr√™n Spark UI
-	1 task ch·∫°y l√¢u b·∫•t th∆∞·ªùng
-	Input records c·ª±c l·ªõn ·ªü 1 partition

> üëâ B·∫°n ƒë√£ ƒë√∫ng khi ph√°t hi·ªán skew ·ªü LAB tr∆∞·ªõc

---

## 7Ô∏è‚É£ T·ªëi ∆∞u #2 ‚Äì Salting (ch·ªâ d√πng khi B·∫ÆT BU·ªòC)

### ‚ö†Ô∏è Salting KH√îNG ph·∫£i default
#### ‚Üí ch·ªâ d√πng khi:
-	Kh√¥ng broadcast ƒë∆∞·ª£c
-	Skew c·ª±c n·∫∑ng
-	Join b·∫Øt bu·ªôc shuffle

### 7.1 √ù t∆∞·ªüng

```code
customer_id = 1
‚Üí chia th√†nh (1, salt=0..N)
```

---

### 7.2 Code salting (ƒë√£ FIX ƒë√∫ng cho b·∫°n)

```python
from pyspark.sql.functions import rand, floor, explode, sequence

SALT_N = 16

o = orders.withColumn(
    "salt",
    floor(rand(7) * SALT_N).cast("int")
)

c = (
    customers
    .withColumn("salt", explode(sequence(0, SALT_N - 1)))
)

j = (
    o.join(
        c,
        (o.customer_id == c.customer_id) & (o.salt == c.salt),
        "left"
    )
)
```

---

### 7.3 Explain salting

#### B·∫°n s·∫Ω th·∫•y:

**SortMergeJoin**
**BUT:**
- partition key = (customer_id, salt)
- skew ƒë∆∞·ª£c d√†n ƒë·ªÅu

### 7.4 Trade-off

|**∆Øu**|**Nh∆∞·ª£c**|
|------|---------|
|Gi·∫£m skew|TƒÉng data dim|
|·ªîn ƒë·ªãnh runtime|	Code ph·ª©c t·∫°p|
|D√πng khi c·∫ßn|Kh√¥ng default|


---

## 8Ô∏è‚É£ Th·ª© t·ª± ∆∞u ti√™n T·ªêI ∆ØU (R·∫§T QUAN TR·ªåNG)

#### 1Ô∏è‚É£ Partition pruning (dt)
#### 2Ô∏è‚É£ Broadcast dim
#### 3Ô∏è‚É£ AQE (Adaptive Query Execution)
#### 4Ô∏è‚É£ Repartition h·ª£p l√Ω
#### 5Ô∏è‚É£ Salting (cu·ªëi c√πng)

#### ‚ùå KH√îNG: salting tr∆∞·ªõc khi th·ª≠ broadcast
#### ‚ùå KH√îNG: repartition m√π qu√°ng

---

## 9Ô∏è‚É£ S∆° ƒë·ªì t∆∞ duy (ASCII ‚Äì d·ªÖ nh·ªõ)

```code
FACT (orders, 10M, partition dt)
   |
   |-- filter dt --> gi·∫£m IO
   |
   |-- join customers
         |
         |-- broadcast --> FAST (default)
         |
         |-- shuffle --> check skew
                 |
                 |-- skew n·∫∑ng --> SALT
```

---

## üîü K·∫øt lu·∫≠n LAB 3.2

-	‚úÖ Hi·ªÉu fact‚Äìdim join quy m√¥ l·ªõn
-	‚úÖ ƒê·ªçc ƒë∆∞·ª£c Spark Explain
-	‚úÖ Ph√¢n bi·ªát:
-	Broadcast vs Shuffle
-	Khi n√†o d√πng salting
-	‚úÖ T∆∞ duy production-grade

