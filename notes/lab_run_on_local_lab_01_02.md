# LAB: SETUP PLAN

H∆∞·ªõng d·∫´n chu·∫©n b·ªã m√¥i tr∆∞·ªùng th·ª±c hi·ªán spark tr√™n local theo h∆∞·ªõng:
- linh ƒë·ªông codespace, colab, local
- c√≥ th·ªÉ xem spark UI

---

## 0) S∆° ƒë·ªì t·ªïng th·ªÉ l·ªô tr√¨nh (local ‚Üí cloud)

```mermaid
flowchart LR
  A[Local Laptop\nSpark 4.0.1 + Spark UI] -->|Dev & Validate| B[Git Repo\nnotebooks + pyspark jobs]
  B -->|CI/CD| C[Artifacts\nwheel/jar/py + dags]
  C -->|Sync| D[GCS Buckets\ncode + dags + data]
  D -->|Orchestrate| E[Cloud Composer\nAirflow DAGs]
  E -->|Run| F[Dataproc Cluster or Serverless Batch]
  F -->|Write| G[Bronze/Silver in GCS\nGold in BigQuery]
  F -->|Observability| H[Cloud Logging + Spark History]
  D -->|Governance| I[Dataplex/Data Catalog\nLineage/Metadata]
```

---

## 1) Setup m√¥i tr∆∞·ªùng Spark 4.0.1 tr√™n local (ƒë·ªÉ xem Spark UI)

ƒê·ªÅ xu·∫•t 2 c√°ch. C√°ch A (Docker) th∆∞·ªùng ‚Äúnhanh g·ªçn ‚Äì √≠t l·ªói‚Äù nh·∫•t. C√°ch B (c√†i native) ph√π h·ª£p n·∫øu b·∫°n mu·ªën Spark ‚Äúthu·∫ßn m√°y‚Äù.

### C√°ch A ‚Äî Docker (khuy·∫øn ngh·ªã ƒë·ªÉ h·ªçc + lab)

#### B1. C√†i Docker Desktop (macOS/Windows).

#### B2. T·∫°o docker-compose.yml (Spark master + worker + history server)
- B·∫°n s·∫Ω ch·∫°y Spark local theo ch·∫ø ƒë·ªô standalone cluster nh·ªè, c√≥ UI:
-		Spark Master UI: http://localhost:8080
- Spark App UI (khi ch·∫°y job): th∆∞·ªùng http://localhost:4040
- 	Spark History UI: http://localhost:18080

#### B3. Run
- docker compose up -d
- 	M·ªü UI, ƒë·∫£m b·∫£o Spark Master/Worker ‚Äúalive‚Äù.

#### B4. Dev code
-	D√πng VS Code local + Python venv.
-	K·∫øt n·ªëi spark://localhost:7077 (master) ho·∫∑c ch·∫°y local[*] t√πy lab.

---

### C√°ch B ‚Äî C√†i native (n·∫øu b·∫°n mu·ªën ch·∫°y ‚Äúthu·∫ßn m√°y‚Äù)

#### B1. Java
- Spark 4.x th∆∞·ªùng ch·∫°y t·ªët v·ªõi Java m·ªõi (tu·ª≥ build), nh∆∞ng ƒë·ªÉ ‚Äú·ªïn ƒë·ªãnh cho h·ªçc‚Äù b·∫°n c√≥ th·ªÉ d√πng Java LTS.
- 	C√†i Java + set JAVA_HOME.

#### B2. C√†i Spark 4.0.1
- Download Spark 4.0.1 prebuilt (Hadoop3).
-	Set env:
*	SPARK_HOME
*		PATH=$SPARK_HOME/bin:$PATH

#### B3. PySpark
-	D√πng pip install pyspark==4.0.1 (ho·∫∑c ƒë√∫ng version b·∫°n ƒëang luy·ªán).
-	Ch·∫°y th·ª≠:
*	pyspark
*	ho·∫∑c notebook + SparkSession.builder.getOrCreate().

#### B4. Spark UI
-	Khi ch·∫°y job: m·ªü http://localhost:4040.

---

## 2) B·ªô LAB theo h∆∞·ªõng 1 (local-first, c√≥ Spark UI)

#### M·ªói lab m√¨nh s·∫Ω tr√¨nh b√†y ƒë√∫ng format b·∫°n y√™u c·∫ßu:
1.	T·∫°o data m·∫´u
2.	X·ª≠ l√Ω
3.	Gi·∫£i th√≠ch
4.	Code demo
5.	C√≥/kh√¥ng d√πng numpy/pandas (d√πng khi ‚Äúƒë√∫ng ch·ªó‚Äù)
6.	C√¥ng c·ª•, lib th·ª±c t·∫ø

### Lab 1‚Äì6 (nh·∫π, n·∫Øm n·ªÅn)
- Lab 1: DataFrame fundamentals + lazy evaluation + explain
-	Lab 2: Partitioning 101 + repartition/coalesce + t√°c ƒë·ªông l√™n UI
-	Lab 3: Shuffle c∆° b·∫£n (groupBy, join) + ƒë·ªçc Exchange
-	Lab 4: Join strategies (broadcast vs sort-merge) + when/why
-	Lab 5: Skew fundamentals + detect skew trong UI
-	Lab 6: Cache/persist ƒë√∫ng c√°ch + spill to disk + memory model

### Lab 7‚Äì10 (v·ª´a, ‚Äúbank-like‚Äù h∆°n)
-	Lab 7: Bronze‚ÜíSilver chu·∫©n ho√° schema + quality checks
-		Lab 8: Silver‚ÜíGold aggregation + window functions + incremental
- Lab 9: Idempotent output + atomic commit pattern (write temp ‚Üí promote)
-	Lab 10: Observability: log + metrics + explain plan snapshot

C√°c lab n√†y ch·∫°y local, v√† b·∫°n lu√¥n xem Spark UI ƒë·ªÉ hi·ªÉu ‚Äúv√¨ sao‚Äù.

---

## 3) Sau lab nh·∫π ‚Üí lab n·∫∑ng ‚Äúdata th·∫≠t‚Äù (local ‚Üí GCP, c√≥ masking/token + governance)

### Data th·∫≠t (g·ª£i √Ω 3 l·ª±a ch·ªçn, b·∫°n ch·ªçn 1)
1.	NYC TLC Trip Record Data (r·∫•t l·ªõn, chu·∫©n big data)
-	C√≥ ngu·ªìn ch√≠nh th·ª©c TLC.  Ôøº
-	C√≥ c·∫£ registry/open data (d·ªÖ l·∫•y ƒë∆∞·ªùng d·∫´n bucket/object).  Ôøº
> **B√†i to√°n bank-like:** t·∫°o ‚Äútravel spending profile‚Äù, fraud-ish rules, daily aggregates, late arriving data.

2.	Olist e-commerce (v·ª´a, d·ªÖ l√†m end-to-end)
-	C√≥ repo t·ªïng h·ª£p dataset (th∆∞·ªùng tr·ªè v·ªÅ Kaggle).  Ôøº

> **B√†i to√°n bank-like:** kh√°ch h√†ng/ƒë∆°n h√†ng/ho√†n ti·ªÅn, cohort, RFM, merchant performance.

3.	Fraud dataset (IEEE-CIS Fraud Detection)
-	Th∆∞·ªùng d√πng ƒë·ªÉ m√¥ ph·ªèng fraud scoring pipeline.  Ôøº

> **B√†i to√°n bank-like:** feature engineering, drift checks, lineage.

### Masking/Tokenization ‚Äúƒë·∫£m b·∫£o policy‚Äù
- Local (on-prem)
-	Mask PII: email/phone/name/address.
-	Tokenization theo deterministic hash (HMAC) ƒë·ªÉ join cross-table v·∫´n ƒë∆∞·ª£c m√† kh√¥ng l·ªô raw PII.
-	Cloud (GCP)
-	L∆∞u mapping trong vault/secret manager (tu·ª≥ m·ª©c b√†i).
-	**Tr√™n BigQuery:** d√πng view/row-level masking n·∫øu c·∫ßn (phase sau).

### Governance/Provenance t·ª´ng b∆∞·ªõc
1.	**Giai ƒëo·∫°n ƒë·∫ßu:**
-	Metadata chu·∫©n ho√°: schema, owner, SLA, tags (PII).
2.	**Giai ƒëo·∫°n l√™n cloud:**
-	ƒê∆∞a v√†o Dataplex / Data Catalog cho qu·∫£n tr·ªã d·ªØ li·ªáu & metadata.  Ôøº

---

## 4) ‚ÄúLinh ho·∫°t c√¥ng c·ª•‚Äù (local / codespace / colab)
-	**Local:** lab ch√≠nh, v√¨ c√≥ Spark UI + debug nhanh.
-	**Colab:** ch·ªâ d√πng khi c·∫ßn notebook test nhanh / demo nh·ªè (kh√¥ng ph·ª• thu·ªôc UI).
-	**Codespace:** d√πng khi c·∫ßn git workflow ti·ªán, nh∆∞ng b·∫°n ƒëang b·ªã quota gi·ªõi h·∫°n ‚Üí m√¨nh ∆∞u ti√™n local + colab tr∆∞·ªõc.

---

# SETUP GUIDE

---

## LAB #1 ‚Äî Local Spark ‚Äúchu·∫©n ngh·ªÅ‚Äù + Spark UI + t·∫°o data m·∫´u + explain

### AIM
-	Ch·∫°y Spark local m√† xem ƒë∆∞·ª£c Spark UI
-		Bi·∫øt c√°ch b·∫≠t event log ƒë·ªÉ xem l·∫°i tr√™n History Server
- T·∫°o data m·∫´u (customers/orders) ƒë√∫ng ki·ªÉu, tr√°nh l·ªói type nh∆∞ b·∫°n g·∫∑p
-	**L√†m 1 pipeline mini:** `generate ‚Üí transform ‚Üí explain ‚Üí write parquet`

---

## 1) Setup m√¥i tr∆∞·ªùng local (khuy·∫øn ngh·ªã nhanh + √≠t ƒëau ƒë·∫ßu)

### B·∫°n ch·ªçn 1 trong 2 c√°ch:

**C√°ch A (khuy·∫øn ngh·ªã):** Docker Compose (·ªïn ƒë·ªãnh, gi·ªëng production)

**∆Øu:** kh√¥ng l·ªá thu·ªôc Java/Python local; c√≥ s·∫µn Spark UI + History Server

**Nh∆∞·ª£c:** c·∫ßn c√†i Docker Desktop

#### B1 ‚Äî t·∫°o folder

```bash
mkdir -p spark-lab/{data,warehouse,logs,notebooks}
cd spark-lab
```

#### B2 ‚Äî t·∫°o file docker-compose.yml

```yaml
services:
  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./data:/data
      - ./warehouse:/warehouse
      - ./logs:/logs

  spark-worker:
    image: bitnami/spark:latest
    depends_on: [spark-master]
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    ports:
      - "8081:8081"
    volumes:
      - ./data:/data
      - ./warehouse:/warehouse
      - ./logs:/logs

  spark-history:
    image: bitnami/spark:latest
    command: >
      bash -lc "
      mkdir -p /tmp/spark-events &&
      /opt/bitnami/spark/sbin/start-history-server.sh &&
      tail -f /opt/bitnami/spark/logs/*"
    environment:
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/logs/spark-events -Dspark.history.ui.port=18080
    ports:
      - "18080:18080"
    volumes:
      - ./logs:/logs
```

#### B3 ‚Äî ch·∫°y

```bash
docker compose up -d
```

#### B4 ‚Äî m·ªü UI
-	Spark Master UI: http://localhost:8080
-	Spark History UI: http://localhost:18080

> History Server d√πng event log c·ªßa Spark. Spark support event logging v√† History Server theo docs ch√≠nh th·ª©c.

---

## C√°ch B: C√†i native (n·∫øu b·∫°n mu·ªën ‚Äúthu·∫ßn local‚Äù)

---

## 2) T·∫°o notebook / script ch·∫°y Spark (local)

**Option ch·∫°y nhanh:** d√πng spark-submit trong container

T·∫°o file `lab1_generate.py` trong folder `spark/lab/`:

```python
from pathlib import Path
from pyspark.sql import SparkSession,functions as F


EVENTS_DIR = (Path.cwd() / "Users" / "nptan2005" / "SourceCode" / "Python" / "spark-4.0.1-bin-hadoop3" / "logs" / "spark-events").resolve()
EVENTS_URI = f"file://{EVENTS_DIR}"

WH_DIR = (Path.cwd() / "warehouse").resolve()
WH_URI = f"file://{WH_DIR}"


DATA_DIR = (Path.cwd() / "data").resolve()
SILVER_DIR = (DATA_DIR / "silver" / "orders_enriched").resolve()
SILVER_URI = f"file://{SILVER_DIR}"


spark = (
    SparkSession.builder
    .appName("lab1-generate")
    # event log ƒë·ªÉ History Server ƒë·ªçc
    .config("spark.eventLog.enabled", "true")
    .config("spark.eventLog.dir", EVENTS_URI)
    .config("spark.sql.warehouse.dir", WH_URI)
    .getOrCreate()
)

print("Event log dir:", EVENTS_URI)
print("Spark UI:", spark.sparkContext.uiWebUrl)

spark.sparkContext.setLogLevel("WARN")

# -----------------------
# 1) customers (50k)
# -----------------------
n_customers = 50_000
segments = ["MASS", "AFFLUENT", "SME"]
risk = ["LOW", "MED", "HIGH"]

seg_arr = F.array(*[F.lit(x) for x in segments])
risk_arr = F.array(*[F.lit(x) for x in risk])

# element_at c·∫ßn index ki·ªÉu INT (Spark b√°o l·ªói BIGINT nh∆∞ b·∫°n g·∫∑p)
seg_idx = (F.pmod(F.col("id"), F.lit(len(segments))) + F.lit(1)).cast("int")
risk_idx = (F.pmod(F.col("id"), F.lit(len(risk))) + F.lit(1)).cast("int")

customers = (
    spark.range(0, n_customers)
    .select(
        (F.col("id") + 1).cast("string").alias("customer_id"),
        F.element_at(seg_arr, seg_idx).alias("segment"),
        F.element_at(risk_arr, risk_idx).alias("risk_tier"),
        (F.current_date() - F.expr("INTERVAL 1 DAYS") - F.pmod(F.col("id"), F.lit(365)).cast("int")).alias("created_date"),
    )
)

# -----------------------
# 2) orders (2M) + skew hot key
# -----------------------
n_orders = 2_000_000
channels = ["POS", "ECOM", "ATM"]
countries = ["VN", "SG", "TH", "ID", "MY"]
statuses = ["SUCCESS", "FAILED", "REVERSED"]

HOT_CUSTOMER = "1"
HOT_RATIO = 0.25

ch_arr = F.array(*[F.lit(x) for x in channels])
cty_arr = F.array(*[F.lit(x) for x in countries])
st_arr = F.array(*[F.lit(x) for x in statuses])

ch_idx = (F.pmod(F.col("id"), F.lit(len(channels))) + F.lit(1)).cast("int")
cty_idx = (F.pmod(F.col("id"), F.lit(len(countries))) + F.lit(1)).cast("int")
st_idx = (F.pmod(F.col("id"), F.lit(len(statuses))) + F.lit(1)).cast("int")

orders = (
    spark.range(0, n_orders)
    .select(
        (F.col("id") + 1).cast("string").alias("order_id"),
        F.when(F.rand(seed=7) < F.lit(HOT_RATIO), F.lit(HOT_CUSTOMER))
         .otherwise((F.pmod(F.col("id") * 17, F.lit(n_customers - 1)) + 2).cast("string"))
         .alias("customer_id"),
        (F.rand(seed=11) * 5000).cast("double").alias("amount"),
        (F.current_timestamp() - (F.pmod(F.col("id"), F.lit(30)).cast("int") * F.expr("INTERVAL 1 DAYS"))).alias("order_ts"),
        F.element_at(ch_arr, ch_idx).alias("channel"),
        F.element_at(cty_arr, cty_idx).alias("country"),
        F.element_at(st_arr, st_idx).alias("status"),
    )
)

customers.cache()
orders.cache()

print("customers =", customers.count())
print("orders    =", orders.count())

customers.show(5, truncate=False)
orders.show(5, truncate=False)

# -----------------------
# 3) mini pipeline
# -----------------------
# NOTE: c·ªë t√¨nh t·∫°o 1 join c√≥ skew ƒë·ªÉ lab sau x·ª≠ l√Ω
silver = (
    orders.join(customers, "customer_id", "left")
    .withColumn("amount_bucket", F.when(F.col("amount") < 1000, "LOW")
                              .when(F.col("amount") < 3000, "MID")
                              .otherwise("HIGH"))
)

print("\n=== EXPLAIN (formatted) ===")
silver.explain("formatted")

# write parquet
silver.write.mode("overwrite").parquet(SILVER_URI)
print("Wrote:", SILVER_URI)

spark.stop()
```

#### Ch·∫°y job

```bash
docker compose exec spark-master spark-submit /data/../lab1_generate.py
```
N·∫øu b·∫°n ƒë·∫∑t file ·ªü `spark-lab/lab1_generate.py`, th√¨ mount ch∆∞a c√≥. *C√°ch nhanh:* copy file v√†o container ho·∫∑c mount th√™m volume.

**C√°ch d·ªÖ nh·∫•t:** s·ª≠a compose ƒë·ªÉ mount `./:/work` r·ªìi ch·∫°y `spark-submit /work/lab1_generate.py`.



## 3) B·∫°n s·∫Ω quan s√°t g√¨ tr√™n Spark UI (LAB #1)

**Sau khi ch·∫°y:**

1.	M·ªü http://localhost:4040 ‚Üí tab Applications ‚Üí click app lab1-generate
2.	V√†o SQL / DAG Visualization / Stages
3.	B·∫°n s·∫Ω th·∫•y:
-	`count()` t·∫°o jobs
-	`join()` c√≥ kh·∫£ nƒÉng t·∫°o Exchange/shuffle
-	`Event log` ƒë∆∞·ª£c ghi ‚Üí `http://localhost:18080` s·∫Ω xem l·∫°i app sau khi job k·∫øt th√∫c

---

## 4) Mermaid s∆° ƒë·ªì lab (ƒë·ªÉ b·∫°n ƒë∆∞a v√†o doc)

```mermaid
flowchart LR
  A[lab1_generate.py] --> B[Spark Master:8080]
  B --> C[Spark Worker]
  A --> D[(Event Logs)]
  D --> E[Spark History Server :18080]
  A --> F[/data/silver/orders_enriched/]
```

---

## 5) ‚ÄúTh·ª±c t·∫ø c√≥ √°p d·ª•ng numpy/pandas kh√¥ng?‚Äù

**C√≥, nh∆∞ng ƒë√∫ng ch·ªó:**
-	**Pandas/Numpy:** d√πng cho EDA nh·ªè, sampling, ki·ªÉm tra quality nhanh, feature engineering nh·∫π trong notebook.
-	**Spark:** d√πng cho dataset l·ªõn, join/agg n·∫∑ng, pipeline batch/streaming.

Trong LAB #1 n√†y m√¨nh ch·ªß ƒë·ªông ch∆∞a d√πng pandas/numpy ƒë·ªÉ b·∫°n n·∫Øm Spark core tr∆∞·ªõc. Sang LAB #2 m√¨nh s·∫Ω th√™m ‚Äúsampling sang pandas‚Äù m·ªôt c√°ch ƒë√∫ng chu·∫©n (kh√¥ng l√†m ch·∫øt RAM).

---

## 6) Data th·∫≠t cho ch·∫∑ng #2 (E2E)

B·∫°n ch·ªçn NYC TLC l√† ƒë√∫ng: c√≥ data r·∫•t l·ªõn, nhi·ªÅu file theo th√°ng. Trang ch√≠nh th·ª©c c·ªßa TLC cung c·∫•p trip record data v√† h∆∞·ªõng d·∫´n/structure.

---

L·ªô tr√¨nh lab (ƒë√∫ng theo note ‚Äúnh·∫π ‚Üí n·∫∑ng‚Äù)

### Nh·∫π (local-first)
-	**LAB #1** (xong ·ªü tr√™n): setup + UI + generate + explain + parquet
-	**LAB #2:** partitions & shuffle cƒÉn b·∫£n (groupBy, explain, spark.sql.shuffle.partitions)
-	**LAB #3:** join strategies (broadcast / sort-merge) + ƒë·ªçc plan ‚Äúnh∆∞ architect‚Äù
-	**LAB #4:** skew th·∫≠t (HOT key) + AQE + salting
- **LAB #5:** incremental + idempotent output (bank-grade)
-	**LAB #6:** quality checks + logging + metrics (chu·∫©n production)

### N·∫∑ng (data th·∫≠t + E2E on-prem ‚Üí GCP)
-	Download NYC TLC ‚Üí mask/token (m√¥ ph·ªèng SBV policy) ‚Üí upload GCS ‚Üí Dataproc/BigQuery ‚Üí lineage/governance (Dataplex)  Ôøº

---

# Ph√¢n t√≠ch v√† h∆∞·ªõng d·∫´n check

---

## 1) LAB 1 ‚Äî B·∫°n v·ª´a l√†m ƒë∆∞·ª£c g√¨?

### 1.1 Data ƒë√£ t·∫°o ƒë√∫ng nh∆∞ thi·∫øt k·∫ø
-	customers = 50,000
-	orders = 2,000,000
-	Hot key: customer_id = 1 c√≥ 500,428 giao d·ªãch (‚âà 25.02%) ‚Üí skew r√µ r√†ng

ƒê√¢y l√† c√°i ‚Äúm·ªìi‚Äù ƒë·ªÉ c√°c lab sau h·ªçc: join/groupBy g·∫∑p skew th√¨ chuy·ªán g√¨ x·∫£y ra.

---

### 1.2 Spark UI b·∫°n th·∫•y l√† ‚Äúƒë√∫ng‚Äù

Spark b√°o UI: http://172.20.10.5:4040

---

### 1.3 V√¨ sao EXPLAIN c·ªßa LAB1 l·∫°i l√† BroadcastHashJoin?

Trong plan:

	‚Ä¢	Orders (2M) l√† b√™n tr√°i
	‚Ä¢	Customers (50k) l√† b√™n ph·∫£i
	‚Ä¢	Spark ch·ªçn BroadcastHashJoin buildRight t·ª©c l√†:
	‚Ä¢	Customers nh·ªè ‚Üí broadcast sang executors
	‚Ä¢	Orders l·ªõn ‚Üí stream qua v√† join v·ªõi b·∫£ng broadcast

‚úÖ ƒê√¢y l√† best practice: dimension nh·ªè (customer) broadcast, fact l·ªõn (orders) kh√¥ng shuffle.

√ù nghƒ©a: LAB1 join ‚Äúnh·∫π‚Äù v√¨ Spark broadcast. B·∫°n ch∆∞a ‚Äúƒëau‚Äù v√¨ skew ·ªü join (v√¨ join kh√¥ng shuffle).

---

### 1.4 Nh∆∞ng skew v·∫´n t·ªìn t·∫°i trong data

Skew kh√¥ng bi·∫øn m·∫•t. N√≥ ch·ªâ ‚Äúch∆∞a g√¢y ƒëau‚Äù ·ªü join ki·ªÉu broadcast.

N√≥ s·∫Ω g√¢y ƒëau khi:

	‚Ä¢	groupBy(customer_id) (nh∆∞ b·∫°n l√†m ·ªü LAB2)
	‚Ä¢	ho·∫∑c join kh√¥ng broadcast (VD: 2 b·∫£ng l·ªõn)

‚∏ª

## 2) LAB 2 ‚Äî B·∫°n v·ª´a h·ªçc ƒë∆∞·ª£c g√¨ qua EXPLAIN?

### 2.1 Top customer check: skew ƒë√∫ng b√†i

B·∫°n th·∫•y:

	‚Ä¢	1 : 500428
	‚Ä¢	c√°c customer kh√°c ~39‚Äì40

=> ƒë√∫ng pattern: 25% v√†o 1 key, ph·∫ßn c√≤n l·∫°i ph√¢n t√°n.

---

### 2.2 EXPLAIN c·ªßa LAB2 c√≥ Exchange ‚Üí ƒë√¢y l√† Shuffle

Plan LAB2:

```code
Scan parquet -> Project(dt) -> HashAggregate(partial) -> Exchange -> HashAggregate(final)
```

ƒêi·ªÉm m·∫•u ch·ªët:

	‚Ä¢	Exchange = shuffle boundary
	‚Ä¢	Nghƒ©a l√† Spark ph·∫£i ‚Äúchia l·∫°i d·ªØ li·ªáu theo key‚Äù ƒë·ªÉ aggregate ƒë√∫ng.

V√† b·∫°n th·∫•y r√µ:

```code
hashpartitioning(..., 200)
```

T·ª©c l√† Spark shuffle th√†nh 200 partitions.

---

### 2.3 V√¨ sao groupBy g√¢y shuffle?

V√¨ aggregation theo key (dt, segment, risk_tier, country, channel) c·∫ßn gom t·∫•t c·∫£ record c√πng key v·ªÅ c√πng reducer.

Spark l√†m 2 phase:

	‚Ä¢	partial aggregate tr∆∞·ªõc shuffle (gi·∫£m data)
	‚Ä¢	shuffle
	‚Ä¢	final aggregate sau shuffle

=> ƒê√¢y l√† ‚Äúc√¥ng th·ª©c s·ªëng‚Äù c·ªßa groupBy.

---

## 3) K·∫øt lu·∫≠n quan tr·ªçng t·ª´ LAB 1‚Äì2

K·∫øt lu·∫≠n A: Broadcast join gi√∫p b·∫°n n√© shuffle

LAB1 join ch·∫°y ‚Äú√™m‚Äù v√¨ dimension nh·ªè ‚Üí broadcast.

K·∫øt lu·∫≠n B: Aggregation lu√¥n c√≥ shuffle (tr·ª´ v√†i case hi·∫øm)

LAB2 groupBy c√≥ Exchange ‚Üí shuffle.

K·∫øt lu·∫≠n C: Skew s·∫Ω ‚Äúgi·∫øt‚Äù khi key skew r∆°i v√†o shuffle

Hi·ªán t·∫°i LAB2 groupBy kh√¥ng theo customer_id, n√™n skew customer_id ch∆∞a l√†m b·∫°n ƒëau.

Nh∆∞ng n·∫øu b·∫°n groupBy theo customer_id ho·∫∑c join 2 b·∫£ng l·ªõn theo customer_id, b·∫°n s·∫Ω th·∫•y:

	‚Ä¢	1 partition ‚Äú√¥m‚Äù 500k records
	‚Ä¢	nhi·ªÅu partition c√≤n l·∫°i √≠t

‚Üí stage ch·∫°y l√¢u v√¨ straggler (1 task k√©o c·∫£ job).

---

## 4) Tr∆∞·ªõc khi ƒëi ti·∫øp: b·∫°n n√™n xem g√¨ tr√™n Spark UI ƒë·ªÉ ‚Äúc·∫£m‚Äù ƒë∆∞·ª£c?

L·∫ßn sau ch·∫°y LAB2 ho·∫∑c l√†m 1 ph√©p groupBy/customer_id:

	1.	SQL / DataFrame tab

	‚Ä¢	xem DAG, c√≥ ‚ÄúExchange‚Äù node kh√¥ng

	2.	Stages tab

	‚Ä¢	click stage c√≥ shuffle
	‚Ä¢	nh√¨n ‚ÄúSummary Metrics‚Äù (task time, input size)

	3.	Executors tab

	‚Ä¢	coi task time c√≥ l·ªách m·∫°nh kh√¥ng (straggler)

---


# Spark History Server 

---

## 1Ô∏è‚É£ Spark UI vs Spark History Server (ph·∫£i ph√¢n bi·ªát r√µ)

Th√†nh ph·∫ßn	D√πng khi n√†o	URL

Spark UI (4040)	Job ƒëang ch·∫°y	http://localhost:4040

Spark History Server	Job ƒë√£ ch·∫°y xong	http://localhost:18080

üëâ B·∫°n c·∫ßn History Server ƒë·ªÉ:

	‚Ä¢	Xem l·∫°i LAB 1, LAB 2 ƒë√£ ch·∫°y xong
	‚Ä¢	ƒê·ªçc DAG, Stage, Task, Shuffle kh√¥ng b·ªã m·∫•t
	‚Ä¢	So s√°nh gi·ªØa c√°c lab

---

## 2Ô∏è‚É£ X√°c nh·∫≠n: b·∫°n ƒë√£ b·∫≠t Event Log ƒë√∫ng (‚úÖ R·∫§T T·ªêT)

Trong code LAB 1 b·∫°n c√≥:

```python
.config("spark.eventLog.enabled", "true")
.config("spark.eventLog.dir", EVENTS_URI)
```


V√† output:

```code
Event log dir: file:///Users/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events
```

üëâ ƒêi·ªÅu n√†y c√≥ nghƒ©a:

	‚Ä¢	Spark ƒë√£ ghi l·∫°i to√†n b·ªô l·ªãch s·ª≠ job
	‚Ä¢	Ch·ªâ c√≤n thi·∫øu History Server ƒë·ªÉ ƒë·ªçc

Ki·ªÉm tra nhanh:

```bash
ls -lah logs/spark-events
```

B·∫°n s·∫Ω th·∫•y file d·∫°ng:

```code
application_170xxxxxx_0001
```

---

## 3Ô∏è‚É£ Kh·ªüi ƒë·ªông Spark History Server (native Mac)

### 3.1 X√°c ƒë·ªãnh SPARK_HOME

```bash
echo $SPARK_HOME
```

N·∫øu r·ªóng (th∆∞·ªùng g·∫∑p tr√™n Mac brew):

```bash
brew --prefix apache-spark
```

V√≠ d·ª• ra:

```code
/opt/homebrew/Cellar/apache-spark/4.0.1/libexec
```

Set t·∫°m cho terminal:

```bash
export SPARK_HOME=/opt/homebrew/Cellar/apache-spark/4.0.1/libexec
```

---

## 3.2 T·∫°o config cho History Server (r·∫•t quan tr·ªçng)

```bash
cp $SPARK_HOME/conf/spark-defaults.conf.template \
   $SPARK_HOME/conf/spark-defaults.conf
```

M·ªü file:

```bash
vim $SPARK_HOME/conf/spark-defaults.conf
```

Th√™m ƒë√∫ng ƒë∆∞·ªùng d·∫´n event log c·ªßa b·∫°n:

```code
spark.eventLog.enabled           true
spark.eventLog.dir               file:///Users/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events
spark.history.fs.logDirectory    file:///Users/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events

```

‚ö†Ô∏è B·∫Øt bu·ªôc file:/// (3 d·∫•u /)

---

### 3.3 Start History Server

```bash
$SPARK_HOME/sbin/start-history-server.sh
```

Ki·ªÉm tra port:

```bash
lsof -nP -iTCP:18080 -sTCP:LISTEN
```

N·∫øu th·∫•y java LISTEN 18080 ‚Üí OK.

---

### 3.4 M·ªü History UI

üëâ Tr√¨nh duy·ªát:

```code
http://localhost:18080
```

B·∫°n s·∫Ω th·∫•y danh s√°ch application:

	‚Ä¢	lab1-generate
	‚Ä¢	spark_lab2 (ho·∫∑c t√™n app b·∫°n ƒë·∫∑t)

---

## 4Ô∏è‚É£ C√°ch ƒë·ªçc LAB 1 trong Spark History (r·∫•t quan tr·ªçng)

### 4.1 Click v√†o LAB 1 application

Xem theo th·ª© t·ª± n√†y:

```code
(A) DAG Visualization
	‚Ä¢	Nh√¨n th·∫•y:
	‚Ä¢	Range ‚Üí Project ‚Üí BroadcastExchange ‚Üí BroadcastHashJoin ‚Üí Write
	‚Ä¢	Hi·ªÉu:
	‚Ä¢	BroadcastHashJoin (dimension nh·ªè)
	‚Ä¢	Kh√¥ng c√≥ shuffle l·ªõn
```

üëâ LAB1 l√† join t·ªëi ∆∞u

---

```code
(B) Stages tab
	‚Ä¢	√çt stage
	‚Ä¢	Kh√¥ng c√≥ stage n√†o ‚Äúk√©o d√†i b·∫•t th∆∞·ªùng‚Äù
```

üëâ Join broadcast = kh√¥ng ƒëau

---


## 5Ô∏è‚É£ C√°ch ƒë·ªçc LAB 2 trong Spark History

### 5.1 DAG Visualization

B·∫°n s·∫Ω th·∫•y r√µ:

```code
Scan parquet
  ‚Üì
Project (dt)
  ‚Üì
HashAggregate (partial)
  ‚Üì
Exchange (SHUFFLE)
  ‚Üì
HashAggregate (final)
```

üëâ Exchange = shuffle boundary

---

### 5.2 Stages tab (quan tr·ªçng nh·∫•t)

	‚Ä¢	T√¨m stage c√≥ ch·ªØ:
	‚Ä¢	ShuffleMapStage
	‚Ä¢	ResultStage
	‚Ä¢	Click v√†o stage shuffle

Quan s√°t:

	‚Ä¢	Task duration c√≥ l·ªách kh√¥ng
	‚Ä¢	Input size / shuffle read

üëâ ƒê√¢y l√† chu·∫©n textbook c·ªßa aggregation

---

## 6Ô∏è‚É£ V√¨ sao b·∫°n PH·∫¢I hi·ªÉu LAB 1‚Äì2 xong m·ªõi l√†m LAB 3?

B·∫°n ƒëang ƒëi ƒë√∫ng th·ª© t·ª± Senior Data Engineer:


1Ô∏è‚É£ LAB 1

‚Üí Join ƒë√∫ng c√°ch (broadcast)

2Ô∏è‚É£ LAB 2

‚Üí Aggregate ‚Üí shuffle l√† g√¨

3Ô∏è‚É£ LAB 3 (s·∫Øp t·ªõi)

‚Üí Skew + shuffle = th·∫£m h·ªça
‚Üí Fix skew b·∫±ng k·ªπ thu·∫≠t th·∫≠t

N·∫øu kh√¥ng hi·ªÉu LAB 1‚Äì2:

	‚Ä¢	LAB 3 s·∫Ω ch·ªâ l√† ‚Äúcopy trick‚Äù
	‚Ä¢	Kh√¥ng hi·ªÉu v√¨ sao n√≥ ch·∫°y nhanh h∆°n

---

# Fix config xem Spark History

---

# A. T·∫°o ƒë√∫ng config cho History Server

## 1) T·∫°o file spark-defaults.conf (n·∫øu ch∆∞a c√≥)

```bash
cd $SPARK_HOME/conf
ls -lah
```

N·∫øu ch∆∞a c√≥ spark-defaults.conf:

```bash
cp spark-defaults.conf.template spark-defaults.conf
```

## 2) M·ªü file v√† th√™m c√°c d√≤ng b·∫Øt bu·ªôc

```bash
nano $SPARK_HOME/conf/spark-defaults.conf
```

Th√™m ·ªü cu·ªëi file (ƒë√∫ng path repo c·ªßa b·∫°n):

```code
spark.eventLog.enabled           true
spark.eventLog.dir               file:///Users/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events
spark.history.fs.logDirectory    file:///Users/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events
spark.history.ui.port            18080
```

L∆∞u √Ω: file:/// 3 d·∫•u /

---

# B. ƒê·∫£m b·∫£o th∆∞ m·ª•c event log t·ªìn t·∫°i (c·ª±c quan tr·ªçng)

```bash
mkdir -p /U```sers/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events
ls -lah /Users/nptan2005/SourceCode/Python/spark401_colab/logs/spark-events
```

B·∫°n ƒë√£ ch·∫°y LAB1/LAB2 r·ªìi n√™n trong n√†y th∆∞·ªùng s·∫Ω c√≥ file ki·ªÉu local-... ho·∫∑c application_....

---

# C. Start History Server + ki·ªÉm tra log

## 1) Start

```bash
$SPARK_HOME/sbin/start-history-server.sh
```

## 2) Ki·ªÉm tra process

```bash
ps aux | grep -i HistoryS```erver | grep -v grep
```

## 3) Ki·ªÉm tra port 18080

```bash
lsof -nP -iTCP:18080 -sTCP:LISTEN
```

N·∫øu v·∫´n tr·ªëng ‚Üí History Server start b·ªã l·ªói. Ta xem log.

## 4) Xem log History Server

Spark s·∫Ω ghi log v√†o: `$SPARK_HOME/logs`

```bash
ls -lah $SPARK_HOME/logs | tail
```

T√¨m file c√≥ ch·ªØ history-server r·ªìi tail:

```bash
tail -n 200 $SPARK_HOME/logs/*history-server*.out
```
üëâ Copy 20‚Äì50 d√≤ng cu·ªëi paste cho m√¨nh l√† m√¨nh ch·ªâ ƒë√∫ng l·ªói ngay.

---

# D. M·ªü History UI

Khi lsof ƒë√£ th·∫•y LISTEN 18080, m·ªü:

	‚Ä¢	http://localhost:18080

---

# E. N·∫øu b·∫°n mu·ªën ‚Äúch·∫°y th·ª≠ History Server b·∫±ng tay‚Äù ƒë·ªÉ th·∫•y l·ªói r√µ h∆°n (debug nhanh)

```code
$SPARK_HOME/bin/spark-class org.apache.spark.deploy.history.HistoryServer
```

N√≥ s·∫Ω in th·∫≥ng l·ªói ra terminal (r·∫•t d·ªÖ b·∫Øt b·ªánh).

---

# Xem Spark History

---

## 1) M·ªü Spark History UI

M·ªü tr√¨nh duy·ªát:

	‚Ä¢	http://localhost:18080

B·∫°n s·∫Ω th·∫•y trang Spark History Server ‚Üí tab Applications.

---

## 2) Mapping: 3 eventlog kia l√† g√¨?

B·∫°n ƒëang c√≥ 3 folder:

```code
	‚Ä¢	...7861xxx (kho·∫£ng 10:17) ‚Üí l·∫ßn ch·∫°y lab1 tr∆∞·ªõc (c√≥ th·ªÉ fail write)
	‚Ä¢	...8194xxx (10:23) ‚Üí LAB1 ch·∫°y OK (b·∫°n ƒë√£ ghi parquet th√†nh c√¥ng)
	‚Ä¢	...8334xxx (10:25) ‚Üí LAB2 (kpi_daily)
```


Trong History UI, b·∫°n s·∫Ω th·∫•y 3 Applications t∆∞∆°ng ·ª©ng theo th·ªùi gian.

üëâ B·∫°n click v√†o app theo th·ªùi gian ƒë·ªÉ m·ªü chi ti·∫øt.

---

## 3) B·∫°n c·∫ßn xem g√¨ trong History ƒë·ªÉ ‚Äúhi·ªÉu Spark‚Äù (LAB1, LAB2)

Khi click v√†o 1 Application, b·∫°n s·∫Ω th·∫•y c√°c m·ª•c ch√≠nh:

### A. Jobs

	‚Ä¢	M·ªói ‚Äúaction‚Äù t·∫°o ra job: count(), show(), write.parquet(), ‚Ä¶
	‚Ä¢	Trong LAB1:
	‚Ä¢	customers.count() l√† 1 job
	‚Ä¢	orders.count() l√† 1 job
	‚Ä¢	silver.write.parquet(...) l√† 1 job (th∆∞·ªùng n·∫∑ng nh·∫•t)

üëâ Click t·ª´ng job ‚Üí xem n√≥ g·ªìm m·∫•y stages, stage n√†o l√¢u.

### B. Stages

	‚Ä¢	M·ªói stage l√† 1 ‚Äúkh√∫c‚Äù c·ªßa DAG.
	‚Ä¢	Trong LAB2 (groupBy KPI) b·∫°n ch·∫Øc ch·∫Øn th·∫•y:
	‚Ä¢	stage map-side partial aggregate
	‚Ä¢	stage shuffle + final aggregate

üëâ Nh√¨n Shuffle Read/Write, Task time ƒë·ªÉ hi·ªÉu ‚Äút·ªën ·ªü ƒë√¢u‚Äù.

### C. SQL / DataFrame tab

ƒê√¢y l√† ph·∫ßn r·∫•t quan tr·ªçng cho lab 1-2 v√¨ b·∫°n d√πng Spark SQL:

	‚Ä¢	LAB1: join orders + customers
	‚Ä¢	LAB2: groupBy dt, segment, risk, country, channel

Trong tab SQL, b·∫°n s·∫Ω th·∫•y:

	‚Ä¢	query plan (DAG)
	‚Ä¢	th·ªùi gian
	‚Ä¢	s·ªë rows
	‚Ä¢	metrics (scan size, shuffle, spilled‚Ä¶)

---

## 4) LAB1 ph√¢n t√≠ch ƒë√∫ng theo k·∫øt qu·∫£ b·∫°n g·ª≠i

B·∫°n g·ª≠i EXPLAIN c·ªßa LAB1 c√≥:

BroadcastHashJoin LeftOuter BuildRight + BroadcastExchange customers

Nghƒ©a l√† Spark ch·ªçn chi·∫øn l∆∞·ª£c:

	‚Ä¢	orders (2M) l√†m ‚Äúleft‚Äù
	‚Ä¢	customers (50k) ƒë·ªß nh·ªè ‚Üí broadcast sang executors
	‚Ä¢	Join s·∫Ω n√© shuffle l·ªõn (r·∫•t t·ªët)

‚úÖ ƒê√¢y l√† l√Ω do LAB1 ch·∫°y nhanh v√† ‚Äú·ªïn‚Äù.

Trong History UI ‚Üí SQL tab, b·∫°n s·∫Ω nh√¨n ra r√µ broadcast.

---

## 5) LAB2 ph√¢n t√≠ch ƒë√∫ng theo k·∫øt qu·∫£ b·∫°n g·ª≠i

EXPLAIN LAB2 c√≥:

```code
	‚Ä¢	HashAggregate ‚Üí Exchange hashpartitioning(..., 200) ‚Üí HashAggregate
```

Nghƒ©a l√†:

1.	aggregate s∆° b·ªô (partial)
2.	shuffle theo keys (dt, segment, risk, country, channel) v·ªÅ 200 partitions
3.	aggregate final

‚úÖ ƒê√¢y ch√≠nh l√† ‚ÄúSpark s·ªëng b·∫±ng shuffle‚Äù, v√† KPI ch√≠nh l√† case ‚Äúshuffle chu·∫©n‚Äù.

Trong History UI, b·∫°n s·∫Ω th·∫•y:

	‚Ä¢	stage c√≥ Shuffle Write
	‚Ä¢	stage sau c√≥ Shuffle Read

---


## 6) Checklist ‚Äúb·∫°n ƒë√£ xong lab 1-2‚Äù khi n√†o?

B·∫°n m·ªü History v√† tr·∫£ l·ªùi ƒë∆∞·ª£c 3 c√¢u n√†y l√† OK:
	1.	LAB1: Job write.parquet c√≥ m·∫•y stages? stage n√†o l√¢u nh·∫•t?
	2.	LAB1: Join strategy l√† g√¨? (Broadcast hay SortMerge?)
	3.	LAB2: C√≥ shuffle kh√¥ng? Shuffle Read/Write ~ bao nhi√™u?

---


# üß† ƒê·ªåC SPARK HISTORY ‚Äì LAB 2 (KPI DAILY)

```code
B·∫°n ƒëang xem SQL / DataFrame ‚Üí Query 2
Th·ªùi gian: 0.8s ‚Äì 2M rows ‚Üí KPI Gold
ƒê√¢y l√† job quan tr·ªçng nh·∫•t c·ªßa LAB 2
```

---

## 1Ô∏è‚É£ T·ªïng quan pipeline (ƒë·ªçc t·ª´ tr√™n xu·ªëng)

Scan parquet (2,000,000 rows)

```code
‚Üí Project
‚Üí HashAggregate (partial)
‚Üí Exchange (shuffle)
‚Üí AQEShuffleRead
‚Üí HashAggregate (final)
‚Üí WriteFiles
```

üëâ ƒê√¢y ch√≠nh l√† m√¥ h√¨nh chu·∫©n c·ªßa Spark aggregation

```code
Map ‚Üí Shuffle ‚Üí Reduce
```

‚∏ª

## 2Ô∏è‚É£ Scan parquet ‚Äì hi·ªÉu ƒë√∫ng t·∫ßng IO

Scan parquet

```code
	‚Ä¢	Files read: 12
	‚Ä¢	Total size: 35.4 MB
	‚Ä¢	Output rows: 2,000,000
	‚Ä¢	Scan time (max): ~59 ms
```

üß† √ù nghƒ©a:

	‚Ä¢	Parquet r·∫•t hi·ªáu qu·∫£
	‚Ä¢	2M rows nh∆∞ng ch·ªâ ~35MB ‚Üí columnar + compression
	‚Ä¢	IO kh√¥ng ph·∫£i bottleneck

üëâ Trong bank:

	‚Ä¢	N·∫øu scan > 5‚Äì10s ‚Üí data layout sai / partition sai / file qu√° nh·ªè

---

## 3Ô∏è‚É£ WholeStageCodegen (1) ‚Äì Spark ‚ÄútƒÉng t·ªëc‚Äù

ColumnarToRow ‚Üí Project ‚Üí HashAggregate (partial)

```code
	‚Ä¢	Aggregation build time: ~4.6s (max 398 ms / task)
	‚Ä¢	Output rows: 1,080 (t·ª´ 2M)
	‚Ä¢	Peak memory: ~3MB
```

üß† √ù nghƒ©a:

	‚Ä¢	ƒê√¢y l√† partial aggregation (map-side)
	‚Ä¢	Spark gom d·ªØ li·ªáu tr∆∞·ªõc khi shuffle
	‚Ä¢	Gi·∫£m d·ªØ li·ªáu shuffle t·ª´ 2M ‚Üí 1,080 rows

üëâ ƒê√¢y l√† l√Ω do Spark s·ªëng
üëâ N·∫øu kh√¥ng c√≥ b∆∞·ªõc n√†y ‚Üí shuffle ch·∫øt ng∆∞·ªùi

---

## 4Ô∏è‚É£ Exchange ‚Äì n∆°i shuffle x·∫£y ra (r·∫•t quan tr·ªçng)

Exchange

```code
	‚Ä¢	Shuffle records written: 1,080
	‚Ä¢	Partitions: 200
	‚Ä¢	Shuffle bytes: ~129 KB
	‚Ä¢	Shuffle write time (max): 74 ms
```

üß† √ù nghƒ©a:

	‚Ä¢	B·∫°n group theo:

```code
dt,``` segment, risk, country, channel
```

	‚Ä¢	Cardinality th·∫•p ‚Üí r·∫•t √≠t group

	‚Ä¢	Shuffle nh·∫π, s·∫°ch, predictable

üëâ Trong bank:

	‚Ä¢	Shuffle > GB ‚Üí ph·∫£i redesign
	‚Ä¢	Shuffle m√† records ‚âà input ‚Üí design fail

---

## 5Ô∏è‚É£ AQEShuffleRead ‚Äì Adaptive Query Execution

AQEShuffleRead

	‚Ä¢	Original partitions: 200
	‚Ä¢	Coalesced to: 1
	‚Ä¢	Partition size: ~137 KB

üß† √ù nghƒ©a:

	‚Ä¢	Spark t·ª± nh·∫≠n ra:

‚ÄúD·ªØ li·ªáu nh·ªè ‚Üí kh√¥ng c·∫ßn 200 tasks‚Äù
	‚Ä¢	AQE g·ªôp l·∫°i ‚Üí 1 partition
	‚Ä¢	Gi·∫£m overhead task scheduling

üëâ ƒê√¢y l√† Spark 3+ / 4 r·∫•t m·∫°nh
üëâ Nh∆∞ng trong bank: kh√¥ng n√™n ph·ª• thu·ªôc AQE m√π qu√°ng

---


## 6Ô∏è‚É£ HashAggregate (final) ‚Äì reduce side
	‚Ä¢	Output rows: 90
	‚Ä¢	Time: 4 ms
	‚Ä¢	Peak memory: 2.2 MB

üß† √ù nghƒ©a:

	‚Ä¢	Final KPI rows r·∫•t √≠t
	‚Ä¢	ƒê√¢y l√† Gold layer chu·∫©n
	‚Ä¢	Kh√¥ng skew, kh√¥ng spill, kh√¥ng OOM

---

## 7Ô∏è‚É£ WriteFiles ‚Äì output Gold
	‚Ä¢	Ghi ra:

```code
data/gold/kpi_daily
```

üëâ B·∫°n ƒë√£:
```code
	‚Ä¢	Silver ‚Üí Gold
	‚Ä¢	Aggregation ƒë√∫ng layer
	‚Ä¢	Output nh·ªè, idempotent
```
---

## ‚úÖ K·∫æT LU·∫¨N LAB 2 (R·∫§T QUAN TR·ªåNG)

LAB 2 ƒë·∫°t chu·∫©n BANK-GRADE

|**Ti√™u ch√≠**|**Tr·∫°ng th√°i**|
|------------|--------------|
|Shuffle	|C√≥, nh∆∞ng nh·ªè & ki·ªÉm so√°t|
|Skew|	‚ùå Kh√¥ng|
|Runtime|	< 1s|
|AQE	|C√≥, h·ª£p l√Ω|
|Gold| design	ƒê√∫ng|
|Cost|	Predictable|

üëâ ƒê√¢y l√† job ƒë∆∞·ª£c ph√©p ch·∫°y h√†ng ng√†y trong bank

---

## üß† SO S√ÅNH NHANH LAB 1 vs LAB 2

|**Lab**|**Pattern**|**Risk**|
|-------|-----------|--------|
|LAB 1|Broadcast Join|	‚ö†Ô∏è sai broadcast ‚Üí OOM|
|LAB 2|Shuffle Aggregate|‚ö†Ô∏è skew keys|

üëâ B·∫°n ƒë√£ th·∫•y t·∫≠n m·∫Øt c·∫£ 2 pattern chu·∫©n c·ªßa Spark

---

