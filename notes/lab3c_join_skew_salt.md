# LAB 3C: JOIN SKEW SALT

#### Code:

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, expr, floor, rand, count as _count

ORDERS_PATH = "data/silver/orders"
CUSTOMERS_PATH = "data/silver/customers"

SALT_N = 16

spark = (
    SparkSession.builder
    .appName("lab3c_join_skew_salt")
    .config("spark.sql.shuffle.partitions", "50")
    .config("spark.sql.autoBroadcastJoinThreshold", "-1")  # √©p shuffle
    .getOrCreate()
)

o = spark.read.parquet(ORDERS_PATH)
c = spark.read.parquet(CUSTOMERS_PATH).select(
    col("customer_id").alias("c_customer_id"),
    col("segment").alias("c_segment")
)

# 1) Salt orders
o_s = o.withColumn("salt", floor(rand(7) * SALT_N).cast("int")).alias("o_s")

# 2) Expand customers
c_exp = c.withColumn("salt", expr(f"explode(sequence(0, {SALT_N-1}))")).alias("c_exp")

# 3) Join on (customer_id, salt)
j = o_s.join(
    c_exp,
    (col("o_s.customer_id") == col("c_exp.c_customer_id")) &
    (col("o_s.salt") == col("c_exp.salt")),
    "left"
)

res = (j.groupBy("c_segment")
         .agg(_count("*").alias("txns")))

res.explain("formatted")
res.show()

spark.stop()
```

#### K·∫øt q·ªßa:

```code
== Physical Plan ==
AdaptiveSparkPlan (16)
+- HashAggregate (15)
   +- Exchange (14)
      +- HashAggregate (13)
         +- Project (12)
            +- SortMergeJoin LeftOuter (11)
               :- Sort (4)
               :  +- Exchange (3)
               :     +- Project (2)
               :        +- Scan parquet  (1)
               +- Sort (10)
                  +- Exchange (9)
                     +- Generate (8)
                        +- Project (7)
                           +- Filter (6)
                              +- Scan parquet  (5)


(1) Scan parquet 
Output [1]: [customer_id#1]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver/orders]
ReadSchema: struct<customer_id:string>

(2) Project
Output [2]: [customer_id#1, cast(FLOOR((rand(7) * 16.0)) as int) AS salt#15]
Input [1]: [customer_id#1]

(3) Exchange
Input [2]: [customer_id#1, salt#15]
Arguments: hashpartitioning(customer_id#1, salt#15, 50), ENSURE_REQUIREMENTS, [plan_id=34]

(4) Sort
Input [2]: [customer_id#1, salt#15]
Arguments: [customer_id#1 ASC NULLS FIRST, salt#15 ASC NULLS FIRST], false, 0

(5) Scan parquet 
Output [2]: [customer_id#7, segment#8]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver/customers]
PushedFilters: [IsNotNull(customer_id)]
ReadSchema: struct<customer_id:string,segment:string>

(6) Filter
Input [2]: [customer_id#7, segment#8]
Condition : isnotnull(customer_id#7)

(7) Project
Output [2]: [customer_id#7 AS c_customer_id#11, segment#8 AS c_segment#12]
Input [2]: [customer_id#7, segment#8]

(8) Generate
Input [2]: [c_customer_id#11, c_segment#12]
Arguments: explode(org.apache.spark.sql.catalyst.expressions.UnsafeArrayData@f1dd012b), [c_customer_id#11, c_segment#12], false, [salt#17]

(9) Exchange
Input [3]: [c_customer_id#11, c_segment#12, salt#17]
Arguments: hashpartitioning(c_customer_id#11, salt#17, 50), ENSURE_REQUIREMENTS, [plan_id=35]

(10) Sort
Input [3]: [c_customer_id#11, c_segment#12, salt#17]
Arguments: [c_customer_id#11 ASC NULLS FIRST, salt#17 ASC NULLS FIRST], false, 0

(11) SortMergeJoin
Left keys [2]: [customer_id#1, salt#15]
Right keys [2]: [c_customer_id#11, salt#17]
Join type: LeftOuter
Join condition: None

(12) Project
Output [1]: [c_segment#12]
Input [5]: [customer_id#1, salt#15, c_customer_id#11, c_segment#12, salt#17]

(13) HashAggregate
Input [1]: [c_segment#12]
Keys [1]: [c_segment#12]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#31L]
Results [2]: [c_segment#12, count#32L]

(14) Exchange
Input [2]: [c_segment#12, count#32L]
Arguments: hashpartitioning(c_segment#12, 50), ENSURE_REQUIREMENTS, [plan_id=42]

(15) HashAggregate
Input [2]: [c_segment#12, count#32L]
Keys [1]: [c_segment#12]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#30L]
Results [2]: [c_segment#12, count(1)#30L AS txns#18L]

(16) AdaptiveSparkPlan
Output [2]: [c_segment#12, txns#18L]
Arguments: isFinalPlan=false


+---------+-------+                                                             
|c_segment|   txns|
+---------+-------+
|     MASS|1000032|
|      SME| 500092|
| AFFLUENT| 499876|
+---------+-------+
```

---

ƒê√¢y l√† k·ªπ thu·∫≠t **Salting** ‚Äì "v≈© kh√≠" h·∫°ng n·∫∑ng ƒë·ªÉ x·ª≠ l√Ω Skew khi c·∫£ hai b·∫£ng ƒë·ªÅu qu√° l·ªõn kh√¥ng th·ªÉ Broadcast. B·∫°n ƒë√£ th·ª±c hi·ªán r·∫•t chu·∫©n x√°c c√°c b∆∞·ªõc ƒë·ªÉ chia nh·ªè "n√∫t th·∫Øt c·ªï chai".

D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt c√°ch thu·∫≠t to√°n n√†y v·∫≠n h√†nh b√™n d∆∞·ªõi Spark:

---

### 1. Ph√¢n t√≠ch K·ªπ thu·∫≠t Salting (Gi·∫£i m√£ Code)

√ù t∆∞·ªüng c·ªët l√µi l√† **"Chia ƒë·ªÉ tr·ªã"**:

1. **`o_s` (Salt orders)**: B·∫°n th√™m m·ªôt c·ªôt `salt` ng·∫´u nhi√™n t·ª´  ƒë·∫øn  cho m·ªói d√≤ng ƒë∆°n h√†ng.
* *K·∫øt qu·∫£:* 500,428 d√≤ng c·ªßa kh√°ch h√†ng ID `1` tr∆∞·ªõc ƒë√¢y d·ªìn v√†o 1 ch·ªó, nay ƒë∆∞·ª£c chia ƒë·ªÅu ra kho·∫£ng 16 "t√∫i" nh·ªè (m·ªói t√∫i ~31,000 d√≤ng).


2. **`c_exp` (Expand customers)**: B·∫°n d√πng `explode(sequence(0, 15))`.
* *K·∫øt qu·∫£:* M·ªói kh√°ch h√†ng (bao g·ªìm c·∫£ kh√°ch h√†ng ID `1`) s·∫Ω ƒë∆∞·ª£c nh√¢n b·∫£n l√™n 16 l·∫ßn, m·ªói b·∫£n ghi ƒëi k√®m v·ªõi m·ªôt m√£ `salt` t·ª´  ƒë·∫øn .


3. **Join (customer_id + salt)**: B√¢y gi·ªù Spark Join d·ª±a tr√™n c·∫∑p kh√≥a n√†y.
* *K·∫øt qu·∫£:* T√∫i ƒë∆°n h√†ng c√≥ `salt=0` c·ªßa kh√°ch h√†ng `1` s·∫Ω kh·ªõp v·ªõi b·∫£n ghi kh√°ch h√†ng `1` c√≥ `salt=0`. V√¨ d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c b·∫ª nh·ªè, 16 t√∫i n√†y s·∫Ω bay v·ªÅ **16 Task kh√°c nhau** thay v√¨ d·ªìn v√†o 1 Task duy nh·∫•t.



---

### 2. Gi·∫£i m√£ Physical Plan (Explain)

H√£y nh√¨n v√†o nh·ªØng ƒëi·ªÉm kh√°c bi·ªát so v·ªõi b√†i lab tr∆∞·ªõc (ch∆∞a Salt):

* **Node (2) Project**: Xu·∫•t hi·ªán bi·ªÉu th·ª©c `cast(FLOOR((rand(7) * 16.0)) as int) AS salt#15`. Spark t√≠nh to√°n gi√° tr·ªã "mu·ªëi" ngay khi ƒë·ªçc d·ªØ li·ªáu.
* **Node (8) Generate**: ƒê√¢y ch√≠nh l√† b∆∞·ªõc `explode`. B·∫°n s·∫Ω th·∫•y `UnsafeArrayData` ‚Äì ƒë√¢y l√† c√°ch Spark l∆∞u tr·ªØ m·∫£ng  trong b·ªô nh·ªõ tr∆∞·ªõc khi nh√¢n b·∫£n c√°c d√≤ng kh√°ch h√†ng.
* **Node (3) & (9) Exchange**: ƒêi·ªÉm quan tr·ªçng nh·∫•t! `hashpartitioning(customer_id, salt, 50)`.
* Tr∆∞·ªõc ƒë√¢y: Ch·ªâ hash theo `customer_id` -> t·∫•t c·∫£ ID `1` v·ªÅ c√πng 1 ch·ªó.
* B√¢y gi·ªù: Hash theo `(customer_id, salt)` -> ID `1` mu·ªëi `0` s·∫Ω ·ªü partition kh√°c v·ªõi ID `1` mu·ªëi `1`.


* **Node (11) SortMergeJoin**: Ph√©p Join di·ªÖn ra tr√™n 2 keys. T·∫°i Spark UI, b·∫°n s·∫Ω th·∫•y th·ªùi gian ch·∫°y c·ªßa c√°c Task b√¢y gi·ªù **ƒë·ªìng ƒë·ªÅu nhau**. Kh√¥ng c√≤n Task n√†o ch·∫°y "l√™ l·∫øt" 10 ph√∫t trong khi c√°c Task kh√°c xong sau 1 gi√¢y.

---

### 3. S∆° ƒë·ªì Mermaid: Salting Flow

```mermaid
graph TD
    subgraph "B·∫£ng Orders (Ph√¢n t√°n)"
        O1[ID 1] -- "rand(0-15)" --> OS1[ID: 1, Salt: 0]
        O2[ID 1] -- "rand(0-15)" --> OS2[ID: 1, Salt: 1]
        O3[ID 1] -- "rand(0-15)" --> OS3[ID: 1, Salt: 15]
    end

    subgraph "B·∫£ng Customers (Nh√¢n b·∫£n)"
        C1[ID 1] -- "explode" --> CE1[ID: 1, Salt: 0]
        C1 -- "explode" --> CE2[ID: 1, Salt: 1]
        C1 -- "explode" --> CE3[ID: 1, Salt: 15]
    end

    OS1 --- |"Join Task 1"| CE1
    OS2 --- |"Join Task 2"| CE2
    OS3 --- |"Join Task 16"| CE3

```

---

### 4. ƒê√°nh gi√° ƒê√°nh ƒë·ªïi (Trade-off)

K·ªπ thu·∫≠t Salting kh√¥ng ph·∫£i l√† "mi·ªÖn ph√≠", b·∫°n ƒë√£ ƒë·ªïi t√†i nguy√™n l·∫•y th·ªùi gian:

| L·ª£i √≠ch | Chi ph√≠ / ƒê√°nh ƒë·ªïi |
| --- | --- |
| **H·∫øt ngh·∫Ωn (Skew):** Job ch·∫°y ·ªïn ƒë·ªãnh, kh√¥ng b·ªã treo ·ªü 99%. | **TƒÉng dung l∆∞·ª£ng b·∫£ng nh·ªè:** B·∫£ng `customers` b·ªã nh√¢n b·∫£n l√™n 16 l·∫ßn trong b·ªô nh·ªõ. |
| **T·∫≠n d·ª•ng CPU:** T·∫•t c·∫£ c√°c core ƒë·ªÅu l√†m vi·ªác thay v√¨ ch·ªâ 1 core g√°nh team. | **T·ªën CPU h∆°n:** Ph√©p Join ph·ª©c t·∫°p h∆°n v√¨ ph·∫£i so kh·ªõp th√™m c·ªôt `salt`. |

---

### 5. K·∫øt lu·∫≠n & B√†i h·ªçc r√∫t ra

1. **Ch·ªçn SALT_N bao nhi√™u?** Th√¥ng th∆∞·ªùng ch·ªçn b·∫±ng s·ªë l∆∞·ª£ng Core c·ªßa c·ª•m Spark ho·∫∑c m·ªôt b·ªôi s·ªë c·ªßa n√≥ (16, 32, 64). N·∫øu ch·ªçn qu√° l·ªõn, b·∫£ng nh·ªè b·ªã nh√¢n b·∫£n qu√° nhi·ªÅu g√¢y tr√†n RAM.
2. **Ch·ªâ Salt khi c·∫ßn thi·∫øt:** N·∫øu b·∫£ng nh·ªè c√≥ th·ªÉ Broadcast, h√£y d√πng **Broadcast Join** tr∆∞·ªõc v√¨ n√≥ hi·ªáu qu·∫£ h∆°n nhi·ªÅu. Ch·ªâ d√πng Salting khi c·∫£ 2 b·∫£ng ƒë·ªÅu kh·ªïng l·ªì v√† b·ªã l·ªách d·ªØ li·ªáu.
3. **K·∫øt qu·∫£ nh·∫•t qu√°n:** Nh∆∞ b·∫°n th·∫•y, `txns` c·ªßa `MASS` v·∫´n l√† `1,000,032`. Thu·∫≠t to√°n ch·ªâ thay ƒë·ªïi c√°ch t√≠nh, kh√¥ng thay ƒë·ªïi b·∫£n ch·∫•t d·ªØ li·ªáu.

---

# Bonus:

ƒê√¢y l√† m·ªôt c√¢u h·ªèi r·∫•t hay! ƒê√¢y ch√≠nh l√† "b√≠ thu·∫≠t" c·ªßa k·ªπ thu·∫≠t Salting. ƒê·ªÉ hi·ªÉu t·∫°i sao hai c·ªôt `salt` ·ªü hai b·∫£ng kh√°c nhau l·∫°i c√≥ th·ªÉ kh·ªõp ƒë∆∞·ª£c v·ªõi nhau, b·∫°n h√£y t∆∞·ªüng t∆∞·ª£ng ƒë√¢y l√† m·ªôt tr√≤ ch∆°i **"Chia b√†i v√†o c√°c t√∫i"**.

H√£y c√πng b√≥c t√°ch t·ª´ng ph·∫ßn:

---

### 1. Ph√¢n t√≠ch c√∫ ph√°p `floor(rand(7) * 16)`

C√∫ ph√°p n√†y d√πng ƒë·ªÉ t·∫°o ra m·ªôt **s·ªë nguy√™n ng·∫´u nhi√™n** trong kho·∫£ng t·ª´  ƒë·∫øn .

* **`rand(7)`**: H√†m n√†y t·∫°o ra m·ªôt s·ªë th·ª±c ng·∫´u nhi√™n () n·∫±m trong kho·∫£ng .
* *S·ªë 7 ·ªü trong ngo·∫∑c (`seed`) gi√∫p k·∫øt qu·∫£ ng·∫´u nhi√™n n√†y c√≥ th·ªÉ t√°i l·∫≠p ƒë∆∞·ª£c n·∫øu b·∫°n ch·∫°y l·∫°i code.*


* **`* 16`**: Nh√¢n s·ªë ng·∫´u nhi√™n ƒë√≥ v·ªõi 16. K·∫øt qu·∫£ s·∫Ω l√† m·ªôt s·ªë th·ª±c n·∫±m trong kho·∫£ng . (V√≠ d·ª•:  ho·∫∑c ).
* **`floor(...)`**: H√†m l·∫•y ph·∫ßn nguy√™n b√™n d∆∞·ªõi (l√†m tr√≤n xu·ªëng).
*  tr·ªü th√†nh .
*  tr·ªü th√†nh .


* **`.cast("int")`**: Chuy·ªÉn ki·ªÉu d·ªØ li·ªáu v·ªÅ s·ªë nguy√™n ƒë·ªÉ Join cho nhanh.

---

### 2. C∆° ch·∫ø "Kh·ªõp n·ªëi" (T·∫°i sao Join ƒë∆∞·ª£c?)

B·∫°n th·∫Øc m·∫Øc l√†m sao hai c·ªôt `salt` n√†y ƒë·ªìng nh·∫•t? C√¢u tr·∫£ l·ªùi l√†: **Ch√∫ng kh√¥ng c·∫ßn ƒë·ªìng nh·∫•t m·ªôt c√°ch ng·∫´u nhi√™n, m√† ch√∫ng ta ch·ªß ƒë·ªông t·∫°o ra m·ªçi kh·∫£ nƒÉng c√≥ th·ªÉ ƒë·ªÉ ch√∫ng "va" v√†o nhau.**

#### T·∫°i b·∫£ng Orders (B·∫£ng b·ªã l·ªách d·ªØ li·ªáu)

Ch√∫ng ta tung x√∫c x·∫Øc cho t·ª´ng d√≤ng ƒë∆°n h√†ng.

* ƒê∆°n h√†ng A c·ªßa kh√°ch h√†ng ID 1 r∆°i v√†o t√∫i (salt) s·ªë 5.
* ƒê∆°n h√†ng B c·ªßa kh√°ch h√†ng ID 1 r∆°i v√†o t√∫i (salt) s·ªë 12.
* Vi·ªác n√†y gi√∫p **chia nh·ªè** 500.000 d√≤ng c·ªßa kh√°ch h√†ng ID 1 ra 16 t√∫i kh√°c nhau.

#### T·∫°i b·∫£ng Customers (B·∫£ng nh·ªè h∆°n/B·∫£ng danh m·ª•c)

Ch√∫ng ta kh√¥ng d√πng `rand()`. Thay v√†o ƒë√≥, ch√∫ng ta d√πng `explode(sequence(0, 15))`.
L·ªánh n√†y bi·∫øn **1 d√≤ng** kh√°ch h√†ng ID 1 th√†nh **16 d√≤ng** kh√°ch h√†ng ID 1.

* D√≤ng 1: ID 1, Salt 0
* D√≤ng 2: ID 1, Salt 1
* ...
* D√≤ng 16: ID 1, Salt 15

#### Khi th·ª±c hi·ªán Join:

Khi b·∫°n Join theo ƒëi·ªÅu ki·ªán: `(o.id == c.id) AND (o.salt == c.salt)`:

* ƒê∆°n h√†ng A (ID 1, Salt 5) s·∫Ω t√¨m th·∫•y d√≤ng kh√°ch h√†ng (ID 1, Salt 5) trong 16 d√≤ng ƒë√£ nh√¢n b·∫£n. **Kh·ªõp!**
* ƒê∆°n h√†ng B (ID 1, Salt 12) s·∫Ω t√¨m th·∫•y d√≤ng kh√°ch h√†ng (ID 1, Salt 12). **Kh·ªõp!**

**K·∫øt qu·∫£:** M·ªçi ƒë∆°n h√†ng ƒë·ªÅu t√¨m th·∫•y "b·∫£n sao" kh√°ch h√†ng t∆∞∆°ng ·ª©ng c·ªßa n√≥, nh∆∞ng quan tr·ªçng l√† c√°c ƒë∆°n h√†ng c√≥ `salt` kh√°c nhau s·∫Ω ƒë∆∞·ª£c Spark ƒë·∫©y v·ªÅ c√°c **m√°y th·ª£ (Tasks) kh√°c nhau** ƒë·ªÉ x·ª≠ l√Ω song song.

---

### 3. S∆° ƒë·ªì minh h·ªça tr·ª±c quan

H√£y nh√¨n v√†o c√°ch kh√°ch h√†ng ID 1 ƒë∆∞·ª£c x·ª≠ l√Ω:

| B·∫£ng Orders (Sau khi Salt) |  | B·∫£ng Customers (Sau khi Explode) | K·∫øt qu·∫£ Join |
| --- | --- | --- | --- |
| ƒê∆°n h√†ng #101 (ID:1, **Salt:0**) |  | Kh√°ch h√†ng (ID:1, **Salt:0**) | Kh·ªõp t·∫°i Task 0 |
| ƒê∆°n h√†ng #102 (ID:1, **Salt:1**) |  | Kh√°ch h√†ng (ID:1, **Salt:1**) | Kh·ªõp t·∫°i Task 1 |
| ... |  | ... | ... |
| ƒê∆°n h√†ng #500k (ID:1, **Salt:15**) |  | Kh√°ch h√†ng (ID:1, **Salt:15**) | Kh·ªõp t·∫°i Task 15 |

---

### t√≥m l·∫°i:

* B·∫£ng **Orders**: D√πng `rand()` ƒë·ªÉ **ph√¢n t√°n** d·ªØ li·ªáu b·ªã l·ªách.
* B·∫£ng **Customers**: D√πng `explode()` ƒë·ªÉ **ph·ªß k√≠n** t·∫•t c·∫£ c√°c gi√° tr·ªã salt c√≥ th·ªÉ c√≥, ƒë·∫£m b·∫£o d√π ƒë∆°n h√†ng r∆°i v√†o "mu·ªëi" n√†o c≈©ng t√¨m th·∫•y kh√°ch h√†ng ·ªü "mu·ªëi" ƒë√≥.
* ƒê√¢y **kh√¥ng ph·∫£i ch·ªâ l√† test**, ƒë√¢y l√† k·ªπ thu·∫≠t x·ª≠ l√Ω d·ªØ li·ªáu th·ª±c t·∫ø khi b·∫°n ƒë·ªëi m·∫∑t v·ªõi c√°c Key c·ª±c kh·ªßng (v√≠ d·ª•: kh√°ch h√†ng "Guest" trong c√°c h·ªá th·ªëng th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠).

**B·∫°n ƒë√£ th·∫•y c√°ch "nh√¢n b·∫£n" b·∫£ng nh·ªè ƒë·ªÉ "chia l·ª≠a" cho b·∫£ng l·ªõn ch∆∞a? B·∫°n c√≥ mu·ªën t√¥i gi·∫£i th√≠ch th√™m v·ªÅ vi·ªác ch·ªçn con s·ªë SALT_N bao nhi√™u l√† t·ªëi ∆∞u kh√¥ng?**

---

D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch 3 ƒëo·∫°n test c·ªßa b·∫°n (skew check ‚Üí join shuffle th∆∞·ªùng ‚Üí join shuffle c√≥ salting). M√¨nh vi·∫øt Markdown nh∆∞ b·∫°n y√™u c·∫ßu.

‚∏ª

1) Skew check: groupBy(customer_id).count().orderBy(desc(count)).limit(20)

K·∫øt qu·∫£ b·∫°n th·∫•y
	‚Ä¢	customer_id = 1 c√≥ 500,428 records.
	‚Ä¢	C√°c customer_id kh√°c ch·ªâ ~ 38‚Äì40 records.

‚úÖ ƒê√¢y l√† skew c·ª±c n·∫∑ng (m·ªôt key chi·∫øm ~25% t·ªïng 2,000,000 orders).

Plan

B·∫°n th·∫•y:
	‚Ä¢	HashAggregate ‚Üí Exchange hashpartitioning(customer_id, 50) ‚Üí HashAggregate ‚Üí TakeOrderedAndProject

Gi·∫£i th√≠ch:
	‚Ä¢	Exchange l√† b·∫Øt bu·ªôc v√¨ groupBy(customer_id) c·∫ßn gom t·∫•t c·∫£ b·∫£n ghi c√πng key v·ªÅ c√πng partition (shuffle).
	‚Ä¢	TakeOrderedAndProject l√† do b·∫°n l·∫•y top 20.

üëâ ƒêo·∫°n n√†y x√°c nh·∫≠n dataset c·ªßa b·∫°n ƒë·ªß ‚Äúskew‚Äù ƒë·ªÉ test Lab 3 (r·∫•t t·ªët).

‚∏ª

2) Join shuffle th∆∞·ªùng (kh√¥ng salting)

Plan b·∫°n th·∫•y (r√∫t g·ªçn)
	‚Ä¢	Scan orders
	‚Ä¢	Exchange hashpartitioning(customer_id, 50)
	‚Ä¢	Sort
	‚Ä¢	Scan customers
	‚Ä¢	Exchange hashpartitioning(c_customer_id, 50)
	‚Ä¢	Sort
	‚Ä¢	SortMergeJoin
	‚Ä¢	groupBy(segment).count() (l·∫°i c√≥ Exchange v√¨ aggregate)

√ù nghƒ©a ch√≠nh

V√¨ sao b·ªã ‚Äúngh·∫Ωn‚Äù khi skew?

V√¨ customer_id=1 s·∫Ω b·ªã hash v√†o 1 partition n√†o ƒë√≥ (v√≠ d·ª• partition #k).
Partition ƒë√≥ s·∫Ω ph·∫£i x·ª≠ l√Ω ~500k rows ‚Üí ch·∫°y l√¢u h∆°n h·∫≥n c√°c partition kh√°c ‚Üí straggler (task k√©o ƒëu√¥i).

D·∫•u hi·ªáu trong UI/History (n·∫øu b·∫°n m·ªü)
	‚Ä¢	·ªû stage join shuffle: s·∫Ω c√≥ 1 task runtime r·∫•t l·ªõn / input records c·ª±c cao.
	‚Ä¢	C√≤n c√°c task kh√°c nh·ªè v√† ch·∫°y nhanh.

üëâ ƒê√¢y l√† baseline ƒë·ªÉ so s√°nh v·ªõi salting.

‚∏ª

3) Join shuffle c√≥ Salting (SALT_N=16)

Plan b·∫°n th·∫•y
	‚Ä¢	Orders: t·∫°o th√™m salt = floor(rand(7)*16)
‚Üí Exchange hashpartitioning(customer_id, salt, 50) ‚Üí Sort
	‚Ä¢	Customers: explode(sequence(0,15)) ƒë·ªÉ nh√¢n b·∫£n
‚Üí Exchange hashpartitioning(c_customer_id, salt, 50) ‚Üí Sort
	‚Ä¢	SortMergeJoin tr√™n (customer_id, salt)

ƒêi·ªÅu salting ‚Äúc·∫£i thi·ªán‚Äù ·ªü ƒë√¢u?

Tr∆∞·ªõc salting
	‚Ä¢	Key skew: (customer_id=1) ‚Üí d·ªìn v√†o 1 partition.

Sau salting
	‚Ä¢	Key skew ƒë∆∞·ª£c ‚Äúb·∫ª‚Äù th√†nh 16 key kh√°c nhau:
	‚Ä¢	(1,0), (1,1), ‚Ä¶, (1,15)
	‚Ä¢	V√¨ v·∫≠y ~500k rows c·ªßa customer=1 s·∫Ω ƒë∆∞·ª£c r·∫£i ra kho·∫£ng 16 nh√≥m ‚Üí load ƒë∆∞·ª£c chia ƒë·ªÅu h∆°n ‚Üí gi·∫£m straggler r√µ r·ªát.

‚úÖ ƒê√¢y ch√≠nh l√† m·ª•c ti√™u c·ªßa salting.

‚∏ª

4) Nh∆∞ng t·∫°i sao output cu·ªëi gi·ªëng nhau?

B·∫°n th·∫•y c·∫£ 2 join (kh√¥ng salt / c√≥ salt) ƒë·ªÅu ra:
	‚Ä¢	MASS: 1,000,032
	‚Ä¢	SME: 500,092
	‚Ä¢	AFFLUENT: 499,876

‚úÖ ƒêi·ªÅu n√†y ƒë√∫ng v√¨ salting ch·ªâ thay ƒë·ªïi c√°ch ph√¢n ph·ªëi ƒë·ªÉ join, kh√¥ng thay ƒë·ªïi logic join.

‚∏ª

5) ƒêi·ªÅu b·∫°n n√™n ki·ªÉm tra th√™m ƒë·ªÉ ‚Äúth·∫•y hi·ªáu qu·∫£‚Äù r√µ r√†ng

Plan text gi·ªëng nhau ch∆∞a ƒë·ªß; hi·ªáu qu·∫£ n·∫±m ·ªü runtime / skew metrics.

B·∫°n n√™n nh√¨n ·ªü Spark UI (Stages c·ªßa Join stage)

So s√°nh 2 case:

Case A: no-salt
	‚Ä¢	C√≥ 1 task:
	‚Ä¢	input records c·ª±c l·ªõn (t·∫ßm 500k+)
	‚Ä¢	duration r·∫•t l√¢u
	‚Ä¢	Nhi·ªÅu task kh√°c r·∫•t nh·ªè

Case B: salted
	‚Ä¢	Thay v√¨ 1 task c·ª±c l·ªõn, b·∫°n s·∫Ω th·∫•y:
	‚Ä¢	nhi·ªÅu task ‚Äúto‚Äù h∆°n m·ª©c trung b√¨nh, nh∆∞ng kh√¥ng c√≤n 1 task qu√° d·ªã.
	‚Ä¢	runtime t·ªïng th·ªÉ th∆∞·ªùng gi·∫£m (t√πy m√°y)

‚∏ª

6) G·ª£i √Ω code ‚Äúƒëo ƒë√∫ng‚Äù (ƒë·ªÉ b·∫°n t·ª± tin qua Lab 3)

N·∫øu b·∫°n mu·ªën ƒëo r√µ, b·∫°n ch·∫°y 2 job v√† l·∫•y Spark UI:
	‚Ä¢	No salt: o.join(c, on="customer_id")...
	‚Ä¢	Salt: nh∆∞ b·∫°n ƒë√£ l√†m

V√† th√™m:
	‚Ä¢	.count() ho·∫∑c .write.mode("overwrite")... ƒë·ªÉ force action (ƒë·∫£m b·∫£o job ch·∫°y ƒë·ªß n·∫∑ng).

‚∏ª

N·∫øu b·∫°n mu·ªën, g·ª≠i m√¨nh:
	‚Ä¢	Spark UI Stage detail (task time / input size) c·ªßa join stage ·ªü 2 case
=> m√¨nh s·∫Ω ch·ªâ ƒë√∫ng ch·ªó ‚Äúskew task‚Äù v√† x√°c nh·∫≠n salting ƒë√£ gi·∫£m skew nh∆∞ th·∫ø n√†o (r·∫•t d·ªÖ nh√¨n).