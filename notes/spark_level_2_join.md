# ðŸš€ Spark Level 2 â€“ BÃ i 2: Filter, Select, WithColumn & Execution Plan

## ðŸŽ¯ Má»¥c tiÃªu bÃ i nÃ y

**Sau bÃ i nÃ y báº¡n sáº½:**
>*	Hiá»ƒu Transformation vs Action
>*	Biáº¿t Spark cÃ³ cháº¡y ngay hay khÃ´ng
>*	Báº¯t Ä‘áº§u cháº¡m vÃ o shuffle (ráº¥t quan trá»ng)

---

## ðŸ§ª BÃ i toÃ¡n

**Dá»¯ liá»‡u giao dá»‹ch:**

|order_id|customer_id|amount|country|
|--------|-----------|------|-------|
|1|C001|120|VN|
|2|C002|80|VN|
|3|C003|200|SG|
|4|C001|50|VN|


---

### ðŸ“Œ BÆ°á»›c 1 â€“ Táº¡o DataFrame:

```python
from pyspark.sql import functions as F

data = [
    (1, "C001", 120, "VN"),
    (2, "C002", 80, "VN"),
    (3, "C003", 200, "SG"),
    (4, "C001", 50, "VN"),
]

columns = ["order_id", "customer_id", "amount", "country"]

df = spark.createDataFrame(data, columns)
df.show()
```
---

### ðŸ“Œ BÆ°á»›c 2 â€“ Transformation (KHÃ”NG cháº¡y ngay)

```python
df_vn = df.filter(df.country == "VN")
df_vn_high = df_vn.filter(df_vn.amount > 100)
```

>â“ Spark Ä‘Ã£ cháº¡y chÆ°a?

>ðŸ‘‰ CHÆ¯A

---

### ðŸ“Œ BÆ°á»›c 3 â€“ Action (Spark báº¯t Ä‘áº§u cháº¡y)

```python
df_vn_high.show()
```

#### ðŸ§  NguyÃªn lÃ½ quan trá»ng

|Loáº¡i|VÃ­ dá»¥|
|----|-----|
|Transformation|filter, select, withColumn|
|Action|show, count, collect|

> ðŸ‘‰ Spark lazy execution

---

### ðŸ“Œ BÆ°á»›c 4 â€“ ThÃªm cá»™t má»›i

```python
df2 = df.withColumn(
    "amount_category",
    F.when(df.amount >= 100, "HIGH").otherwise("LOW")
)

df2.show()
```
---


### ðŸ“Œ BÆ°á»›c 5 â€“ Xem Execution Plan:

```python
df2.explain(True)
```

Result:

```code
== Physical Plan ==
*(1) Project
+- *(1) Scan ExistingRDD
```

> ðŸ‘‰ ChÆ°a cÃ³ shuffle â†’ nháº¹

---

# ðŸš€ Spark Level 2 â€“ BÃ i 3: groupBy, agg & SHUFFLE (cá»‘t lÃµi Spark)

> **ÄÃ¢y lÃ  bÃ i QUAN TRá»ŒNG NHáº¤T trÆ°á»›c khi báº¡n lÃ m Spark tháº­t sá»± trong CDP / Dataproc / EMR**


## ðŸŽ¯ Má»¥c tiÃªu bÃ i nÃ y

**Sau bÃ i nÃ y báº¡n sáº½:**
>*	Hiá»ƒu shuffle lÃ  gÃ¬ (Ä‘Ãºng báº£n cháº¥t)
>*	Biáº¿t vÃ¬ sao groupBy ráº¥t Ä‘áº¯t
>*	Äá»c Ä‘Æ°á»£c execution plan cÃ³ shuffle
>*	Biáº¿t khi nÃ o Spark scale / khi nÃ o cháº¿t

---

## 1ï¸âƒ£ BÃ i toÃ¡n

Dá»¯ liá»‡u order (nhÆ° trÆ°á»›c):

```python
from pyspark.sql import functions as F

data = [
    (1, "C001", 120, "VN"),
    (2, "C002", 80, "VN"),
    (3, "C003", 200, "SG"),
    (4, "C001", 50, "VN"),
    (5, "C002", 70, "SG"),
]

columns = ["order_id", "customer_id", "amount", "country"]
df = spark.createDataFrame(data, columns)
df.show()
```

---

## 2ï¸âƒ£ GroupBy cÆ¡ báº£n

â“ YÃªu cáº§u

ðŸ‘‰ Tá»•ng tiá»n theo customer_id

```python
df_group = df.groupBy("customer_id").agg(
    F.sum("amount").alias("total_amount")
)

df_group.show()
```

---

# ðŸ”¥ STOP â€“ ÄÃ¢y lÃ  lÃºc SHUFFLE xáº£y ra

## 3ï¸âƒ£ Shuffle lÃ  gÃ¬? (Hiá»ƒu Ä‘Ãºng, khÃ´ng mÆ¡ há»“)

### ðŸ§  Äá»‹nh nghÄ©a CHUáº¨N:

**Shuffle** = **Spark** pháº£i di chuyá»ƒn dá»¯ liá»‡u giá»¯a cÃ¡c executor Ä‘á»ƒ gom cÃ¡c key giá»‘ng nhau vá» cÃ¹ng 1 nÆ¡i

**VÃ­ dá»¥:**
>*	Order cá»§a C001 náº±m á»Ÿ partition 1
>*	Order khÃ¡c cá»§a C001 náº±m á»Ÿ partition 5
>> â†’ Spark báº¯t buá»™c pháº£i chuyá»ƒn dá»¯ liá»‡u

>ðŸ‘‰ Network + Disk + Serialize = tá»‘n tÃ i nguyÃªn

### ðŸ§© Minh há»a logic

```code
Partition 1: C001, C002
Partition 2: C003
Partition 3: C001, C002

groupBy(customer_id)
        â†“
Shuffle
        â†“
Partition A: C001
Partition B: C002
Partition C: C003
```
---

## 5ï¸âƒ£ VÃ¬ sao shuffle NGUY HIá»‚M?

|Váº¥n Ä‘á»|Háº­u quáº£|
|------|-------|
|Nhiá»u dá»¯ liá»‡u|Cháº­m|
|Skew key|Executor cháº¿t|
|Network yáº¿u|Timeout|
|Disk cháº­m|Spill|

> ðŸ‘‰ 90% job Spark cháº­m = shuffle kÃ©m kiá»ƒm soÃ¡t

---

## 6ï¸âƒ£ VÃ­ dá»¥ SHUFFLE Tá»† (anti-pattern)

```python
df.groupBy("country", "customer_id").count().show()
```

>âŒ GroupBy nhiá»u cá»™t khÃ´ng cáº§n thiáº¿t

>âŒ Cardinality cao â†’ shuffle náº·ng

---

## 7ï¸âƒ£ Giáº£m shuffle â€“ cÃ¡ch Ä‘áº§u tiÃªn (cÆ¡ báº£n)

### âœ… Chá»‰ groupBy Ä‘Ãºng thá»© cáº§n

```python
df.groupBy("country").sum("amount").show()
```

---

## 8ï¸âƒ£ Kiá»ƒm soÃ¡t sá»‘ partition khi shuffle

```python
# Máº·c Ä‘á»‹nh:

spark.conf.get("spark.sql.shuffle.partitions")

# â†’ thÆ°á»ng lÃ  200 (QUÃ NHIá»€U cho dataset nhá»)
```

```python
# ðŸ”§ Giáº£m xuá»‘ng khi test / small data

spark.conf.set("spark.sql.shuffle.partitions", "4")

# ðŸ‘‰ Cháº¡y láº¡i groupBy vÃ  explain
```

---

# ðŸ§  CÃ‚U Há»ŽI & TRáº¢ Lá»œI

## ðŸ”¹ Spark Lazy Evaluation & Action

### â“ 1. VÃ¬ sao filter() khÃ´ng cháº¡y ngay?

### Tráº£ lá»i:

**filter()** lÃ  **transformation**, **Spark** sá»­ dá»¥ng **lazy evaluation**, nÃªn chÆ°a thá»±c thi ngay mÃ  chá»‰ xÃ¢y dá»±ng logical execution plan.

**Giáº£i thÃ­ch ngáº¯n:**
>*	Spark chÆ°a Ä‘á»c dá»¯ liá»‡u
>*	Chá»‰ ghi nhá»›: â€œkhi nÃ o cáº§n thÃ¬ filter tháº¿ nÃ yâ€

---

### â“ 2. show() khÃ¡c collect() á»Ÿ Ä‘iá»ƒm nÃ o?

### Tráº£ lá»i:

|**show()**|**collect()**|
|----------|-------------|
|LÃ  action|LÃ  action|
|Chá»‰ láº¥y má»™t pháº§n dá»¯ liá»‡u (máº·c Ä‘á»‹nh 20 dÃ²ng)|Láº¥y toÃ n bá»™ dá»¯ liá»‡u|
|An toÃ n vá»›i data lá»›n|Ráº¤T NGUY HIá»‚M vá»›i data lá»›n|
|DÃ¹ng Ä‘á»ƒ debug|Chá»‰ dÃ¹ng khi data ráº¥t nhá»|

#### Káº¿t luáº­n:

> âŒ KhÃ´ng dÃ¹ng collect() trong production

> âœ… DÃ¹ng show(), take(), limit()

---

### â“ 3. Khi nÃ o Spark má»›i thá»±c sá»± Ä‘á»c dá»¯ liá»‡u?

### Tráº£ lá»i:

> Spark chá»‰ thá»±c sá»± Ä‘á»c vÃ  xá»­ lÃ½ dá»¯ liá»‡u khi gáº·p ACTION nhÆ°:
>> show(), count(), collect(), write()

---

## ðŸ”¹ GroupBy & Shuffle:

### â“ 4. VÃ¬ sao groupBy() luÃ´n gÃ¢y shuffle?

>VÃ¬ **Spark** cáº§n gom táº¥t cáº£ cÃ¡c record cÃ³ cÃ¹ng **key** vá» cÃ¹ng má»™t **executor**, nÃªn báº¯t buá»™c pháº£i di chuyá»ƒn dá»¯ liá»‡u giá»¯a cÃ¡c **partition** â†’ gÃ¢y **shuffle**.

>> ðŸ“Œ KhÃ´ng cÃ³ cÃ¡ch nÃ o groupBy mÃ  khÃ´ng shuffle (trá»« vÃ i case ráº¥t Ä‘áº·c biá»‡t).

---

### â“ 5. Exchange trong execution plan nghÄ©a lÃ  gÃ¬?

### Tráº£ lá»i:

**Exchange biá»ƒu thá»‹ giai Ä‘oáº¡n shuffle, nÆ¡i Spark:**

>*	repartition dá»¯ liá»‡u
>*	truyá»n dá»¯ liá»‡u qua network
>*	ghi/Ä‘á»c disk náº¿u cáº§n

>>ðŸ“Œ ÄÃ¢y lÃ  bÆ°á»›c Ä‘áº¯t nháº¥t trong Spark.

---

### â“ 6. VÃ¬ sao spark.sql.shuffle.partitions = 200 nguy hiá»ƒm vá»›i dataset nhá»?

### Tráº£ lá»i:

VÃ¬ **Spark** táº¡o **200 task shuffle**, trong khi dá»¯ liá»‡u ráº¥t Ã­t â†’
**overhead** (task scheduling, network, file) lá»›n hÆ¡n xá»­ lÃ½ dá»¯ liá»‡u.

**ðŸ“Œ Vá»›i data nhá»:**
>*	200 partitions = lÃ£ng phÃ­
>*	Job cháº­m hÆ¡n thay vÃ¬ nhanh

---

### â“ 7. Khi nÃ o shuffle báº¯t buá»™c, khi nÃ o trÃ¡nh Ä‘Æ°á»£c?

### Tráº£ lá»i:

**Shuffle Báº®T BUá»˜C khi:**
>*	groupBy
>*	join (khÃ´ng broadcast)
>*	distinct
>*	orderBy

**Shuffle CÃ“ THá»‚ TRÃNH khi:**
>*	filter
>*	select
>*	withColumn
>*	map
>*	limit
>*	broadcast join





