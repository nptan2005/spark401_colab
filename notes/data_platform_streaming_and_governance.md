# üß† Oracle ‚Üí Spark (PySpark) Cheat Sheet

---

# üèóÔ∏è Data Platform Track (Streaming + Governance ‚Äì Production Mindset)

T·ª´ ph·∫ßn n√†y tr·ªü ƒëi, n·ªôi dung **ƒëi song song Streaming + Governance**, ƒë√∫ng m√¥ h√¨nh **Data Platform ng√¢n h√†ng / enterprise**.

T√†i li·ªáu n√†y gi√∫p **chuy·ªÉn t∆∞ duy + c√∫ ph√°p t·ª´ Oracle (SQL/PLSQL)** sang **Spark SQL / PySpark**, t·∫≠p trung v√†o **th·ª±c t·∫ø Data Engineering**.

---

## 1Ô∏è‚É£ SELECT / PROJECT

| Oracle                     | Spark SQL                  | PySpark                              |
| -------------------------- | -------------------------- | ------------------------------------ |
| `SELECT col1, col2 FROM t` | `SELECT col1, col2 FROM t` | `df.select("col1", "col2")`          |
| `SELECT *`                 | `SELECT *`                 | `df`                                 |
| Alias                      | `SELECT col1 AS c1`        | `df.select(col("col1").alias("c1"))` |

---

## 2Ô∏è‚É£ WHERE / FILTER

| Oracle         | Spark SQL      | PySpark                             |
| -------------- | -------------- | ----------------------------------- |
| `WHERE a = 1`  | `WHERE a = 1`  | `df.filter(col("a") == 1)`          |
| `BETWEEN`      | `BETWEEN`      | `df.filter(col("a").between(1,10))` |
| `LIKE '%abc%'` | `LIKE '%abc%'` | `df.filter(col("x").like("%abc%"))` |
| `IN (1,2)`     | `IN (1,2)`     | `df.filter(col("a").isin(1,2))`     |

---

## 3Ô∏è‚É£ INSERT / APPEND

| Oracle                     | Spark                                   |                |
| -------------------------- | --------------------------------------- | -------------- |
| `INSERT INTO t SELECT ...` | `df.write.mode("append").parquet(path)` |                |
| Bulk insert                | Distributed write                       | Spark t·ª± scale |

üí° Spark **kh√¥ng c√≥ INSERT t·ª´ng d√≤ng** ‚Üí lu√¥n ghi theo batch.

---

## 4Ô∏è‚É£ UPDATE / DELETE (‚ùó kh√°c Oracle)

| Oracle                       | Spark                    |
| ---------------------------- | ------------------------ |
| `UPDATE t SET a=1 WHERE ...` | ‚ùå Kh√¥ng h·ªó tr·ª£ tr·ª±c ti·∫øp |
| `DELETE FROM t WHERE ...`    | ‚ùå Kh√¥ng                  |

### ‚úÖ Spark pattern (Rewrite Partition)

```python
(df.filter("dt != '2026-01-10'")
 .union(updated_df)
 .write.mode("overwrite").partitionBy("dt").parquet(path))
```

‚û°Ô∏è **UPDATE = ƒë·ªçc ‚Üí bi·∫øn ƒë·ªïi ‚Üí ghi l·∫°i partition**

---

## 5Ô∏è‚É£ MERGE INTO (UPSERT)

### Oracle

```sql
MERGE INTO tgt t
USING src s
ON (t.id = s.id)
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT
```

### Spark (foreachBatch + overwrite partition)

```python
existing = spark.read.parquet(path).filter(col("dt") == dt)
merged = existing.union(src).dropDuplicates(["id"])
merged.write.mode("overwrite").partitionBy("dt").parquet(path)
```

‚û°Ô∏è Th∆∞·ªùng d√πng trong **Streaming / Incremental**

---

## 6Ô∏è‚É£ JOIN

| Oracle            | Spark                        |
| ----------------- | ---------------------------- |
| `INNER JOIN`      | `df.join(df2, "id")`         |
| `LEFT JOIN`       | `df.join(df2, "id", "left")` |
| `/*+ USE_HASH */` | `broadcast(df2)`             |

```python
from pyspark.sql.functions import broadcast
fact.join(broadcast(dim), "id")
```

---

## 7Ô∏è‚É£ UNION / UNION ALL

### ‚ö†Ô∏è Oracle y√™u c·∫ßu ƒë√∫ng th·ª© t·ª± c·ªôt

### Spark (an to√†n h∆°n)

```python
df1.unionByName(df2)
```

‚û°Ô∏è \*\*Lu√¥n d√πng \*\***unionByName** trong production

---

## 8Ô∏è‚É£ GROUP BY / HAVING

| Oracle             | Spark                  |
| ------------------ | ---------------------- |
| `GROUP BY`         | `groupBy()`            |
| `HAVING sum(a)>10` | `.filter(sum("a")>10)` |

```python
df.groupBy("dt") \
  .agg(sum("amount").alias("total")) \
  .filter(col("total") > 1000)
```

---

## 9Ô∏è‚É£ Analytic Functions (OVER PARTITION BY)

| Oracle         | Spark                  |
| -------------- | ---------------------- |
| `ROW_NUMBER()` | `row_number()`         |
| `RANK()`       | `rank()`               |
| `PARTITION BY` | `Window.partitionBy()` |

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

w = Window.partitionBy("customer_id").orderBy(col("event_ts").desc())
df.withColumn("rn", row_number().over(w))
```

---

## üîü LISTAGG ‚Üí collect\_list / collect\_set

| Oracle            | Spark             |
| ----------------- | ----------------- |
| `LISTAGG(x, ',')` | `collect_list(x)` |

```python
from pyspark.sql.functions import collect_list

df.groupBy("order_id") \
  .agg(collect_list("product_name").alias("products"))
```

‚û°Ô∏è Spark tr·∫£ v·ªÅ **ARRAY** (m·∫°nh h∆°n string)

---

## 1Ô∏è‚É£1Ô∏è‚É£ EXPLODE (Ng∆∞·ª£c LISTAGG)

```python
from pyspark.sql.functions import explode

df.select("order_id", explode("products").alias("product"))
```

‚û°Ô∏è Oracle l√†m r·∫•t kh√≥, Spark l√†m c·ª±c t·ªët

---

## 1Ô∏è‚É£2Ô∏è‚É£ STRING / REGEX

| Oracle           | Spark            |
| ---------------- | ---------------- |
| `SUBSTR`         | `substring`      |
| `INSTR`          | `instr`          |
| `REGEXP_LIKE`    | `rlike`          |
| `REGEXP_REPLACE` | `regexp_replace` |

```python
df.withColumn("clean", regexp_replace("raw", "[^0-9]", ""))
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ JSON

| Oracle       | Spark                 |
| ------------ | --------------------- |
| `JSON_VALUE` | `get_json_object`     |
| `JSON_TABLE` | `from_json + explode` |

```python
from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StringType

schema = StructType().add("id", StringType())
df.withColumn("j", from_json("json_col", schema))
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ HIERARCHY (CONNECT BY)

| Oracle             | Spark             |
| ------------------ | ----------------- |
| `CONNECT BY PRIOR` | ‚ùå Kh√¥ng tr·ª±c ti·∫øp |

### Spark pattern

- Iterative self-join
- GraphFrames
- BFS

```python
# parent_id ‚Üí id self join
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ PACKAGE / PROCEDURE / FUNCTION

| Oracle    | Spark                 |
| --------- | --------------------- |
| PACKAGE   | Python module         |
| PROCEDURE | PySpark job           |
| FUNCTION  | Python function / UDF |

```python
def calc_fee(amount):
    return amount * 0.01
```

---

## üß† T∆∞ duy quan tr·ªçng

| Oracle Mindset | Spark Mindset          |
| -------------- | ---------------------- |
| Row-based      | Columnar               |
| Update/Delete  | Rewrite                |
| Stateful       | Stateless + checkpoint |
| Single DB      | Distributed compute    |

---

## ‚úÖ Checklist khi chuy·ªÉn Oracle ‚Üí Spark

-

---

---

## 1Ô∏è‚É£ Data Streaming chuy√™n nghi·ªáp (Oracle ‚Üí Spark)

### 1.1 Oracle background (Batch / CDC)

Trong Oracle:

- OLTP ghi t·ª´ng row
- ETL batch ch·∫°y theo gi·ªù/ng√†y
- CDC d√πng GoldenGate / trigger / logminer

```sql
INSERT INTO orders VALUES (...);
COMMIT;
```

üëâ **Stateful, row-based**

---

### 1.2 Spark Streaming mindset

Trong Spark:

- D·ªØ li·ªáu **append-only**
- X·ª≠ l√Ω theo **micro-batch**
- State qu·∫£n l√Ω b·∫±ng **checkpoint + watermark**

```text
Source (Kafka / File / PubSub)
  ‚Üí Bronze (raw)
    ‚Üí Silver (dedup + clean)
      ‚Üí Gold (KPI)
```

---

### 1.3 Bronze ‚Üí Silver (Streaming Upsert th·ª±c t·∫ø)

#### Oracle MERGE INTO

```sql
MERGE INTO orders tgt
USING orders_src src
ON (tgt.order_id = src.order_id)
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT;
```

#### Spark equivalent (foreachBatch)

```python
def upsert(batch_df, batch_id):
    dedup = batch_df.dropDuplicates(["order_id"])
    (dedup
        .write
        .mode("overwrite")
        .partitionBy("dt")
        .parquet(SILVER_PATH))
```

üí° **Key idea**:

- Spark kh√¥ng update row
- Rewrite partition theo `dt`

---

### 1.4 Late data & Watermark

```python
stream
  .withWatermark("event_ts", "3 days")
  .dropDuplicates(["order_id", "event_ts"])
```

| Oracle           | Spark      |
| ---------------- | ---------- |
| Commit time      | Event time |
| Kh√¥ng x·ª≠ l√Ω late | Watermark  |

---

### 1.5 Streaming tuning (Production)

| V·∫•n ƒë·ªÅ    | Gi·∫£i ph√°p                     |
| --------- | ----------------------------- |
| Batch lag | tƒÉng trigger interval         |
| OOM       | gi·∫£m partition, tr√°nh count() |
| Duplicate | idempotent write              |

---

## 2Ô∏è‚É£ Data Governance / Provenance (Enterprise)

### 2.1 Oracle governance truy·ªÅn th·ªëng

- Data dictionary
- DB audit
- Manual lineage

---

### 2.2 Spark-native Governance pattern

#### a) Job Run Log (Provenance)

```text
run_id | job_name | input_rows | output_rows | status
```

üëâ b·∫°n ƒë√£ implement `log_job_run()` ‚úÖ

---

#### b) Schema Registry

```python
snapshot_schema(spark, df, "gold.kpi_daily", path)
```

L∆∞u:

- version
- schema\_json
- created\_at

---

#### c) Lineage logic

```text
bronze.orders_raw
  ‚Üí silver.orders_fact_dt_stream
    ‚Üí gold.kpi_daily
```

---

### 2.3 C√¥ng c·ª• th·ª±c t·∫ø (GCP / OSS)

| Tool               | M·ª•c ƒë√≠ch     |
| ------------------ | ------------ |
| Data Catalog       | Metadata     |
| Dataplex           | Governance   |
| OpenLineage        | Lineage      |
| Marquez            | Lineage UI   |
| Great Expectations | Data Quality |

---

## üß† T·ªïng k·∫øt t∆∞ duy

| Oracle     | Spark Platform   |
| ---------- | ---------------- |
| DB-centric | Pipeline-centric |
| Procedure  | Job              |
| Commit     | Checkpoint       |
| Update     | Rewrite          |

---

## üîú Ti·∫øp theo

- Streaming ‚Üí Gold aggregation
- CDC simulation
- Data Quality rules
- SLA / SLO







---



# **üöÄ Kafka end-to-end lab (Docker ‚Üí Kafka UI ‚Üí Produce JSON ‚Üí Spark Streaming ‚Üí Silver)**





## **üéØ M·ª•c ti√™u th·ª±c t·∫ø**



\


B·∫°n s·∫Ω l√†m ƒë√∫ng pipeline ‚ÄúData Platform‚Äù ki·ªÉu production (mini):



- **Kafka (Bronze streaming source)**: nh·∫≠n message JSON t·ª´ng s·ª± ki·ªán
- **Spark Structured Streaming**: parse JSON + watermark + dedup
- **Silver (Parquet partition dt)**: d·ªØ li·ªáu s·∫°ch, query nhanh theo ng√†y
- **Kafka UI**: nh√¨n th·∫•y topic, message, lag (tr·ª±c quan



##



## **0) Checklist tr∆∞·ªõc khi ch·∫°y**





- Docker Desktop ch·∫°y OK
- Port kh√¥ng b·ªã chi·∫øm:¬†9094,¬†8089¬†(Kafka UI),¬†4040+¬†(Spark UI)
- Python env:¬†cdp\_env¬†c√≥ PySpark 4.0.1



## **1) Docker Compose Kafka + Kafka UI (ƒë√£ fix l·ªói image)**



\




### **‚úÖ L·ªói b·∫°n g·∫∑p**



\


bitnami/kafka:3.7¬†‚Üí¬†**manifest not found**¬†(tag kh√¥ng t·ªìn t·∫°i / kh√¥ng match ki·∫øn tr√∫c)

\




### **‚úÖ Chu·∫©n d√πng (khuy·∫øn ngh·ªã)**



\


D√πng¬†apache/kafka:3.7.2¬†(KRaft mode), c√≥ EXTERNAL listener:¬†9094



```
```
