# ğŸ§  Oracle â†’ Spark (PySpark) Cheat Sheet

---

# ğŸ—ï¸ Data Platform Track (Streaming + Governance â€“ Production Mindset)

Tá»« pháº§n nÃ y trá»Ÿ Ä‘i, ná»™i dung **Ä‘i song song Streaming + Governance**, Ä‘Ãºng mÃ´ hÃ¬nh **Data Platform ngÃ¢n hÃ ng / enterprise**.

TÃ i liá»‡u nÃ y giÃºp **chuyá»ƒn tÆ° duy + cÃº phÃ¡p tá»« Oracle (SQL/PLSQL)** sang **Spark SQL / PySpark**, táº­p trung vÃ o **thá»±c táº¿ Data Engineering**.

---

## 1ï¸âƒ£ SELECT / PROJECT

| Oracle                     | Spark SQL                  | PySpark                              |
| -------------------------- | -------------------------- | ------------------------------------ |
| `SELECT col1, col2 FROM t` | `SELECT col1, col2 FROM t` | `df.select("col1", "col2")`          |
| `SELECT *`                 | `SELECT *`                 | `df`                                 |
| Alias                      | `SELECT col1 AS c1`        | `df.select(col("col1").alias("c1"))` |

---

## 2ï¸âƒ£ WHERE / FILTER

| Oracle         | Spark SQL      | PySpark                             |
| -------------- | -------------- | ----------------------------------- |
| `WHERE a = 1`  | `WHERE a = 1`  | `df.filter(col("a") == 1)`          |
| `BETWEEN`      | `BETWEEN`      | `df.filter(col("a").between(1,10))` |
| `LIKE '%abc%'` | `LIKE '%abc%'` | `df.filter(col("x").like("%abc%"))` |
| `IN (1,2)`     | `IN (1,2)`     | `df.filter(col("a").isin(1,2))`     |

---

## 3ï¸âƒ£ INSERT / APPEND

| Oracle                     | Spark                                   |                |
| -------------------------- | --------------------------------------- | -------------- |
| `INSERT INTO t SELECT ...` | `df.write.mode("append").parquet(path)` |                |
| Bulk insert                | Distributed write                       | Spark tá»± scale |

ğŸ’¡ Spark **khÃ´ng cÃ³ INSERT tá»«ng dÃ²ng** â†’ luÃ´n ghi theo batch.

---

## 4ï¸âƒ£ UPDATE / DELETE (â— khÃ¡c Oracle)

| Oracle                       | Spark                    |
| ---------------------------- | ------------------------ |
| `UPDATE t SET a=1 WHERE ...` | âŒ KhÃ´ng há»— trá»£ trá»±c tiáº¿p |
| `DELETE FROM t WHERE ...`    | âŒ KhÃ´ng                  |

### âœ… Spark pattern (Rewrite Partition)

```python
(df.filter("dt != '2026-01-10'")
 .union(updated_df)
 .write.mode("overwrite").partitionBy("dt").parquet(path))
```

â¡ï¸ **UPDATE = Ä‘á»c â†’ biáº¿n Ä‘á»•i â†’ ghi láº¡i partition**

---

## 5ï¸âƒ£ MERGE INTO (UPSERT)

### Oracle

```sql
MERGE INTO tgt t
USING src s
ON (t.id = s.id)
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT
```

### Spark (foreachBatch + overwrite partition)

```python
existing = spark.read.parquet(path).filter(col("dt") == dt)
merged = existing.union(src).dropDuplicates(["id"])
merged.write.mode("overwrite").partitionBy("dt").parquet(path)
```

â¡ï¸ ThÆ°á»ng dÃ¹ng trong **Streaming / Incremental**

---

## 6ï¸âƒ£ JOIN

| Oracle            | Spark                        |
| ----------------- | ---------------------------- |
| `INNER JOIN`      | `df.join(df2, "id")`         |
| `LEFT JOIN`       | `df.join(df2, "id", "left")` |
| `/*+ USE_HASH */` | `broadcast(df2)`             |

```python
from pyspark.sql.functions import broadcast
fact.join(broadcast(dim), "id")
```

---

## 7ï¸âƒ£ UNION / UNION ALL

### âš ï¸ Oracle yÃªu cáº§u Ä‘Ãºng thá»© tá»± cá»™t

### Spark (an toÃ n hÆ¡n)

```python
df1.unionByName(df2)
```

â¡ï¸ \*\*LuÃ´n dÃ¹ng \*\***unionByName** trong production

---

## 8ï¸âƒ£ GROUP BY / HAVING

| Oracle             | Spark                  |
| ------------------ | ---------------------- |
| `GROUP BY`         | `groupBy()`            |
| `HAVING sum(a)>10` | `.filter(sum("a")>10)` |

```python
df.groupBy("dt") \
  .agg(sum("amount").alias("total")) \
  .filter(col("total") > 1000)
```

---

## 9ï¸âƒ£ Analytic Functions (OVER PARTITION BY)

| Oracle         | Spark                  |
| -------------- | ---------------------- |
| `ROW_NUMBER()` | `row_number()`         |
| `RANK()`       | `rank()`               |
| `PARTITION BY` | `Window.partitionBy()` |

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

w = Window.partitionBy("customer_id").orderBy(col("event_ts").desc())
df.withColumn("rn", row_number().over(w))
```

---

## ğŸ”Ÿ LISTAGG â†’ collect\_list / collect\_set

| Oracle            | Spark             |
| ----------------- | ----------------- |
| `LISTAGG(x, ',')` | `collect_list(x)` |

```python
from pyspark.sql.functions import collect_list

df.groupBy("order_id") \
  .agg(collect_list("product_name").alias("products"))
```

â¡ï¸ Spark tráº£ vá» **ARRAY** (máº¡nh hÆ¡n string)

---

## 1ï¸âƒ£1ï¸âƒ£ EXPLODE (NgÆ°á»£c LISTAGG)

```python
from pyspark.sql.functions import explode

df.select("order_id", explode("products").alias("product"))
```

â¡ï¸ Oracle lÃ m ráº¥t khÃ³, Spark lÃ m cá»±c tá»‘t

---

## 1ï¸âƒ£2ï¸âƒ£ STRING / REGEX

| Oracle           | Spark            |
| ---------------- | ---------------- |
| `SUBSTR`         | `substring`      |
| `INSTR`          | `instr`          |
| `REGEXP_LIKE`    | `rlike`          |
| `REGEXP_REPLACE` | `regexp_replace` |

```python
df.withColumn("clean", regexp_replace("raw", "[^0-9]", ""))
```

---

## 1ï¸âƒ£3ï¸âƒ£ JSON

| Oracle       | Spark                 |
| ------------ | --------------------- |
| `JSON_VALUE` | `get_json_object`     |
| `JSON_TABLE` | `from_json + explode` |

```python
from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StringType

schema = StructType().add("id", StringType())
df.withColumn("j", from_json("json_col", schema))
```

---

## 1ï¸âƒ£4ï¸âƒ£ HIERARCHY (CONNECT BY)

| Oracle             | Spark             |
| ------------------ | ----------------- |
| `CONNECT BY PRIOR` | âŒ KhÃ´ng trá»±c tiáº¿p |

### Spark pattern

- Iterative self-join
- GraphFrames
- BFS

```python
# parent_id â†’ id self join
```

---

## 1ï¸âƒ£5ï¸âƒ£ PACKAGE / PROCEDURE / FUNCTION

| Oracle    | Spark                 |
| --------- | --------------------- |
| PACKAGE   | Python module         |
| PROCEDURE | PySpark job           |
| FUNCTION  | Python function / UDF |

```python
def calc_fee(amount):
    return amount * 0.01
```

---

## ğŸ§  TÆ° duy quan trá»ng

| Oracle Mindset | Spark Mindset          |
| -------------- | ---------------------- |
| Row-based      | Columnar               |
| Update/Delete  | Rewrite                |
| Stateful       | Stateless + checkpoint |
| Single DB      | Distributed compute    |

---

## âœ… Checklist khi chuyá»ƒn Oracle â†’ Spark

-

---

---

## 1ï¸âƒ£ Data Streaming chuyÃªn nghiá»‡p (Oracle â†’ Spark)

### 1.1 Oracle background (Batch / CDC)

Trong Oracle:

- OLTP ghi tá»«ng row
- ETL batch cháº¡y theo giá»/ngÃ y
- CDC dÃ¹ng GoldenGate / trigger / logminer

```sql
INSERT INTO orders VALUES (...);
COMMIT;
```

ğŸ‘‰ **Stateful, row-based**

---

### 1.2 Spark Streaming mindset

Trong Spark:

- Dá»¯ liá»‡u **append-only**
- Xá»­ lÃ½ theo **micro-batch**
- State quáº£n lÃ½ báº±ng **checkpoint + watermark**

```text
Source (Kafka / File / PubSub)
  â†’ Bronze (raw)
    â†’ Silver (dedup + clean)
      â†’ Gold (KPI)
```

---

### 1.3 Bronze â†’ Silver (Streaming Upsert thá»±c táº¿)

#### Oracle MERGE INTO

```sql
MERGE INTO orders tgt
USING orders_src src
ON (tgt.order_id = src.order_id)
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT;
```

#### Spark equivalent (foreachBatch)

```python
def upsert(batch_df, batch_id):
    dedup = batch_df.dropDuplicates(["order_id"])
    (dedup
        .write
        .mode("overwrite")
        .partitionBy("dt")
        .parquet(SILVER_PATH))
```

ğŸ’¡ **Key idea**:

- Spark khÃ´ng update row
- Rewrite partition theo `dt`

---

### 1.4 Late data & Watermark

```python
stream
  .withWatermark("event_ts", "3 days")
  .dropDuplicates(["order_id", "event_ts"])
```

| Oracle           | Spark      |
| ---------------- | ---------- |
| Commit time      | Event time |
| KhÃ´ng xá»­ lÃ½ late | Watermark  |

---

### 1.5 Streaming tuning (Production)

| Váº¥n Ä‘á»    | Giáº£i phÃ¡p                     |
| --------- | ----------------------------- |
| Batch lag | tÄƒng trigger interval         |
| OOM       | giáº£m partition, trÃ¡nh count() |
| Duplicate | idempotent write              |

---

## 2ï¸âƒ£ Data Governance / Provenance (Enterprise)$1

---

## 2.4 Marquez UI (Lineage) â€“ cháº¡y & debug tá»«ng bÆ°á»›c

### A) Docker Compose máº«u (Ä‘Ãºng image UI)
> UI cá»§a Marquez lÃ  **marquez-web** (khÃ´ng pháº£i marquez-ui).

```yaml
marquez:
  image: marquezproject/marquez:latest
  container_name: marquez-api
  depends_on:
    postgres:
      condition: service_healthy
  environment:
    MARQUEZ_DB_HOST: postgres
    MARQUEZ_DB_PORT: 5432
    MARQUEZ_DB_USER: marquez
    MARQUEZ_DB_PASSWORD: marquez
    MARQUEZ_DB_NAME: marquez
  ports:
    - "5000:5000"

marquez-ui:
  image: marquezproject/marquez-web:0.45.0
  container_name: marquez-ui
  depends_on:
    - marquez
  environment:
    MARQUEZ_HOST: marquez
    MARQUEZ_PORT: 5000
    WEB_PORT: 3000
  ports:
    - "3000:3000"
```

### B) Start láº¡i stack
```bash
docker compose down -v
docker compose pull
docker compose up -d
```

### C) Check health / port
```bash
docker ps
# port 3000 cÃ³ bá»‹ chiáº¿m khÃ´ng?
lsof -nP -iTCP:3000 -sTCP:LISTEN || true
# test API
curl -s http://localhost:5000/api/v1/namespaces | head
```

### D) Debug khi khÃ´ng vÃ o Ä‘Æ°á»£c http://localhost:3000
1) **Xem log UI**
```bash
docker logs -n 200 marquez-ui
```
2) **Xem log API**
```bash
docker logs -n 200 marquez-api
```
3) **Test tá»« trong container UI sang API** (quan trá»ng Ä‘á»ƒ xÃ¡c Ä‘á»‹nh DNS ná»™i bá»™)
```bash
docker exec -it marquez-ui sh -lc "apk add --no-cache curl >/dev/null 2>&1 || true; curl -s http://marquez:5000/api/v1/namespaces | head"
```
4) Náº¿u port 3000 bá»‹ chiáº¿m â†’ Ä‘á»•i port host:
```yaml
ports:
  - "3010:3000"
```
Rá»“i vÃ o `http://localhost:3010`.

### E) LÆ°u Ã½ quan trá»ng
- Trong Docker network, hostname pháº£i lÃ  **tÃªn service** (`marquez`, `postgres`) â€“ khÃ´ng pháº£i `container_name`.
- Náº¿u báº¡n dÃ¹ng `image: marquezproject/marquez-web:latest` mÃ  lá»—i/khÃ´ng lÃªn UI â†’ pin version `0.45.0`.

---

## ğŸ§  Tá»•ng káº¿t tÆ° duy

| Oracle     | Spark Platform   |
| ---------- | ---------------- |
| DB-centric | Pipeline-centric |
| Procedure  | Job              |
| Commit     | Checkpoint       |
| Update     | Rewrite          |

---

## ğŸ”œ Tiáº¿p theo

- Streaming â†’ Gold aggregation
- CDC simulation
- Data Quality rules
- SLA / SLO







---



# **ğŸš€ Kafka end-to-end lab (Docker â†’ Kafka UI â†’ Produce JSON â†’ Spark Streaming â†’ Silver)**





## **ğŸ¯ Má»¥c tiÃªu thá»±c táº¿**



\


Báº¡n sáº½ lÃ m Ä‘Ãºng pipeline â€œData Platformâ€ kiá»ƒu production (mini):



- **Kafka (Bronze streaming source)**: nháº­n message JSON tá»«ng sá»± kiá»‡n
- **Spark Structured Streaming**: parse JSON + watermark + dedup
- **Silver (Parquet partition dt)**: dá»¯ liá»‡u sáº¡ch, query nhanh theo ngÃ y
- **Kafka UI**: nhÃ¬n tháº¥y topic, message, lag (trá»±c quan



##



## **0) Checklist trÆ°á»›c khi cháº¡y**





- Docker Desktop cháº¡y OK
- Port khÃ´ng bá»‹ chiáº¿m:Â 9094,Â 8089Â (Kafka UI),Â 4040+Â (Spark UI)
- Python env:Â cdp\_envÂ cÃ³ PySpark 4.0.1



## **1) Docker Compose Kafka + Kafka UI (Ä‘Ã£ fix lá»—i image)**



\




### **âœ… Lá»—i báº¡n gáº·p**



\


bitnami/kafka:3.7Â â†’Â **manifest not found**Â (tag khÃ´ng tá»“n táº¡i / khÃ´ng match kiáº¿n trÃºc)

\




### **âœ… Chuáº©n dÃ¹ng (khuyáº¿n nghá»‹)**



\


DÃ¹ngÂ apache/kafka:3.7.2Â (KRaft mode), cÃ³ EXTERNAL listener:Â 9094



```
```
