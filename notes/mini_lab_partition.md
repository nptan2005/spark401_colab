
# ğŸ”¥ Partition column + Partition pruning + Shuffle impact

KhÃ´ng cÃ³ skew, khÃ´ng join phá»©c táº¡p, chá»‰ 1 concept nhÆ°ng nhÃ¬n ráº¥t rÃµ trong Spark UI.

---

# ğŸ§ª MINI LAB â€“ PARTITION COLUMN & PARTITION PRUNING

## ğŸ¯ Má»¥c tiÃªu lab nÃ y

Sau lab nÃ y, báº¡n pháº£i tá»± tin tráº£ lá»i Ä‘Æ°á»£c:

1. Partition column khÃ¡c repartition á»Ÿ Ä‘iá»ƒm nÃ o?
2. VÃ¬ sao partition giÃºp IO chá»© khÃ´ng giÃºp shuffle?
3. Partition column dÃ¹ng Ä‘á»ƒ lÃ m gÃ¬ (KHÃ”NG pháº£i repartition)
4. VÃ¬ sao partition Ä‘Ãºng â†’ Spark scan Ã­t file hÆ¡n
5. VÃ¬ sao partition sai â†’ IO cháº¿t, dÃ¹ logic Ä‘Ãºng
6. NhÃ¬n Spark UI biáº¿t partition pruning cÃ³ xáº£y ra hay khÃ´ng

---

## ğŸ§  TÆ¯ DUY TRÆ¯á»šC KHI CODE (Ráº¤T QUAN TRá»ŒNG)

## âŒ Sai láº§m phá»• biáº¿n
-	â€œpartition Ä‘á»ƒ tÄƒng parallelismâ€
-	â€œpartition = repartitionâ€

> ğŸ‘‰ SAI

## âœ… Báº£n cháº¥t Ä‘Ãºng
-	Partition column = tá»‘i Æ°u IO
-	Repartition = tá»‘i Æ°u compute

---

## ğŸ“¦ DATA DÃ™NG Láº I

Ta dÃ¹ng láº¡i:

```code
data/silver/orders_enriched
```

Schema:

```code
order_id
customer_id
amount
order_ts
channel
country
segment
risk_tier
created_date
```

---

## ğŸ”¬ STEP 1 â€“ GHI DATA KHÃ”NG PARTITION (baseline)

### Code (lab2a_no_partition.py)

```python
from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("lab2_no_partition")
    .getOrCreate()
)

df = spark.read.parquet("data/silver/orders_enriched")

(
    df
    .write
    .mode("overwrite")
    .parquet("data/silver_np/orders")
)

print("DONE: no partition")
spark.stop()
```

### ğŸ‘‰ Cháº¡y:

```bash
python spark/lab/lab2a_no_partition.py
```

---

## ğŸ” STEP 2 â€“ QUERY DATA KHÃ”NG PARTITION

```python
df = spark.read.parquet("data/silver_np/orders")

df.filter("order_ts >= '2026-01-10' AND order_ts < '2026-01-11'") \
  .groupBy("country") \
  .count() \
  .explain("formatted")
```

### Káº¿t qá»§a:

```code
== Physical Plan ==
AdaptiveSparkPlan (7)
+- HashAggregate (6)
   +- Exchange (5)
      +- HashAggregate (4)
         +- Project (3)
            +- Filter (2)
               +- Scan parquet  (1)


(1) Scan parquet 
Output [2]: [order_ts#3, country#5]
Batched: true
Location: InMemoryFileIndex [file:/Users/nptan2005/SourceCode/Python/spark401_colab/data/silver_np/orders]
PushedFilters: [IsNotNull(order_ts), GreaterThanOrEqual(order_ts,2026-01-10 00:00:00.0), LessThan(order_ts,2026-01-11 00:00:00.0)]
ReadSchema: struct<order_ts:timestamp,country:string>

(2) Filter
Input [2]: [order_ts#3, country#5]
Condition : ((isnotnull(order_ts#3) AND (order_ts#3 >= 2026-01-10 00:00:00)) AND (order_ts#3 < 2026-01-11 00:00:00))

(3) Project
Output [1]: [country#5]
Input [2]: [order_ts#3, country#5]

(4) HashAggregate
Input [1]: [country#5]
Keys [1]: [country#5]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#25L]
Results [2]: [country#5, count#26L]

(5) Exchange
Input [2]: [country#5, count#26L]
Arguments: hashpartitioning(country#5, 200), ENSURE_REQUIREMENTS, [plan_id=15]

(6) HashAggregate
Input [2]: [country#5, count#26L]
Keys [1]: [country#5]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#24L]
Results [2]: [country#5, count(1)#24L AS count#12L]

(7) AdaptiveSparkPlan
Output [2]: [country#5, count#12L]
Arguments: isFinalPlan=false
```

### ğŸ” KHI XEM SPARK UI

Báº¡n sáº½ tháº¥y:

-	Scan parquet
-	Files read = táº¥t cáº£
-	KhÃ´ng cÃ³ dÃ²ng nÃ o nÃ³i vá» PartitionFilters

ğŸ‘‰ â— Spark pháº£i Ä‘á»c toÃ n bá»™ data

---

### ğŸ§  Káº¾T LUáº¬N Táº M

Filter Ä‘Ãºng logic nhÆ°ng khÃ´ng giÃºp IO nhanh hÆ¡n

---

## ğŸ§ª STEP 3 â€“ GHI DATA CÃ“ PARTITION COLUMN (chuáº©n)

### Chá»n partition column Ä‘Ãºng

Trong bank â†’ date / dt / business_date

###  ğŸ‘‰ Ta dÃ¹ng:

```code
dt = to_date(order_ts)
```

### Code (lab2b_partitioned.py)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, col

spark = (
    SparkSession.builder
    .appName("lab2_partitioned")
    .getOrCreate()
)

df = spark.read.parquet("data/silver/orders_enriched")

df = df.withColumn("dt", to_date(col("order_ts")))

(
    df
    .write
    .mode("overwrite")
    .partitionBy("dt")
    .parquet("data/silver_p/orders")
)

print("DONE: partitioned by dt")
spark.stop()
```


### ğŸ‘‰ Cháº¡y:

```bash
python spark/lab/lab2b_partitioned.py
```

---

### ğŸ“ KIá»‚M TRA STRUCTURE (Ráº¤T QUAN TRá»ŒNG)

```bash
ls data/silver_p/orders | head
```

Báº¡n sáº½ tháº¥y:

```code
dt=2026-01-08/
dt=2026-01-09/
dt=2026-01-10/
dt=2026-01-11/
```

ğŸ‘‰ ÄÃ¢y lÃ  physical partition, khÃ´ng pháº£i repartition

---

### ğŸ” STEP 4 â€“ QUERY CÃ“ PARTITION PRUNING

```python
df = spark.read.parquet("data/silver_p/orders")

df.filter("dt = '2026-01-10'") \
  .groupBy("country") \
  .count() \
  .explain("formatted")
```

---

### ğŸ”¥ Äá»ŒC SPARK UI (CHá»– QUAN TRá»ŒNG NHáº¤T)

Trong Scan parquet, báº¡n PHáº¢I tháº¥y:

-	PartitionFilters:

```code
dt = 2026-01-10
```

-	Files read: chá»‰ cá»§a 1 ngÃ y
-	Rows scanned: giáº£m ráº¥t máº¡nh

ğŸ‘‰ Náº¿u tháº¥y váº­y â†’ partition pruning WORKING

---

### ğŸ§  SO SÃNH TRá»°C QUAN

|**TiÃªu chÃ­**|**KhÃ´ng partition**|**Partition by dt**|
|-----------|-------------------|-------------------|
|Files read|Táº¥t cáº£|1 ngÃ y|
|Scan time|Cao hÆ¡n|Tháº¥p|
|Shuffle|Giá»‘ng|Giá»‘ng|
|IO cost|âŒ Tá»‘n|âœ… Ráº»|
|Bank-grade|âŒ|âœ…|

---

## ğŸ§  ÄIá»€U Ráº¤T QUAN TRá»ŒNG (BANK CONTEXT)

### âŒ KhÃ´ng partition theo:
-	customer_id
-	order_id
-	amount

> â†’ Cardinality cao â†’ folder explosion

### âœ… NÃªn partition theo:
-	business_date
-	dt
-	month (náº¿u data ráº¥t lá»›n)

---

## CÃ¢u há»i liÃªn quan bÃ i lab

## 1ï¸âƒ£ Partition column khÃ¡c repartition á»Ÿ Ä‘iá»ƒm nÃ o?

### Partition column (partitionBy)
-	Xáº£y ra lÃºc WRITE (lÆ°u dá»¯ liá»‡u ra storage).
-	Táº¡o cáº¥u trÃºc thÆ° má»¥c váº­t lÃ½ kiá»ƒu dt=2026-01-10/â€¦ â†’ giÃºp Spark Ä‘á»c Ã­t file hÆ¡n khi filter theo partition column.
-	Má»¥c tiÃªu: tá»‘i Æ°u IO / scan (Ä‘á»c dá»¯ liá»‡u).

### Repartition (repartition, coalesce)
-	Xáº£y ra lÃºc COMPUTE (trong DAG).
-	Thay Ä‘á»•i sá»‘ partition cá»§a RDD/DataFrame Ä‘á»ƒ Ä‘iá»u phá»‘i task song song / cÃ¢n báº±ng táº£i.
-	ThÆ°á»ng táº¡o shuffle (repartition thÆ°á»ng shuffle; coalesce thÆ°á»ng khÃ´ng).
-	Má»¥c tiÃªu: tá»‘i Æ°u compute / parallelism / file output count.

> NÃ³i ngáº¯n: partitionBy = layout trÃªn Ä‘Ä©a. repartition = layout trong RAM/cluster Ä‘á»ƒ cháº¡y.

---

### 2ï¸âƒ£ VÃ¬ sao partition giÃºp IO chá»© khÃ´ng giÃºp shuffle?

### VÃ¬ partition pruning chá»‰ áº£nh hÆ°á»Ÿng tá»›i bÆ°á»›c Scan parquet:
-	Khi filter theo dt, Spark chá»‰ má»Ÿ nhá»¯ng folder dt phÃ¹ há»£p â†’ giáº£m file Ä‘á»c â†’ giáº£m IO.

### CÃ²n shuffle xáº£y ra do cÃ¡c phÃ©p:
-	groupBy, join, distinct, orderBy, windowâ€¦

Nhá»¯ng phÃ©p nÃ y cáº§n repartition dá»¯ liá»‡u theo key Ä‘á»ƒ aggregate/join Ä‘Ãºng â†’ shuffle lÃ  â€œbáº¯t buá»™c logicâ€, khÃ´ng liÃªn quan dá»¯ liá»‡u náº±m á»Ÿ folder nÃ o.

> Partition giÃºp â€œÄ‘á»c Ã­tâ€, nhÆ°ng khÃ´ng giÃºp â€œtrá»™n dá»¯ liá»‡uâ€ trong groupBy/join.

---

### 3ï¸âƒ£ Filter báº±ng to_date(order_ts) thay vÃ¬ dt thÃ¬ pruning cÃ³ xáº£y ra khÃ´ng?

### ThÆ°á»ng lÃ  KHÃ”NG (hoáº·c khÃ´ng tá»‘i Æ°u).
-	Partition pruning hoáº¡t Ä‘á»™ng tá»‘t nháº¥t khi filter trá»±c tiáº¿p trÃªn cá»™t partition: dt = '2026-01-10'.
-	Náº¿u báº¡n viáº¿t: to_date(order_ts) = '2026-01-10' thÃ¬:
-	Spark pháº£i tÃ­nh to_date(order_ts) cho tá»«ng row â†’ filter trá»Ÿ thÃ nh â€œrow-level predicateâ€.
-	NÃ³ khÃ´ng map trá»±c tiáº¿p vá» folder dt=â€¦ Ä‘á»ƒ skip tá»« Ä‘áº§u â†’ dá»… máº¥t pruning.

> Best practice: luÃ´n filter trÃªn dt (cá»™t partition) náº¿u cÃ³.

---

# âœ… Tiáº¿p luÃ´n: MINI-LAB â€œCHá»¨NG MINH PRUNINGâ€ (ráº¥t rÃµ trÃªn Spark UI)

## Má»¥c tiÃªu: 

Cháº¡y 2 query giá»‘ng nhau, 1 cÃ¡i pruning ON, 1 cÃ¡i pruning OFF.

---

## A) Chuáº©n bá»‹: Ä‘á»c dataset partitioned

Báº¡n Ä‘Ã£ cÃ³ `data/silver_p/orders (partitionBy dt)`. OK.

## B) Query 1 â€” Pruning ON (Ä‘Ãºng chuáº©n)

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("lab2c_pruning_on").getOrCreate()
df = spark.read.parquet("data/silver_p/orders")

q1 = (
    df.filter("dt = '2026-01-10'")
      .groupBy("country")
      .count()
)

q1.explain("formatted")
q1.show(20, False)

spark.stop()
```


### Báº¡n nhÃ¬n Spark UI / SQL tab / Details:
- Scan parquet
- PartitionFilters: [isnotnull(dt), (dt = 2026-01-10)] (hoáº·c tÆ°Æ¡ng tá»±)
- Files read giáº£m máº¡nh (chá»‰ 1 folder)

---

## C) Query 2 â€” Pruning OFF (cá»‘ tÃ¬nh viáº¿t sai)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import to_date, col

spark = SparkSession.builder.appName("lab2c_pruning_off").getOrCreate()
df = spark.read.parquet("data/silver_p/orders")

q2 = (
    df.filter(to_date(col("order_ts")) == "2026-01-10")
      .groupBy("country")
      .count()
)

q2.explain("formatted")
q2.show(20, False)

spark.stop()
```

### Báº¡n nhÃ¬n Spark UI:
-	ThÆ°á»ng khÃ´ng tháº¥y PartitionFilters (hoáº·c khÃ´ng prune Ä‘Ãºng)
-	Files read cÃ³ thá»ƒ tÄƒng (Ä‘á»c nhiá»u folder hÆ¡n)

> ÄÃ¢y chÃ­nh lÃ  â€œbáº±ng chá»©ng sá»‘ngâ€ cho cÃ¢u #3.

---

# âœ… Bonus: phÃ¢n biá»‡t PartitionBy vs Repartition trong output file count

## D) Test: náº¿u báº¡n muá»‘n â€œmá»—i ngÃ y chá»‰ 1 fileâ€ (production hay cáº§n)

PartitionBy táº¡o folder theo dt, nhÆ°ng má»—i dt cÃ³ thá»ƒ nhiá»u file vÃ¬ sá»‘ partitions lÃºc write.

**Báº¡n lÃ m:**

```python
df = spark.read.parquet("data/silver/orders_enriched") \
          .withColumn("dt", to_date(col("order_ts")))

# 1) partitionBy dt nhÆ°ng sáº½ ra N files má»—i dt (do partitions)
df.write.mode("overwrite").partitionBy("dt").parquet("data/tmp_p1")

# 2) Ã©p má»—i dt ~ Ã­t file hÆ¡n: repartition theo dt trÆ°á»›c khi write
(df.repartition("dt")
   .write.mode("overwrite")
   .partitionBy("dt")
   .parquet("data/tmp_p2"))
```

#### Giáº£i thÃ­ch:
-	partitionBy(dt) táº¡o folder (IO optimization & layout).
-	repartition(dt) quyáº¿t Ä‘á»‹nh bao nhiÃªu task ghi má»—i dt (compute/output file control).

---

# ÄÃ¡nh giÃ¡ lab 2: partition vÃ  repartition

---

## 1ï¸âƒ£ Partition column khÃ¡c repartition á»Ÿ Ä‘iá»ƒm nÃ o?

#### Partition column (khi write) = layout dá»¯ liá»‡u trÃªn disk
-	VÃ­ dá»¥: .write.partitionBy("dt")...
-	Spark sáº½ táº¡o folder theo dt: .../dt=2026-01-10/part-...parquet
-	Lá»£i Ã­ch chÃ­nh: partition pruning â†’ query cÃ³ filter dt=... sáº½ bá» qua cáº£ folder khÃ´ng liÃªn quan â†’ giáº£m IO.

#
#### repartition (khi transform) = chia láº¡i partition trong Spark execution
-	VÃ­ dá»¥: .repartition(200, "dt") hoáº·c .repartition("dt")
-	ÄÃ¢y lÃ  shuffle (Ä‘a pháº§n) Ä‘á»ƒ phÃ¢n phá»‘i láº¡i rows giá»¯a executors/tasks.
-	Má»¥c tiÃªu: tÄƒng/giáº£m parallelism, giáº£m skew, chuáº©n bá»‹ cho join/groupBy, hoáº·c giáº£m small files trÆ°á»›c khi write (náº¿u dÃ¹ng Ä‘Ãºng cÃ¡ch).

##### ğŸ‘‰ NÃ³i ngáº¯n gá»n:
	â€¢	partitionBy = tá»• chá»©c file/folder trÃªn storage (read nhanh hÆ¡n khi filter Ä‘Ãºng cá»™t).
	â€¢	repartition = tá»• chá»©c láº¡i dá»¯ liá»‡u â€œtrong RAM/computeâ€ (tÃ¡c Ä‘á»™ng shuffle/parallel).

---

## 2ï¸âƒ£ VÃ¬ sao partition giÃºp IO chá»© khÃ´ng giÃºp shuffle?

#### VÃ¬ partitionBy táº¡o folder, cÃ²n shuffle xáº£y ra do yÃªu cáº§u â€œgom/Ä‘á»‘i chiáº¿u keyâ€ trong compute.
-	Khi báº¡n filter dt = '2026-01-10' (Ä‘Ãºng partition col) â†’ Spark chá»‰ Ä‘á»c 1 partition folder â†’ IO giáº£m máº¡nh (plan cá»§a báº¡n cÃ³ PartitionFilters vÃ  History UI tháº¥y â€œnumber of partitions read: 1â€).
-	NhÆ°ng khi báº¡n groupBy(country) hay groupBy(dt, segment, ...) â†’ Spark cáº§n Ä‘Æ°a cÃ¹ng key vá» cÃ¹ng reducer â†’ pháº£i Exchange (shuffle) Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh Ä‘Ãºng.

> âœ… Partition cÃ³ thá»ƒ giÃ¡n tiáº¿p giÃºp shuffle trong má»™t sá»‘ tÃ¬nh huá»‘ng ráº¥t cá»¥ thá»ƒ (vd: báº¡n groupBy chá»‰ theo dt vÃ  dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chia/Ä‘á»c theo tá»«ng dt ráº¥t â€œgá»nâ€), nhÆ°ng Spark khÃ´ng coi â€œfolder partitionâ€ lÃ  distribution guarantee Ä‘á»ƒ bá» shuffle má»™t cÃ¡ch cháº¯c cháº¯n. VÃ¬ váº­y báº¡n váº«n tháº¥y Exchange trong lab2.

---

## 3ï¸âƒ£ Náº¿u filter báº±ng to_date(order_ts) thay vÃ¬ dt thÃ¬ pruning cÃ³ xáº£y ra khÃ´ng?

**KhÃ´ng (hoáº·c gáº§n nhÆ° khÃ´ng)** â€” vÃ  Ä‘Ãºng y nhÆ° báº¡n Ä‘Ã£ Ä‘o Ä‘Æ°á»£c:
-	Pruning ON (lab2c_pruning_on.py): plan cÃ³
 `PartitionFilters: ... (dt = 2026-01-10)`

> â†’ Spark bá» qua partitions khÃ¡c.

-	Pruning OFF (`lab2c_pruning_off.py`): plan khÃ´ng cÃ³ PartitionFilters, chá»‰ cÃ³
`PushedFilters: order_ts >= ... AND order_ts < ...`

> â†’ Spark pháº£i scan má»i partition folder, rá»“i má»›i lá»c báº±ng predicate pushdown/row group stats.

**ğŸ‘‰ LÃ½ do:** partition pruning hoáº¡t Ä‘á»™ng khi predicate tham chiáº¿u trá»±c tiáº¿p partition column (dt). CÃ²n to_date(order_ts) lÃ  expression trÃªn cá»™t data â†’ Spark khÃ´ng map ngÆ°á»£c Ä‘Æ°á»£c Ä‘á»ƒ â€œchá»‰ chá»n folder dt=â€¦â€.

> **âœ… Best practice:** materialize dt (cá»™t date) vÃ  partitionBy(dt), rá»“i filter báº±ng dt.

---

# Tiáº¿p: 1 vÃ­ dá»¥ â€œkáº¿t há»£p partition + repartition theo columnâ€ (Ä‘á»ƒ báº¡n tá»± tin trÆ°á»›c lab 3)

## Má»¥c tiÃªu vÃ­ dá»¥:
1.	Dá»¯ liá»‡u partitionBy(dt) Ä‘á»ƒ pruning (IO).
2.	Dá»¯ liá»‡u repartition theo dt Ä‘á»ƒ giáº£m shuffle writers + giáº£m small files, vÃ  quan sÃ¡t trÃªn UI.

## A) Táº¡o silver partitioned theo dt (Ä‘Ãºng bÃ i)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date

spark = (SparkSession.builder
  .appName("lab2d_partitioned_write")
  .config("spark.sql.shuffle.partitions", "50")   # giáº£m máº·c Ä‘á»‹nh 200 cho mÃ¡y local
  .getOrCreate())

df = (spark.read.parquet("data/silver/orders_enriched")
      .withColumn("dt", to_date(col("order_ts"))))

# (1) repartition theo dt trÆ°á»›c khi write:
# - má»¥c tiÃªu: má»—i dt gom vá» 1 sá»‘ partition á»•n Ä‘á»‹nh -> write Ã­t file hÆ¡n, Ä‘á»¡ MemoryManager warn
df = df.repartition("dt")

(df.write
   .mode("overwrite")
   .partitionBy("dt")
   .parquet("data/silver_p/orders"))
```

**Báº¡n sáº½ nhÃ¬n trong Spark UI/History:**

-	Write sáº½ cÃ³ shuffle (vÃ¬ repartition), nhÆ°ng sá»‘ file trong má»—i dt=.../ thÆ°á»ng â€œÄ‘áº¹pâ€ hÆ¡n.
-	Quan trá»ng: vá» sau query filter dt=... sáº½ prune.

â¸»

## B) Compare 2 query Ä‘á»ƒ tháº¥y rÃµ pruning vs khÃ´ng pruning (y nhÆ° báº¡n Ä‘ang lÃ m)

### Query 1 (prune tá»‘t):

```python
spark.read.parquet("data/silver_p/orders") \
  .where(col("dt") == "2026-01-10") \
  .groupBy("country").count().explain("formatted")
```

### Query 2 (khÃ´ng prune):

```python
from pyspark.sql.functions import to_date

spark.read.parquet("data/silver_p/orders") \
  .where(to_date(col("order_ts")) == "2026-01-10") \
  .groupBy("country").count().explain("formatted")
```

**TrÃªn History UI báº¡n ká»³ vá»ng tháº¥y:**
-	Query 1: â€œfiles readâ€ Ã­t hÆ¡n ráº¥t nhiá»u + â€œpartitions read: 1â€
-	Query 2: â€œfiles readâ€ nhiá»u hÆ¡n (scan rá»™ng), dÃ¹ output giá»‘ng nhau

---

### partition + xá»­ lÃ½ partition theo column (repartition)

### repartition(dt) giÃºp â€œshapeâ€ file output (sá»‘ file / kÃ­ch thÆ°á»›c) vÃ  parallelism khi write


# âœ… Lab2e: So sÃ¡nh sá»‘ file khi write (khÃ´ng repartition vs repartition)

---

## 1) Write partitioned nhÆ°ng khÃ´ng repartition

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date

spark = (SparkSession.builder
  .appName("lab2e_write_no_repartition")
  .config("spark.sql.shuffle.partitions", "50")
  .getOrCreate())

df = (spark.read.parquet("../data/silver/orders_enriched")
      .withColumn("dt", to_date(col("order_ts"))))

(df.write.mode("overwrite")
   .partitionBy("dt")
   .parquet("../data/silver_p_lab2e/no_repart"))

spark.stop()
```

## 2) Write partitioned vÃ  repartition theo dt

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date

spark = (SparkSession.builder
  .appName("lab2e_write_repartition_dt")
  .config("spark.sql.shuffle.partitions", "50")
  .getOrCreate())

df = (spark.read.parquet("../data/silver/orders_enriched")
      .withColumn("dt", to_date(col("order_ts")))
      .repartition("dt"))   # key Ä‘iá»ƒm

(df.write.mode("overwrite")
   .partitionBy("dt")
   .parquet("../data/silver_p_lab2e/repart_dt"))

spark.stop()
```

## 3) Check báº±ng lá»‡nh shell (so file count)

```bash
find data/silver_p_lab2e/no_repart -name "*.parquet" | wc -l
find data/silver_p_lab2e/repart_dt -name "*.parquet" | wc -l
```

# xem riÃªng 1 ngÃ y

```bash
find data/silver_p_lab2e/no_repart/dt=2026-01-10 -name "*.parquet" | wc -l
find data/silver_p_lab2e/repart_dt/dt=2026-01-10 -name "*.parquet" | wc -l
```

**Báº¡n sáº½ tháº¥y thÆ°á»ng:**

-	no_repart: nhiá»u file hÆ¡n / partition ngÃ y cÃ³ nhiá»u part hÆ¡n
-	repart_dt: Ã­t file hÆ¡n (á»•n Ä‘á»‹nh hÆ¡n theo dt)

> **LÆ°u Ã½:** ***repartition("dt")*** váº«n shuffle (tá»‘n compute lÃºc write), nhÆ°ng Ä‘á»•i láº¡i file layout â€œÄ‘áº¹pâ€, query sau nÃ y Ä‘á»c nhanh hÆ¡n vÃ  Ã­t overhead.

---

## C) Táº¡i sao trong Query 1 báº¡n váº«n tháº¥y Exchange dÃ¹ Ä‘Ã£ prune?

VÃ¬ báº¡n groupBy theo country chá»© khÃ´ng pháº£i dt.

Náº¿u báº¡n thá»­:

```python
spark.read.parquet("../data/silver_p_lab2d/orders") \
  .where(col("dt") == "2026-01-10") \
  .groupBy("dt").count().explain("formatted")
```

Báº¡n sáº½ tháº¥y shuffle cÃ³ thá»ƒ â€œnháº¹â€ hoáº·c plan khÃ¡c (tÃ¹y Spark quyáº¿t Ä‘á»‹nh), nhÆ°ng nguyÃªn táº¯c: aggregate theo key thÆ°á»ng váº«n cáº§n Exchange.

---
