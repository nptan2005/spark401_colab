# üß† Oracle ‚Üí Spark (PySpark) Cheat Sheet

T√†i li·ªáu n√†y gi√∫p **chuy·ªÉn t∆∞ duy + c√∫ ph√°p t·ª´ Oracle (SQL/PLSQL)** sang **Spark SQL / PySpark**, t·∫≠p trung v√†o **th·ª±c t·∫ø Data Engineering**.

---

## 1Ô∏è‚É£ SELECT / PROJECT

| Oracle | Spark SQL | PySpark |
|------|-----------|---------|
| `SELECT col1, col2 FROM t` | `SELECT col1, col2 FROM t` | `df.select("col1", "col2")` |
| `SELECT *` | `SELECT *` | `df` |
| Alias | `SELECT col1 AS c1` | `df.select(col("col1").alias("c1"))` |

---

## 2Ô∏è‚É£ WHERE / FILTER

| Oracle | Spark SQL | PySpark |
|------|-----------|---------|
| `WHERE a = 1` | `WHERE a = 1` | `df.filter(col("a") == 1)` |
| `BETWEEN` | `BETWEEN` | `df.filter(col("a").between(1,10))` |
| `LIKE '%abc%'` | `LIKE '%abc%'` | `df.filter(col("x").like("%abc%"))` |
| `IN (1,2)` | `IN (1,2)` | `df.filter(col("a").isin(1,2))` |

---

## 3Ô∏è‚É£ INSERT / APPEND

| Oracle | Spark |
|------|-------|
| `INSERT INTO t SELECT ...` | `df.write.mode("append").parquet(path)` |
| Bulk insert | Distributed write | Spark t·ª± scale |

üí° Spark **kh√¥ng c√≥ INSERT t·ª´ng d√≤ng** ‚Üí lu√¥n ghi theo batch.

---

## 4Ô∏è‚É£ UPDATE / DELETE (‚ùó kh√°c Oracle)

| Oracle | Spark |
|------|-------|
| `UPDATE t SET a=1 WHERE ...` | ‚ùå Kh√¥ng h·ªó tr·ª£ tr·ª±c ti·∫øp |
| `DELETE FROM t WHERE ...` | ‚ùå Kh√¥ng |

### ‚úÖ Spark pattern (Rewrite Partition)
```python
(df.filter("dt != '2026-01-10'")
 .union(updated_df)
 .write.mode("overwrite").partitionBy("dt").parquet(path))
```

‚û°Ô∏è **UPDATE = ƒë·ªçc ‚Üí bi·∫øn ƒë·ªïi ‚Üí ghi l·∫°i partition**

---

## 5Ô∏è‚É£ MERGE INTO (UPSERT)

### Oracle
```sql
MERGE INTO tgt t
USING src s
ON (t.id = s.id)
WHEN MATCHED THEN UPDATE
WHEN NOT MATCHED THEN INSERT
```

### Spark (foreachBatch + overwrite partition)
```python
existing = spark.read.parquet(path).filter(col("dt") == dt)
merged = existing.union(src).dropDuplicates(["id"])
merged.write.mode("overwrite").partitionBy("dt").parquet(path)
```

‚û°Ô∏è Th∆∞·ªùng d√πng trong **Streaming / Incremental**

---

## 6Ô∏è‚É£ JOIN

| Oracle | Spark |
|------|-------|
| `INNER JOIN` | `df.join(df2, "id")` |
| `LEFT JOIN` | `df.join(df2, "id", "left")` |
| `/*+ USE_HASH */` | `broadcast(df2)` |

```python
from pyspark.sql.functions import broadcast
fact.join(broadcast(dim), "id")
```

---

## 7Ô∏è‚É£ UNION / UNION ALL

### ‚ö†Ô∏è Oracle y√™u c·∫ßu ƒë√∫ng th·ª© t·ª± c·ªôt

### Spark (an to√†n h∆°n)
```python
df1.unionByName(df2)
```

‚û°Ô∏è **Lu√¥n d√πng `unionByName`** trong production

---

## 8Ô∏è‚É£ GROUP BY / HAVING

| Oracle | Spark |
|------|-------|
| `GROUP BY` | `groupBy()` |
| `HAVING sum(a)>10` | `.filter(sum("a")>10)` |

```python
df.groupBy("dt") \
  .agg(sum("amount").alias("total")) \
  .filter(col("total") > 1000)
```

---

## 9Ô∏è‚É£ Analytic Functions (OVER PARTITION BY)

| Oracle | Spark |
|------|-------|
| `ROW_NUMBER()` | `row_number()` |
| `RANK()` | `rank()` |
| `PARTITION BY` | `Window.partitionBy()` |

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import row_number

w = Window.partitionBy("customer_id").orderBy(col("event_ts").desc())
df.withColumn("rn", row_number().over(w))
```

---

## üîü LISTAGG ‚Üí collect_list / collect_set

| Oracle | Spark |
|------|-------|
| `LISTAGG(x, ',')` | `collect_list(x)` |

```python
from pyspark.sql.functions import collect_list

df.groupBy("order_id") \
  .agg(collect_list("product_name").alias("products"))
```

‚û°Ô∏è Spark tr·∫£ v·ªÅ **ARRAY** (m·∫°nh h∆°n string)

---

## 1Ô∏è‚É£1Ô∏è‚É£ EXPLODE (Ng∆∞·ª£c LISTAGG)

```python
from pyspark.sql.functions import explode

df.select("order_id", explode("products").alias("product"))
```

‚û°Ô∏è Oracle l√†m r·∫•t kh√≥, Spark l√†m c·ª±c t·ªët

---

## 1Ô∏è‚É£2Ô∏è‚É£ STRING / REGEX

| Oracle | Spark |
|------|-------|
| `SUBSTR` | `substring` |
| `INSTR` | `instr` |
| `REGEXP_LIKE` | `rlike` |
| `REGEXP_REPLACE` | `regexp_replace` |

```python
df.withColumn("clean", regexp_replace("raw", "[^0-9]", ""))
```

---

## 1Ô∏è‚É£3Ô∏è‚É£ JSON

| Oracle | Spark |
|------|-------|
| `JSON_VALUE` | `get_json_object` |
| `JSON_TABLE` | `from_json + explode` |

```python
from pyspark.sql.functions import from_json
from pyspark.sql.types import StructType, StringType

schema = StructType().add("id", StringType())
df.withColumn("j", from_json("json_col", schema))
```

---

## 1Ô∏è‚É£4Ô∏è‚É£ HIERARCHY (CONNECT BY)

| Oracle | Spark |
|------|-------|
| `CONNECT BY PRIOR` | ‚ùå Kh√¥ng tr·ª±c ti·∫øp |

### Spark pattern
- Iterative self-join
- GraphFrames
- BFS

```python
# parent_id ‚Üí id self join
```

---

## 1Ô∏è‚É£5Ô∏è‚É£ PACKAGE / PROCEDURE / FUNCTION

| Oracle | Spark |
|------|-------|
| PACKAGE | Python module |
| PROCEDURE | PySpark job |
| FUNCTION | Python function / UDF |

```python
def calc_fee(amount):
    return amount * 0.01
```

---

## üß† T∆∞ duy quan tr·ªçng

| Oracle Mindset | Spark Mindset |
|---------------|---------------|
| Row-based | Columnar |
| Update/Delete | Rewrite |
| Stateful | Stateless + checkpoint |
| Single DB | Distributed compute |

---

## ‚úÖ Checklist khi chuy·ªÉn Oracle ‚Üí Spark

- [ ] C√≥ partition key ch∆∞a?
- [ ] C√≥ tr√°nh UPDATE t·ª´ng d√≤ng kh√¥ng?
- [ ] C√≥ d√πng unionByName kh√¥ng?
- [ ] C√≥ broadcast dim nh·ªè kh√¥ng?
- [ ] C√≥ checkpoint khi streaming kh√¥ng?

---

üìå **Next ƒë·ªÅ xu·∫•t**
- Streaming cheat sheet
- Oracle PL/SQL ‚Üí Spark Streaming patterns
- Real banking pipelines (ACQ / ISS / AML)

