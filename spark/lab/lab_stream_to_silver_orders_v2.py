# spark/lab/lab_stream_to_silver_orders_v2.py
import os
import shutil
from datetime import datetime

from pyspark.sql import SparkSession
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, TimestampType, DateType
)
from pyspark.sql.functions import col, to_timestamp, to_date, lit, row_number
from pyspark.sql.window import Window

BRONZE_ORDERS_PATH = "data/bronze_lab33/orders_raw"
SILVER_OUT = "data/silver/orders_fact_dt_stream"
CHECKPOINT = "checkpoints/orders_to_silver_v2"

TMP_ROOT = "data/tmp/orders_upsert"

os.makedirs(SILVER_OUT, exist_ok=True)
os.makedirs(CHECKPOINT, exist_ok=True)
os.makedirs(TMP_ROOT, exist_ok=True)

spark = (
    SparkSession.builder
    .appName("lab_stream_to_silver_orders_v2")
    .config("spark.sql.shuffle.partitions", "50")
    # streaming khÃ´ng dÃ¹ng AQE: Spark sáº½ tá»± warning & disable
    .getOrCreate()
)

spark.sparkContext.setLogLevel("WARN")

# 1) Streaming file source báº¯t buá»™c schema (báº¡n gáº·p lá»—i nÃ y láº§n Ä‘áº§u)
schema = StructType([
    StructField("order_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("merchant_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("event_ts", StringType(), True),   # bronze lÃ  string
    StructField("channel", StringType(), True),
    StructField("country", StringType(), True),
    StructField("status", StringType(), True),
    StructField("ingest_ts", StringType(), True),
    StructField("dt", StringType(), True),         # bronze dt string
])

def atomic_replace_dir(src_dir: str, dst_dir: str):
    """
    Thao tÃ¡c â€œan toÃ n tÆ°Æ¡ng Ä‘á»‘iâ€ trÃªn local:
    - XoÃ¡ dst náº¿u tá»“n táº¡i
    - Move src -> dst (rename/move lÃ  atomic hÆ¡n copy)
    """
    if os.path.exists(dst_dir):
        shutil.rmtree(dst_dir)
    os.makedirs(os.path.dirname(dst_dir), exist_ok=True)
    shutil.move(src_dir, dst_dir)

def upsert_batch(batch_df, batch_id: int):
    """
    foreachBatch: cháº¡y trÃªn má»—i micro-batch (DataFrame tÄ©nh).
    Má»¥c tiÃªu:
    - watermark + dedup theo order_id (giáº£ láº­p idempotent)
    - upsert theo dt: rewrite tá»«ng dt partition báº±ng swap folder
    """
    if batch_df.rdd.isEmpty():
        print(f"â„¹ï¸ batch={batch_id} empty")
        return

    # 2) Chuáº©n hoÃ¡ kiá»ƒu dá»¯ liá»‡u
    df = (
        batch_df
        .withColumn("event_ts", to_timestamp(col("event_ts")))
        .withColumn("ingest_ts", to_timestamp(col("ingest_ts")))
        .withColumn("dt", to_date(col("dt")))
    )

    # 3) Dedup thá»±c táº¿: giá»¯ record má»›i nháº¥t theo ingest_ts cho má»—i order_id
    w = Window.partitionBy("order_id").orderBy(col("ingest_ts").desc())
    dedup = (
        df
        .withColumn("rn", row_number().over(w))
        .where(col("rn") == 1)
        .drop("rn")
    )

    # Láº¥y danh sÃ¡ch dt trong batch Ä‘á»ƒ upsert tá»«ng partition
    dts = [r["dt"] for r in dedup.select("dt").distinct().collect()]

    # TÃ­nh rows trÆ°á»›c khi write Ä‘á»ƒ khÃ´ng trigger Ä‘á»c sau khi replace
    rows_in_batch = dedup.count()
    print(f"âœ… batch={batch_id} rows_after_dedup={rows_in_batch} partitions={len(dts)} dts={dts}")

    for dtv in dts:
        dt_str = dtv.isoformat()
        part_path = os.path.join(SILVER_OUT, f"dt={dt_str}")
        tmp_path = os.path.join(TMP_ROOT, f"batch={batch_id}", f"dt={dt_str}")

        # (a) lá»c partition dt cá»§a batch
        part_new = dedup.where(col("dt") == lit(dtv))

        # (b) náº¿u partition Ä‘Ã£ tá»“n táº¡i â†’ Ä‘á»c cÅ©, union, dedup láº¡i theo order_id
        if os.path.exists(part_path):
            old = spark.read.parquet(part_path)
            merged = old.unionByName(part_new)

            w2 = Window.partitionBy("order_id").orderBy(col("ingest_ts").desc())
            final_part = (
                merged
                .withColumn("rn", row_number().over(w2))
                .where(col("rn") == 1)
                .drop("rn")
            )
        else:
            final_part = part_new

        # (c) write ra tmp (khÃ´ng Ä‘á»¥ng partition tháº­t)
        if os.path.exists(tmp_path):
            shutil.rmtree(tmp_path)
        os.makedirs(tmp_path, exist_ok=True)

        # (
        #     final_part
        #     .coalesce(1)   # local lab: 1 file/partition cho dá»… nhÃ¬n (production: KHÃ”NG lÃ m váº­y)
        #     .write
        #     .mode("overwrite")
        #     .parquet(tmp_path)
        # )

        # (c) write ra tmp (khÃ´ng Ä‘á»¥ng partition tháº­t)
        # IMPORTANT: drop("dt") Ä‘á»ƒ dt chá»‰ láº¥y tá»« partition folder dt=...
        (
            final_part
            .drop("dt")
            .coalesce(1)   # lab local cho dá»… nhÃ¬n
            .write
            .mode("overwrite")
            .parquet(tmp_path)
        )

        # (d) swap folder: tmp -> dt=...
        atomic_replace_dir(tmp_path, part_path)
        print(f"   â†ª upserted dt={dt_str} ok")

# 4) Äá»c stream tá»« file source
stream_df = (
    spark.readStream
    .schema(schema)
    .json(BRONZE_ORDERS_PATH)
)

# 5) Start streaming
query = (
    stream_df
    .writeStream
    .foreachBatch(upsert_batch)
    .option("checkpointLocation", CHECKPOINT)
    # .trigger(processingTime="5 seconds")
    # thay vÃ¬ .trigger(processingTime="5 seconds")
    .trigger(availableNow=True)
    .start()
)

print("ğŸš€ Streaming started. Spark UI:", spark.sparkContext.uiWebUrl)

query.awaitTermination()