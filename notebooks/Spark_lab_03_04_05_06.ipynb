{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPej1ZlwDErqrwOofogjyCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nptan2005/spark401_colab/blob/main/notebooks/Spark_lab_03_04_05_06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OVhwKX7q_YH5",
        "outputId": "9021772d-f63e-408b-a9d1-b920f3cc2a03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libgail-common libgail18 libgtk2.0-0\n",
            "  libgtk2.0-bin libgtk2.0-common librsvg2-common libxcomposite1 libxt-dev\n",
            "  libxtst6 libxxf86dga1 openjdk-17-jre session-migration x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libxt-doc openjdk-17-demo openjdk-17-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libgail-common libgail18 libgtk2.0-0\n",
            "  libgtk2.0-bin libgtk2.0-common librsvg2-common libxcomposite1 libxt-dev\n",
            "  libxtst6 libxxf86dga1 openjdk-17-jdk openjdk-17-jre session-migration\n",
            "  x11-utils\n",
            "0 upgraded, 24 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 8,212 kB of archives.\n",
            "After this operation, 24.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre amd64 17.0.17+10-1~22.04 [238 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk amd64 17.0.17+10-1~22.04 [1,521 kB]\n",
            "Fetched 8,212 kB in 1s (7,819 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../05-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../06-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../07-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../08-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../09-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../10-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../11-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../12-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../13-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../14-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../15-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../16-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../17-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../18-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../19-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../20-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../21-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-17-jre:amd64.\n",
            "Preparing to unpack .../22-openjdk-17-jre_17.0.17+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jre:amd64 (17.0.17+10-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-17-jdk:amd64.\n",
            "Preparing to unpack .../23-openjdk-17-jdk_17.0.17+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk:amd64 (17.0.17+10-1~22.04) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service ‚Üí /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up openjdk-17-jre:amd64 (17.0.17+10-1~22.04) ...\n",
            "Setting up openjdk-17-jdk:amd64 (17.0.17+10-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y openjdk-17-jdk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
        "!tar xf spark-4.0.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRmPkxKH_dx9",
        "outputId": "c7e7beac-f0cb-4546-b31d-290443108bc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-10 06:28:30--  https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548955321 (524M) [application/x-gzip]\n",
            "Saving to: ‚Äòspark-4.0.1-bin-hadoop3.tgz‚Äô\n",
            "\n",
            "spark-4.0.1-bin-had 100%[===================>] 523.52M  12.5MB/s    in 66s     \n",
            "\n",
            "2026-01-10 06:29:37 (7.91 MB/s) - ‚Äòspark-4.0.1-bin-hadoop3.tgz‚Äô saved [548955321/548955321]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Spark 4.0.1 Setup (REQUIRED)\n",
        "# ===============================\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-4.0.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":/content/spark-4.0.1-bin-hadoop3/bin\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"lab\").getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpWbD-KG_xqm",
        "outputId": "adbffb4c-3ac6-48d7-e882-80bab05a2ae0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ LAB 3 ‚Äì FILTER & IO (ƒê·ªåC √çT H∆†N = NHANH H∆†N)\n",
        "\n",
        "## üéØ M·ª•c ti√™u LAB 3\n",
        "*\tHi·ªÉu Spark KH√îNG ƒë·ªçc h·∫øt d·ªØ li·ªáu\n",
        "*\tHi·ªÉu Predicate Pushdown\n",
        "*\t**Ph√¢n bi·ªát:**\n",
        "-\tfilter() tr∆∞·ªõc / sau khi ƒë·ªçc\n",
        "-\tSpark vs Pandas\n",
        "-\tBi·∫øt vi·∫øt Spark job ‚Äúƒë·ª° t·ªën IO‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## 0Ô∏è‚É£ Nh·∫Øc l·∫°i t∆∞ duy s·ªëng c√≤n\n",
        "\n",
        "* ‚ùå Spark kh√¥ng nhanh v√¨ nhi·ªÅu core\n",
        "* ‚úÖ Spark nhanh v√¨ ƒë·ªçc √çT d·ªØ li·ªáu h∆°n\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p (c√≥ ƒëi·ªÅu ki·ªán l·ªçc)\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = (\n",
        "    spark.range(0, 5_000_000)\n",
        "    .withColumn(\"country\", F.when(F.col(\"id\") % 5 == 0, \"VN\")\n",
        "                            .when(F.col(\"id\") % 5 == 1, \"SG\")\n",
        "                            .when(F.col(\"id\") % 5 == 2, \"TH\")\n",
        "                            .when(F.col(\"id\") % 5 == 3, \"ID\")\n",
        "                            .otherwise(\"MY\"))\n",
        "    .withColumn(\"amount\", (F.rand() * 10000).cast(\"double\"))\n",
        ")\n",
        "\n",
        "df.count()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ Ghi d·ªØ li·ªáu ra Parquet (gi·∫£ l·∫≠p Data Lake)\n",
        "\n",
        "```python\n",
        "df.write.mode(\"overwrite\").parquet(\"/tmp/lab3_orders\")\n",
        "```\n",
        "\n",
        "üìå Parquet = columnar format ‚Üí ƒëi·ªÅu ki·ªán ƒë·ªÉ Spark pushdown\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Case 1 ‚Äì Filter SAU khi ƒë·ªçc (c√°ch NGU)\n",
        "\n",
        "```python\n",
        "df_read = spark.read.parquet(\"/tmp/lab3_orders\")\n",
        "\n",
        "df_bad = df_read.filter(F.col(\"country\") == \"VN\")\n",
        "\n",
        "df_bad.explain()\n",
        "```\n",
        "\n",
        "### ‚ùå ƒêi·ªÅu g√¨ x·∫£y ra?\n",
        "*\tSpark ƒë·ªçc to√†n b·ªô file\n",
        "*\tSau ƒë√≥ m·ªõi filter\n",
        "*\tIO l·ªõn ‚Üí ch·∫≠m ‚Üí t·ªën ti·ªÅn\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Case 2 ‚Äì Filter NGAY KHI ƒë·ªçc (chu·∫©n)\n",
        "\n",
        "```python\n",
        "df_good = (\n",
        "    spark.read.parquet(\"/tmp/lab3_orders\")\n",
        "    .filter(F.col(\"country\") == \"VN\")\n",
        ")\n",
        "\n",
        "df_good.explain()\n",
        "```\n",
        "\n",
        "### üîç Nh√¨n trong explain():\n",
        "\n",
        "#### B·∫°n s·∫Ω th·∫•y:\n",
        "\n",
        "```code\n",
        "PushedFilters: [IsNotNull(country), EqualTo(country,VN)]\n",
        "```\n",
        "\n",
        "üéâ ƒê√ÇY CH√çNH L√Ä PREDICATE PUSHDOWN\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Predicate Pushdown l√† g√¨? (Hi·ªÉu cho ƒë√∫ng)\n",
        "\n",
        "Spark ƒë·∫©y ƒëi·ªÅu ki·ªán l·ªçc xu·ªëng t·∫≠n file storage\n",
        "\n",
        "### üì¶ Parquet file:\n",
        "*\tC√≥ metadata theo column\n",
        "*\tSpark ch·ªâ ƒë·ªçc block c·∫ßn thi·∫øt\n",
        "\n",
        "> üìå IO gi·∫£m m·∫°nh\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Spark vs Pandas ‚Äì kh√°c bi·ªát s·ªëng c√≤n\n",
        "\n",
        "### Pandas\n",
        "\n",
        "```python\n",
        "df = pd.read_parquet(\"big.parquet\")\n",
        "df[df[\"country\"] == \"VN\"]\n",
        "```\n",
        "\n",
        "‚ùå ƒê·ªçc to√†n b·ªô file v√†o RAM\n",
        "\n",
        "---\n",
        "\n",
        "### Spark\n",
        "\n",
        "```python\n",
        "spark.read.parquet(\"big.parquet\").filter(col(\"country\") == \"VN\")\n",
        "```\n",
        "\n",
        "‚úÖ Ch·ªâ ƒë·ªçc ph·∫ßn c·∫ßn\n",
        "\n",
        "---\n",
        "\n",
        "## 7Ô∏è‚É£ Nh·ªØng filter KH√îNG pushdown ƒë∆∞·ª£c (r·∫•t hay sai)\n",
        "\n",
        "### ‚ùå UDF\n",
        "\n",
        "```python\n",
        "df.filter(my_udf(col(\"country\")))\n",
        "```\n",
        "\n",
        "### ‚ùå Function ph·ª©c t·∫°p\n",
        "\n",
        "```python\n",
        "df.filter(F.lower(F.col(\"country\")) == \"vn\")\n",
        "```\n",
        "\n",
        "### ‚ùå Python logic\n",
        "\n",
        "```python\n",
        "df.filter(lambda x: x.country == \"VN\")\n",
        "```\n",
        "\n",
        "üëâ Spark bu·ªôc ph·∫£i ƒë·ªçc h·∫øt\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Filter ƒë√∫ng chu·∫©n bank-grade\n",
        "\n",
        "```python\n",
        "df.filter(\n",
        "    (F.col(\"country\") == \"VN\") &\n",
        "    (F.col(\"amount\") > 1000)\n",
        ")\n",
        "```\n",
        "\n",
        "### üìå ƒêi·ªÅu ki·ªán:\n",
        "*\tC·ªôt g·ªëc\n",
        "*\tKh√¥ng UDF\n",
        "*\tKh√¥ng transform\n",
        "\n",
        "‚∏ª\n",
        "\n",
        "## 9Ô∏è‚É£ Partition + Filter = combo m·∫°nh\n",
        "\n",
        "```python\n",
        "df_partitioned = (\n",
        "    df.write\n",
        "    .partitionBy(\"country\")\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(\"/tmp/lab3_partitioned\")\n",
        ")\n",
        "\n",
        "spark.read.parquet(\"/tmp/lab3_partitioned\") \\\n",
        "    .filter(F.col(\"country\") == \"VN\") \\\n",
        "    .explain()\n",
        "```\n",
        "\n",
        "üéØ Spark ch·ªâ ƒë·ªçc folder country=VN\n",
        "\n",
        "---\n",
        "\n",
        "## üîü T·ªïng k·∫øt LAB 3\n",
        "\n",
        "### ‚ùå Sai l·∫ßm ph·ªï bi·∫øn\n",
        "*\tƒê·ªçc r·ªìi m·ªõi filter\n",
        "*\tD√πng UDF ƒë·ªÉ filter\n",
        "*\tNghƒ© Spark nhanh v√¨ nhi·ªÅu core\n",
        "\n",
        "### ‚úÖ ƒê√∫ng chu·∫©n\n",
        "*\tFilter c√†ng s·ªõm c√†ng t·ªët\n",
        "*\tD√πng column g·ªëc\n",
        "*\tParquet + predicate pushdown\n",
        "*\tPartition theo business key\n",
        "\n",
        "---\n",
        "\n",
        "## üß† C√ÇU H·ªéI B·∫ÆT BU·ªòC (TR∆Ø·ªöC LAB 4)\n",
        "1.\tV√¨ sao filter sau khi read l√† t·ªën IO?\n",
        "2.\tPredicate pushdown ho·∫°t ƒë·ªông ·ªü t·∫ßng n√†o?\n",
        "3.\tV√¨ sao UDF ph√° predicate pushdown?\n",
        "4.\tPartitionBy gi√∫p g√¨ cho filter?\n",
        "5.\tKhi n√†o KH√îNG n√™n partitionBy?\n"
      ],
      "metadata": {
        "id": "j-kvESJ4CLs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = (\n",
        "    spark.range(0, 5_000_000)\n",
        "    .withColumn(\"country\", F.when(F.col(\"id\") % 5 == 0, \"VN\")\n",
        "                            .when(F.col(\"id\") % 5 == 1, \"SG\")\n",
        "                            .when(F.col(\"id\") % 5 == 2, \"TH\")\n",
        "                            .when(F.col(\"id\") % 5 == 3, \"ID\")\n",
        "                            .otherwise(\"MY\"))\n",
        "    .withColumn(\"amount\", (F.rand() * 10000).cast(\"double\"))\n",
        ")"
      ],
      "metadata": {
        "id": "v0Z0MAMcEOL8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE72RzKQEZFk",
        "outputId": "cda0133f-060f-4a74-820a-bc8051ec0882"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000000"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.mode(\"overwrite\").parquet(\"/content/tmp/lab3_orders\")"
      ],
      "metadata": {
        "id": "f6ThhIUIEeoh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ton IO\n",
        "\n",
        "df_read = spark.read.parquet(\"/content/tmp/lab3_orders\")\n",
        "\n",
        "df_bad = df_read.filter(F.col(\"country\") == \"VN\")\n",
        "\n",
        "df_bad.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_77VaB5EwnQ",
        "outputId": "ff4b4a64-31c0-4a32-cba1-32ff1848bed6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Filter (isnotnull(country#11) AND (country#11 = VN))\n",
            "+- *(1) ColumnarToRow\n",
            "   +- FileScan parquet [id#10L,country#11,amount#12] Batched: true, DataFilters: [isnotnull(country#11), (country#11 = VN)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_orders], PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,VN)], ReadSchema: struct<id:bigint,country:string,amount:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chuan\n",
        "df_good = (\n",
        "    spark.read.parquet(\"/content/tmp/lab3_orders\")\n",
        "    .filter(F.col(\"country\") == \"VN\")\n",
        ")\n",
        "\n",
        "df_good.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1o1nCqVFDvM",
        "outputId": "0c24cb1c-ee27-45b5-9013-0e03e393361f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Filter (isnotnull(country#15) AND (country#15 = VN))\n",
            "+- *(1) ColumnarToRow\n",
            "   +- FileScan parquet [id#14L,country#15,amount#16] Batched: true, DataFilters: [isnotnull(country#15), (country#15 = VN)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_orders], PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,VN)], ReadSchema: struct<id:bigint,country:string,amount:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\n",
        "    (F.col(\"country\") == \"VN\") &\n",
        "    (F.col(\"amount\") > 1000)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X1pbRtyFpCQ",
        "outputId": "3c087003-cf4d-4968-aee3-09acf57d420d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, country: string, amount: double]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_partitioned = (\n",
        "    df.write\n",
        "    .partitionBy(\"country\")\n",
        "    .mode(\"overwrite\")\n",
        "    .parquet(\"/content/tmp/lab3_partitioned\")\n",
        ")"
      ],
      "metadata": {
        "id": "YOCGEtUvFxwZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.parquet(\"/content/tmp/lab3_partitioned\") \\\n",
        "    .filter(F.col(\"country\") == \"VN\") \\\n",
        "    .explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "neqr_qUJGGHc",
        "outputId": "86b3587f-5f61-46c7-f483-d7c048cdff3a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) ColumnarToRow\n",
            "+- FileScan parquet [id#18L,amount#19,country#20] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_partitioned], PartitionFilters: [isnotnull(country#20), (country#20 = VN)], PushedFilters: [], ReadSchema: struct<id:bigint,amount:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¥ LAB 4 ‚Äì GROUP BY & SHUFFLE (HI·ªÇU = S·ªêNG)\n",
        "\n",
        "## üéØ M·ª•c ti√™u LAB 4\n",
        "\n",
        "*\tHi·ªÉu Shuffle l√† g√¨ (th·ª±c s·ª±)\n",
        "*\tV√¨ sao groupBy() lu√¥n nguy hi·ªÉm\n",
        "*\tBi·∫øt ƒë·ªçc explain() ƒë·ªÉ nh√¨n th·∫•y ti·ªÅn ƒëang ch√°y\n",
        "*\tBi·∫øt vi·∫øt aggregation bank-grade\n",
        "\n",
        "---\n",
        "\n",
        "# 0Ô∏è‚É£ √în l·∫°i 1 c√¢u s·ªëng c√≤n\n",
        "\n",
        "* ‚ùå Spark ch·∫≠m kh√¥ng ph·∫£i do CPU\n",
        "* ‚ùå Spark ch·∫øt v√¨ SHUFFLE\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Chu·∫©n b·ªã d·ªØ li·ªáu (d√πng l·∫°i LAB 3)\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = spark.read.parquet(\"/tmp/lab3_orders\")\n",
        "df.count()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ GROUP BY ƒë∆°n gi·∫£n (nh∆∞ng nguy hi·ªÉm)\n",
        "\n",
        "\n",
        "```pyhon\n",
        "df_group = (\n",
        "    df.groupBy(\"country\")\n",
        "      .agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
        ")\n",
        "\n",
        "df_group.explain()\n",
        "```\n",
        "\n",
        "### üî• Quan s√°t explain()\n",
        "\n",
        "B·∫°n s·∫Ω th·∫•y d√≤ng r·∫•t quan tr·ªçng:\n",
        "\n",
        "```code\n",
        "Exchange hashpartitioning(country, 200)\n",
        "```\n",
        "\n",
        "üëâ Exchange = SHUFFLE\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Shuffle l√† g√¨? (hi·ªÉu cho ƒë√∫ng)\n",
        "\n",
        "**Khi groupBy(country):**\n",
        "\n",
        "1.\tSpark ph·∫£i gom t·∫•t c·∫£ record c√πng country\n",
        "2.\tRecord ƒëang n·∫±m r·∫£i r√°c ·ªü nhi·ªÅu executor\n",
        "3.\t‚Üí Ph·∫£i network transfer\n",
        "4.\t‚Üí Disk spill\n",
        "5.\t‚Üí Task imbalance\n",
        "\n",
        "> üìå Shuffle = network + disk + sync\n",
        "\n",
        "---\n",
        "\n",
        "4Ô∏è‚É£ V√¨ sao GROUP BY lu√¥n shuffle?\n",
        "\n",
        "|**Operation**|**Shuffle**|\n",
        "|-------------|-----------|\n",
        "|select\t|‚ùå|\n",
        "|filter|\t‚ùå|\n",
        "|withColumn\t|‚ùå|\n",
        "|groupBy|\t‚úÖ|\n",
        "|join\t|‚úÖ|\n",
        "|orderBy|\t‚úÖ|\n",
        "\n",
        "üëâ V√¨ c·∫ßn data c√πng key ·ªü c√πng task\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Nh√¨n shuffle b·∫±ng Spark UI (t∆∞ duy)\n",
        "\n",
        "**Trong Spark UI ‚Üí Stage:**\n",
        "\n",
        "*\tShuffle Read (MB / GB)\n",
        "*\tShuffle Write\n",
        "*\tTask time l·ªách nhau\n",
        "\n",
        "üìå Stage stuck 99% = skew + shuffle\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Th·ª≠ GROUP BY tr√™n skew data (th·ª±c chi·∫øn)\n",
        "\n",
        "```pyhon\n",
        "df_skew = (\n",
        "    df.withColumn(\n",
        "        \"customer_id\",\n",
        "        F.when(F.col(\"country\") == \"VN\", \"HOT\")\n",
        "         .otherwise(F.col(\"country\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "df_skew.groupBy(\"customer_id\").count().explain()\n",
        "```\n",
        "\n",
        "### üî• K·∫øt qu·∫£:\n",
        "\n",
        "*\t1 key = HOT\n",
        "*\t1 task x·ª≠ l√Ω kh·ªïng l·ªì\n",
        "*\tC√°c task kh√°c r·∫£nh\n",
        "\n",
        "‚∏ª\n",
        "\n",
        "## 7Ô∏è‚É£ Gi·∫£m t√°c h·∫°i shuffle ‚Äì c√°ch 1: pre-aggregation\n",
        "\n",
        "### ‚ùå Sai:\n",
        "\n",
        "```pyhon\n",
        "df.groupBy(\"country\").sum(\"amount\")\n",
        "```\n",
        "\n",
        "### ‚úÖ ƒê√∫ng (khi c√≥ b∆∞·ªõc trung gian):\n",
        "\n",
        "```python\n",
        "df.repartition(\"country\") \\\n",
        "  .groupBy(\"country\") \\\n",
        "  .sum(\"amount\")\n",
        "```\n",
        "\n",
        "üìå Kh√¥ng lo·∫°i shuffle, nh∆∞ng gi·∫£m cost\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Gi·∫£m t√°c h·∫°i shuffle ‚Äì c√°ch 2: gi·∫£m partitions\n",
        "\n",
        "```pyhon\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n",
        "```\n",
        "\n",
        "üìå M·∫∑c ƒë·ªãnh = 200\n",
        "\n",
        "Dataset nh·ªè ‚Üí 200 task = overhead\n",
        "\n",
        "---\n",
        "\n",
        "## 9Ô∏è‚É£ Gi·∫£m t√°c h·∫°i shuffle ‚Äì c√°ch 3: AQE (Spark 3+)\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "#### AQE l√†m g√¨?\n",
        "*\tGi·∫£m shuffle partitions runtime\n",
        "*\tDetect skew\n",
        "*\tRebalance tasks\n",
        "\n",
        "üìå Kh√¥ng ph·∫£i ph√©p m√†u ‚Äì design v·∫´n quan tr·ªçng\n",
        "\n",
        "---\n",
        "\n",
        "## üîü GROUP BY chu·∫©n bank-grade\n",
        "\n",
        "### ‚ùå Kh√¥ng l√†m\n",
        "*\tgroupBy tr√™n key skew\n",
        "*\tgroupBy nhi·ªÅu c·ªôt l·ªõn\n",
        "*\tgroupBy trong Gold layer\n",
        "\n",
        "### ‚úÖ N√™n l√†m\n",
        "*\tAggregate s·ªõm (Silver)\n",
        "*\tKey √≠t cardinality\n",
        "*\tC√≥ explain l∆∞u l·∫°i\n",
        "*\tShuffle predictable\n",
        "\n",
        "‚∏ª\n",
        "\n",
        "## üß† C√ÇU H·ªéI B·∫ÆT BU·ªòC ‚Äì LAB 4\n",
        "\n",
        "**Tr·∫£ l·ªùi b·∫±ng √Ω hi·ªÉu c·ªßa b·∫°n:**\n",
        "\n",
        "1.\tV√¨ sao groupBy() lu√¥n g√¢y shuffle?\n",
        "2.\tExchange trong plan nghƒ©a l√† g√¨?\n",
        "3.\tV√¨ sao shuffle partitions = 200 nguy hi·ªÉm?\n",
        "4.\tKhi n√†o shuffle l√† b·∫Øt bu·ªôc?\n",
        "5.\tD·∫•u hi·ªáu job ƒëang ch·∫øt v√¨ shuffle?\n"
      ],
      "metadata": {
        "id": "oVbdw5gylU1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "df = spark.read.parquet(\"/content/tmp/lab3_orders\")\n",
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvlMz0Q4oZmS",
        "outputId": "d25f4cec-a4e2-4ad8-be76-837c02ff6f63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GROUP BY ƒë∆°n gi·∫£n (nh∆∞ng nguy hi·ªÉm)\n",
        "\n",
        "df_group = (\n",
        "    df.groupBy(\"country\")\n",
        "      .agg(F.sum(\"amount\").alias(\"total_amount\"))\n",
        ")\n",
        "\n",
        "df_group.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6zY66hnojsf",
        "outputId": "e213d01d-d0ac-439e-c1f8-521e8b02f9c7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[country#4], functions=[sum(amount#5)])\n",
            "   +- Exchange hashpartitioning(country#4, 200), ENSURE_REQUIREMENTS, [plan_id=89]\n",
            "      +- HashAggregate(keys=[country#4], functions=[partial_sum(amount#5)])\n",
            "         +- FileScan parquet [country#4,amount#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_orders], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<country:string,amount:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Th·ª≠ GROUP BY tr√™n skew data (th·ª±c chi·∫øn)\n",
        "df_skew = (\n",
        "    df.withColumn(\n",
        "        \"customer_id\",\n",
        "        F.when(F.col(\"country\") == \"VN\", \"HOT\")\n",
        "         .otherwise(F.col(\"country\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "df_skew.groupBy(\"customer_id\").count().explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-myVJ3Go4PS",
        "outputId": "5c1bddde-b382-4730-a3d3-1ebcb9a1e2bc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[customer_id#21], functions=[count(1)])\n",
            "   +- Exchange hashpartitioning(customer_id#21, 200), ENSURE_REQUIREMENTS, [plan_id=104]\n",
            "      +- HashAggregate(keys=[customer_id#21], functions=[partial_count(1)])\n",
            "         +- Project [CASE WHEN (country#4 = VN) THEN HOT ELSE country#4 END AS customer_id#21]\n",
            "            +- FileScan parquet [country#4] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_orders], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<country:string>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  repartition\n",
        "\n",
        "df_p = df.repartition(\"country\") \\\n",
        "  .groupBy(\"country\") \\\n",
        "  .sum(\"amount\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bO9Wc-NCpNGu",
        "outputId": "9e3a7071-0f57-4659-e0b4-4eb0cce0637b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[country#4], functions=[sum(amount#5)])\n",
            "   +- HashAggregate(keys=[country#4], functions=[partial_sum(amount#5)])\n",
            "      +- Exchange hashpartitioning(country#4, 200), REPARTITION_BY_COL, [plan_id=117]\n",
            "         +- FileScan parquet [country#4,amount#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_orders], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<country:string,amount:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# giam partition\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 50)\n",
        "\n",
        "df_p = df.repartition(\"country\") \\\n",
        "  .groupBy(\"country\") \\\n",
        "  .sum(\"amount\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts0RGgmXppOf",
        "outputId": "87727496-01c6-4717-9ab6-d80a74867b9f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[country#4], functions=[sum(amount#5)])\n",
            "   +- HashAggregate(keys=[country#4], functions=[partial_sum(amount#5)])\n",
            "      +- Exchange hashpartitioning(country#4, 50), REPARTITION_BY_COL, [plan_id=132]\n",
            "         +- FileScan parquet [country#4,amount#5] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/content/tmp/lab3_orders], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<country:string,amount:double>\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 5 l√† lab quan tr·ªçng nh·∫•t trong Spark. N·∫øu LAB 4 l√† ‚Äúƒë·ªët ti·ªÅn‚Äù, th√¨ LAB 5 l√† ‚Äún·ªï m√°y‚Äù üí• (OOM, fail job, incident ban ƒë√™m).\n",
        "\n",
        "Ta l√†m ch·∫≠m ‚Äì r·∫•t r√µ ‚Äì t·ª´ng b∆∞·ªõc.\n",
        "\n",
        "---\n",
        "\n",
        "# üí£ LAB 5 ‚Äì JOIN, BROADCAST & OOM\n",
        "\n",
        "> *‚Äú90% Spark incident trong bank ƒë·∫øn t·ª´ JOIN‚Äù*\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ M·ª•c ti√™u LAB 5\n",
        "\n",
        "**Sau lab n√†y b·∫°n ph·∫£i tr·∫£ l·ªùi ƒë∆∞·ª£c:**\n",
        "\n",
        "*\tV√¨ sao JOIN g√¢y shuffle\n",
        "*\tV√¨ sao broadcast sai nguy hi·ªÉm h∆°n shuffle\n",
        "*\t**Khi n√†o Spark ch·ªçn:**\n",
        "-\tBroadcastHashJoin\n",
        "-\tSortMergeJoin\n",
        "-\tV√¨ sao OOM th∆∞·ªùng ƒë·∫øn t·ª´ JOIN\n",
        "-\tC√°ch JOIN bank-grade\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Chu·∫©n b·ªã data (NH·ªé ‚Äì ƒë·ªÉ hi·ªÉu)\n",
        "\n",
        "Customers (lookup table ‚Äì nh·ªè)\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "customers = spark.createDataFrame([\n",
        "    (\"1\", \"VN\", \"LOW\"),\n",
        "    (\"2\", \"SG\", \"MED\"),\n",
        "    (\"3\", \"TH\", \"HIGH\"),\n",
        "], [\"customer_id\", \"country\", \"risk\"])\n",
        "\n",
        "customers.show()\n",
        "```\n",
        "\n",
        "Orders (fact table ‚Äì l·ªõn h∆°n)\n",
        "\n",
        "```python\n",
        "orders = spark.createDataFrame([\n",
        "    (\"o1\", \"1\", 100.0),\n",
        "    (\"o2\", \"1\", 200.0),\n",
        "    (\"o3\", \"2\", 300.0),\n",
        "    (\"o4\", \"3\", 400.0),\n",
        "], [\"order_id\", \"customer_id\", \"amount\"])\n",
        "\n",
        "orders.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ JOIN c∆° b·∫£n (KH√îNG broadcast)\n",
        "\n",
        "```python\n",
        "orders.join(customers, \"customer_id\").explain()\n",
        "```\n",
        "\n",
        "### üî• Quan s√°t Physical Plan\n",
        "\n",
        "B·∫°n s·∫Ω th·∫•y:\n",
        "\n",
        "```code\n",
        "SortMergeJoin\n",
        "Exchange hashpartitioning(customer_id, 200)\n",
        "```\n",
        "üëâ C√≥ Exchange = SHUFFLE\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ V√¨ sao JOIN g√¢y shuffle?\n",
        "\n",
        "**JOIN c·∫ßn:**\n",
        "\n",
        "*\tRecord c√πng customer_id\n",
        "*\tNh∆∞ng data ƒëang ·ªü nhi·ªÅu executor\n",
        "\n",
        "üëâ Spark ph·∫£i shuffle c·∫£ 2 b·∫£ng\n",
        "\n",
        "üìå JOIN = shuffle √ó 2 b·∫£ng\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Broadcast Join (V≈® KH√ç NGUY HI·ªÇM)\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "orders.join(\n",
        "    broadcast(customers),\n",
        "    \"customer_id\"\n",
        ").explain()\n",
        "```\n",
        "\n",
        "### üî• Quan s√°t plan\n",
        "\n",
        "```code\n",
        "BroadcastHashJoin\n",
        "BroadcastExchange\n",
        "```\n",
        "\n",
        "üëâ Kh√¥ng c√≤n Exchange tr√™n orders\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Broadcast th·ª±c s·ª± l√† g√¨?\n",
        "*\tSpark copy b·∫£ng nh·ªè\n",
        "*\tG·ª≠i sang t·∫•t c·∫£ executor\n",
        "*\tM·ªói executor JOIN local\n",
        "\n",
        "üìå Nhanh v√¨ kh√¥ng shuffle fact table\n",
        "\n",
        "---\n",
        "\n",
        "6Ô∏è‚É£ V√¨ sao broadcast c√≥ th·ªÉ g√¢y OOM?\n",
        "\n",
        "Gi·∫£ s·ª≠:\n",
        "\n",
        "*\tcustomers = 5GB\n",
        "*\texecutor memory = 4GB\n",
        "\n",
        "üëâ M·ªói executor ph·∫£i load 5GB\n",
        "\n",
        "üí• JVM ch·∫øt\n",
        "\n",
        "üí• Job fail\n",
        "\n",
        "üí• Incident\n",
        "\n",
        "---\n",
        "\n",
        "## 7Ô∏è‚É£ T·∫Øt auto broadcast ƒë·ªÉ quan s√°t\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "orders.join(customers, \"customer_id\").explain()\n",
        "```\n",
        "\n",
        "üëâ Spark b·∫Øt bu·ªôc d√πng SortMergeJoin\n",
        "\n",
        "üìå ƒê√¢y l√† c√°ch debug JOIN strategy\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Khi n√†o Spark ch·ªçn BroadcastHashJoin?\n",
        "\n",
        "**Spark ch·ªçn broadcast khi:**\n",
        "\n",
        "*\tB·∫£ng nh·ªè < `spark.sql.autoBroadcastJoinThreshold` (10MB default)\n",
        "*\tJoin key ƒë∆°n gi·∫£n\n",
        "*\tKh√¥ng b·ªã hint override\n",
        "\n",
        "---\n",
        "\n",
        "## 9Ô∏è‚É£ Khi n√†o Spark ch·ªçn SortMergeJoin?\n",
        "*\tC·∫£ 2 b·∫£ng l·ªõn\n",
        "*\tBroadcast b·ªã disable\n",
        "*\tJoin key sortable\n",
        "\n",
        "üìå SortMergeJoin = shuffle n·∫∑ng nh·∫•t\n",
        "\n",
        "---\n",
        "\n",
        "## üîü So s√°nh chi·∫øn l∆∞·ª£c JOIN\n",
        "\n",
        "|**Strategy**|**Shuffle**|**Risk**|\n",
        "|------------|-----------|--------|\n",
        "|BroadcastHashJoin|\t‚ùå|\tüí£ OOM|\n",
        "|SortMergeJoin\t|‚úÖ\t|üêå Ch·∫≠m|\n",
        "|ShuffledHashJoin\t|‚úÖ|\t‚ö†Ô∏è √çt d√πng|\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£1Ô∏è‚É£ JOIN bank-grade (QUY T·∫ÆC S·ªêNG)\n",
        "\n",
        "### ‚úÖ N√äN\n",
        "*\tBroadcast lookup < 50MB\n",
        "*\tBroadcast dimension table\n",
        "*\tExplain tr∆∞·ªõc khi ch·∫°y\n",
        "*\tCache lookup n·∫øu reuse\n",
        "\n",
        "### ‚ùå KH√îNG\n",
        "*\tBroadcast table kh√¥ng r√µ size\n",
        "*\tJoin nhi·ªÅu b·∫£ng l·ªõn\n",
        "*\tJoin trong Gold layer\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£2Ô∏è‚É£ C√¢u h·ªèi s·ªëng c√≤n (PH·∫¢I TR·∫¢ L·ªúI)\n",
        "\n",
        "**Tr·∫£ l·ªùi b·∫±ng √Ω hi·ªÉu c·ªßa b·∫°n:**\n",
        "\n",
        "1.\tV√¨ sao JOIN lu√¥n nguy hi·ªÉm h∆°n GROUP BY?\n",
        "2.\tBroadcast nhanh h∆°n v√¨ sao?\n",
        "3.\tV√¨ sao broadcast sai g√¢y OOM?\n",
        "4.\tKhi n√†o n√™n t·∫Øt auto broadcast?\n",
        "5.\tJoin trong Gold layer v√¨ sao b·ªã c·∫•m?\n",
        "\n"
      ],
      "metadata": {
        "id": "roSc8FZPrBSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "customers = spark.createDataFrame([\n",
        "    (\"1\", \"VN\", \"LOW\"),\n",
        "    (\"2\", \"SG\", \"MED\"),\n",
        "    (\"3\", \"TH\", \"HIGH\"),\n",
        "], [\"customer_id\", \"country\", \"risk\"])\n",
        "\n",
        "customers.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgfsUbJ_uDes",
        "outputId": "3cd1b660-fcec-457c-d8ae-b43f72645f72"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+----+\n",
            "|customer_id|country|risk|\n",
            "+-----------+-------+----+\n",
            "|          1|     VN| LOW|\n",
            "|          2|     SG| MED|\n",
            "|          3|     TH|HIGH|\n",
            "+-----------+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders = spark.createDataFrame([\n",
        "    (\"o1\", \"1\", 100.0),\n",
        "    (\"o2\", \"1\", 200.0),\n",
        "    (\"o3\", \"2\", 300.0),\n",
        "    (\"o4\", \"3\", 400.0),\n",
        "], [\"order_id\", \"customer_id\", \"amount\"])\n",
        "\n",
        "orders.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WxpvwvQuLWF",
        "outputId": "6549bd9a-6d63-46b0-e869-816334160de4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+\n",
            "|order_id|customer_id|amount|\n",
            "+--------+-----------+------+\n",
            "|      o1|          1| 100.0|\n",
            "|      o2|          1| 200.0|\n",
            "|      o3|          2| 300.0|\n",
            "|      o4|          3| 400.0|\n",
            "+--------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "orders.join(customers, \"customer_id\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui6Gvxu-uOln",
        "outputId": "1761fee6-4850-4729-e0bd-3e3b46687aa7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#58, order_id#57, amount#59, country#45, risk#46]\n",
            "   +- SortMergeJoin [customer_id#58], [customer_id#44], Inner\n",
            "      :- Sort [customer_id#58 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(customer_id#58, 50), ENSURE_REQUIREMENTS, [plan_id=185]\n",
            "      :     +- Filter isnotnull(customer_id#58)\n",
            "      :        +- Scan ExistingRDD[order_id#57,customer_id#58,amount#59]\n",
            "      +- Sort [customer_id#44 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(customer_id#44, 50), ENSURE_REQUIREMENTS, [plan_id=186]\n",
            "            +- Filter isnotnull(customer_id#44)\n",
            "               +- Scan ExistingRDD[customer_id#44,country#45,risk#46]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "orders.join(\n",
        "    broadcast(customers),\n",
        "    \"customer_id\"\n",
        ").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f57pqCDtua9w",
        "outputId": "422a77cf-9a89-4ea2-d4f7-d98379b0a629"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#58, order_id#57, amount#59, country#45, risk#46]\n",
            "   +- BroadcastHashJoin [customer_id#58], [customer_id#44], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(customer_id#58)\n",
            "      :  +- Scan ExistingRDD[order_id#57,customer_id#58,amount#59]\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=215]\n",
            "         +- Filter isnotnull(customer_id#44)\n",
            "            +- Scan ExistingRDD[customer_id#44,country#45,risk#46]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NbmOx65pxGqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "orders.join(customers, \"customer_id\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YL_TLyRfuoYj",
        "outputId": "c8743a1f-a27d-4f5d-a781-5d5bcacfccaa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#58, order_id#57, amount#59, country#45, risk#46]\n",
            "   +- SortMergeJoin [customer_id#58], [customer_id#44], Inner\n",
            "      :- Sort [customer_id#58 ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(customer_id#58, 50), ENSURE_REQUIREMENTS, [plan_id=243]\n",
            "      :     +- Filter isnotnull(customer_id#58)\n",
            "      :        +- Scan ExistingRDD[order_id#57,customer_id#58,amount#59]\n",
            "      +- Sort [customer_id#44 ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(customer_id#44, 50), ENSURE_REQUIREMENTS, [plan_id=244]\n",
            "            +- Filter isnotnull(customer_id#44)\n",
            "               +- Scan ExistingRDD[customer_id#44,country#45,risk#46]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LAB 6 ‚Äì ƒë√¢y l√† n√∫t th·∫Øt cu·ªëi c√πng khi·∫øn Spark ‚Äúƒë·ª©ng h√¨nh‚Äù d√π code ƒë√∫ng & cluster c√≤n tr·ªëng.\n",
        "\n",
        "---\n",
        "\n",
        "# üí• LAB 6 ‚Äì DATA SKEW, SALTING & AQE\n",
        "\n",
        "> *‚ÄúJob ch·∫°y 99% r·ªìi‚Ä¶ ƒë·ª©ng lu√¥n 40 ph√∫t‚Äù*\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ M·ª•c ti√™u LAB 6\n",
        "\n",
        "**Sau lab n√†y b·∫°n ph·∫£i hi·ªÉu s√¢u:**\n",
        "\n",
        "*\tData skew l√† g√¨ (b·∫±ng M·∫ÆT üëÄ)\n",
        "*\tV√¨ sao 1 key c√≥ th·ªÉ gi·∫øt c·∫£ cluster\n",
        "*\tV√¨ sao retry kh√¥ng c·ª©u ƒë∆∞·ª£c skew\n",
        "*\tSalting l√† g√¨ ‚Äì d√πng khi n√†o\n",
        "*\tAQE x·ª≠ l√Ω skew th·∫ø n√†o (Spark 3+ / 4)\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ T·∫°o data c√≥ SKEW (R√ï R√ÄNG)\n",
        "\n",
        "Orders ‚Äì 1 customer chi·∫øm 70%\n",
        "\n",
        "```python\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "orders = (\n",
        "    spark.range(0, 100_000)\n",
        "    .select(\n",
        "        F.col(\"id\").alias(\"order_id\"),\n",
        "        F.when(F.col(\"id\") < 70_000, F.lit(\"HOT\"))\n",
        "         .otherwise(F.lit(\"COLD\"))\n",
        "         .alias(\"customer_id\"),\n",
        "        (F.rand() * 1000).alias(\"amount\")\n",
        "    )\n",
        ")\n",
        "\n",
        "orders.groupBy(\"customer_id\").count().show()\n",
        "```\n",
        "\n",
        "#### üëâ K·∫øt qu·∫£:\n",
        "\n",
        "```code\n",
        "HOT   ~70,000\n",
        "COLD  ~30,000\n",
        "```\n",
        "\n",
        "üìå ƒê√¢y l√† skew\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ GroupBy g√¢y th·∫£m h·ªça\n",
        "\n",
        "```python\n",
        "orders.groupBy(\"customer_id\").sum(\"amount\").explain()\n",
        "```\n",
        "\n",
        "B·∫°n s·∫Ω th·∫•y:\n",
        "\n",
        "```code\n",
        "Exchange hashpartitioning(customer_id, 200)\n",
        "```\n",
        "\n",
        "üëâ 1 partition x·ª≠ l√Ω 70% d·ªØ li·ªáu\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ ƒêi·ªÅu g√¨ x·∫£y ra trong th·ª±c t·∫ø?\n",
        "\n",
        "*\t199 task: DONE\n",
        "*\t1 task (HOT key): ch·∫°y m√£i\n",
        "*\tStage stuck 99%\n",
        "\n",
        "üìå Cluster r·∫£nh nh∆∞ng job kh√¥ng xong\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ V√¨ sao retry kh√¥ng c·ª©u ƒë∆∞·ª£c skew?\n",
        "\n",
        "*\tRetry = ch·∫°y l·∫°i c√πng logic\n",
        "*\tSkew v·∫´n t·ªìn t·∫°i\n",
        "*\tTask HOT v·∫´n ch·∫øt\n",
        "\n",
        "üëâ Retry ‚â† Fix design\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ SALTING ‚Äì C√°ch c·ªï ƒëi·ªÉn (NH∆ØNG HI·ªÜU QU·∫¢)\n",
        "\n",
        "Th√™m salt cho HOT key\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import concat, lit, rand, floor\n",
        "\n",
        "orders_salted = orders.withColumn(\n",
        "    \"salted_key\",\n",
        "    F.when(\n",
        "        F.col(\"customer_id\") == \"HOT\",\n",
        "        concat(F.col(\"customer_id\"), lit(\"_\"), floor(rand() * 10))\n",
        "    ).otherwise(F.col(\"customer_id\"))\n",
        ")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "GroupBy v·ªõi salted key\n",
        "\n",
        "```python\n",
        "orders_salted.groupBy(\"salted_key\").sum(\"amount\").show()\n",
        "```\n",
        "\n",
        "üëâ HOT ƒë∆∞·ª£c chia th√†nh:\n",
        "\n",
        "```code\n",
        "HOT_0\n",
        "HOT_1\n",
        "...\n",
        "HOT_9\n",
        "```\n",
        "\n",
        "üìå Skew ƒë∆∞·ª£c chia nh·ªè\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Nh∆∞·ª£c ƒëi·ªÉm c·ªßa salting\n",
        "\n",
        "* ‚ùå Code ph·ª©c t·∫°p\n",
        "* ‚ùå Ph·∫£i ‚Äúunsalt‚Äù l·∫°i\n",
        "* ‚ùå Kh√¥ng generic\n",
        "* ‚ùå Kh√≥ maintain\n",
        "\n",
        "üëâ Bank r·∫•t ng·∫°i salting th·ªß c√¥ng\n",
        "\n",
        "---\n",
        "\n",
        "## 7Ô∏è‚É£ AQE ‚Äì Adaptive Query Execution (C·ª®U TINH)\n",
        "\n",
        "#### B·∫≠t AQE\n",
        "\n",
        "```pythin\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### Ch·∫°y l·∫°i job\n",
        "\n",
        "```python\n",
        "orders.groupBy(\"customer_id\").sum(\"amount\").explain()\n",
        "```\n",
        "#### üëâ Spark s·∫Ω:\n",
        "*\tDetect skew partition\n",
        "*\tT·ª± ƒë·ªông split\n",
        "*\tKh√¥ng c·∫ßn salting th·ªß c√¥ng\n",
        "\n",
        "üìå AQE = salting t·ª± ƒë·ªông ·ªü runtime\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Khi n√†o AQE KH√îNG c·ª©u ƒë∆∞·ª£c?\n",
        "\n",
        "* ‚ùå Spark < 3\n",
        "* ‚ùå Join logic qu√° ph·ª©c t·∫°p\n",
        "* ‚ùå Skew c·ª±c n·∫∑ng (1 key = 99%)\n",
        "* ‚ùå Memory kh√¥ng ƒë·ªß\n",
        "\n",
        "---\n",
        "\n",
        "## 9Ô∏è‚É£ Bank-grade rules v·ªÅ skew\n",
        "\n",
        "### ‚úÖ N√äN\n",
        "*\tDetect skew s·ªõm (groupBy count)\n",
        "*\tB·∫≠t AQE m·∫∑c ƒë·ªãnh\n",
        "*\tLog skew metrics\n",
        "*\tThi·∫øt k·∫ø data tr√°nh hot key\n",
        "\n",
        "### ‚ùå KH√îNG\n",
        "*\tRetry v√¥ h·∫°n\n",
        "*\tBroadcast table skewed\n",
        "*\tJoin skewed key trong Gold\n",
        "\n",
        "---\n",
        "\n",
        "## üîü CHECKPOINT T∆Ø DUY (R·∫§T QUAN TR·ªåNG)\n",
        "\n",
        "**Tr·∫£ l·ªùi b·∫±ng l·ªùi c·ªßa b·∫°n:**\n",
        "\n",
        "1.\tV√¨ sao skew l√†m Spark ch·∫≠m d√π CPU r·∫£nh?\n",
        "2.\tV√¨ sao 1 task l√¢u block c·∫£ stage?\n",
        "3.\tSalting gi·∫£i quy·∫øt skew b·∫±ng c√°ch n√†o?\n",
        "4.\tAQE t·ªët h∆°n salting ·ªü ƒëi·ªÉm n√†o?\n",
        "5.\tKhi n√†o KH√îNG n√™n d√πng salting?\n"
      ],
      "metadata": {
        "id": "bHJKZNx9xNnE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tao data skew\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "orders = (\n",
        "    spark.range(0, 100_000)\n",
        "    .select(\n",
        "        F.col(\"id\").alias(\"order_id\"),\n",
        "        F.when(F.col(\"id\") < 70_000, F.lit(\"HOT\"))\n",
        "         .otherwise(F.lit(\"COLD\"))\n",
        "         .alias(\"customer_id\"),\n",
        "        (F.rand() * 1000).alias(\"amount\")\n",
        "    )\n",
        ")\n",
        "\n",
        "orders.groupBy(\"customer_id\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FvX9R3P5DDd",
        "outputId": "69128dc5-2696-4120-bec6-faf86131c1de"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+\n",
            "|customer_id|count|\n",
            "+-----------+-----+\n",
            "|        HOT|70000|\n",
            "|       COLD|30000|\n",
            "+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# group by g√¢y th·∫£m ho·∫°\n",
        "\n",
        "orders.groupBy(\"customer_id\").sum(\"amount\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcIt9dqM5Qh5",
        "outputId": "cb22fae4-3a55-4a23-f3b0-5742cb834e88"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[customer_id#72], functions=[sum(amount#73)])\n",
            "   +- Exchange hashpartitioning(customer_id#72, 50), ENSURE_REQUIREMENTS, [plan_id=307]\n",
            "      +- HashAggregate(keys=[customer_id#72], functions=[partial_sum(amount#73)])\n",
            "         +- Project [CASE WHEN (id#70L < 70000) THEN HOT ELSE COLD END AS customer_id#72, (rand(-4169074559868208654) * 1000.0) AS amount#73]\n",
            "            +- Range (0, 100000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# salting\n",
        "\n",
        "from pyspark.sql.functions import concat, lit, rand, floor\n",
        "\n",
        "orders_salted = orders.withColumn(\n",
        "    \"salted_key\",\n",
        "    F.when(\n",
        "        F.col(\"customer_id\") == \"HOT\",\n",
        "        concat(F.col(\"customer_id\"), lit(\"_\"), floor(rand() * 10))\n",
        "    ).otherwise(F.col(\"customer_id\"))\n",
        ")"
      ],
      "metadata": {
        "id": "GCK1HqAU5jW3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# group by with salting\n",
        "\n",
        "orders_salted.groupBy(\"salted_key\").sum(\"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY-9tL1M5qnk",
        "outputId": "5ee2a404-ac53-4f2a-ae5f-1a92aecd3697"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+\n",
            "|salted_key|         sum(amount)|\n",
            "+----------+--------------------+\n",
            "|     HOT_6|  3480921.2529835147|\n",
            "|     HOT_0|   3549517.763600345|\n",
            "|     HOT_1|  3481041.7993842703|\n",
            "|     HOT_4|  3471754.3236366506|\n",
            "|     HOT_7|  3448158.3647908983|\n",
            "|     HOT_8|   3496083.587307894|\n",
            "|     HOT_3|  3530541.7772539984|\n",
            "|     HOT_2|  3474265.2077110955|\n",
            "|     HOT_9|    3493761.31312631|\n",
            "|     HOT_5|  3527731.2368190824|\n",
            "|      COLD|1.5049008286771532E7|\n",
            "+----------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b·∫≠t AQE\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "\n",
        "orders.groupBy(\"customer_id\").sum(\"amount\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXyHHwve50ts",
        "outputId": "2354899b-52d7-414f-9b18-4dfd7b06294e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[customer_id#72], functions=[sum(amount#73)])\n",
            "   +- Exchange hashpartitioning(customer_id#72, 50), ENSURE_REQUIREMENTS, [plan_id=373]\n",
            "      +- HashAggregate(keys=[customer_id#72], functions=[partial_sum(amount#73)])\n",
            "         +- Project [CASE WHEN (id#70L < 70000) THEN HOT ELSE COLD END AS customer_id#72, (rand(-4169074559868208654) * 1000.0) AS amount#73]\n",
            "            +- Range (0, 100000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† RECAP KI·∫æN TR√öC ‚Äì SPARK LAB 4 ‚Üí 5 ‚Üí 6\n",
        "\n",
        "(Shuffle ‚Äì Join ‚Äì Skew)\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 1Ô∏è‚É£ Big Picture: Spark th·ª±c s·ª± ch·∫°y nh∆∞ th·∫ø n√†o?\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    A[Source Data] --> B[Transformations]\n",
        "    B -->|Action| C[DAG]\n",
        "    C --> D[Stages]\n",
        "    D --> E[Tasks]\n",
        "    E --> F[Executors]\n",
        "```\n",
        "\n",
        "üìå M·ªçi th·ª© ch·ªâ x·∫£y ra khi c√≥ ACTION\n",
        "\n",
        "üìå Performance ch·∫øt ·ªü Stage boundary (shuffle)\n",
        "\n",
        "---\n",
        "\n",
        "## üîÄ 2Ô∏è‚É£ LAB 4 ‚Äì SHUFFLE (K·∫∫ GI·∫æT SPARK)\n",
        "\n",
        "Khi n√†o shuffle x·∫£y ra?\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    A[map / filter] -->|NO shuffle| B[Same partition]\n",
        "    B --> C[groupBy / join / distinct]\n",
        "    C -->|SHUFFLE| D[Exchange]\n",
        "```\n",
        "\n",
        "**V√¨ sao shuffle nguy hi·ªÉm?**\n",
        "\n",
        "-\tDisk I/O\n",
        "-\tNetwork transfer\n",
        "-\tMemory pressure\n",
        "-\tStage m·ªõi ‚Üí block pipeline\n",
        "\n",
        "> üìå M·ªói Exchange = r·ªßi ro\n",
        "\n",
        "---\n",
        "\n",
        "## üîó 3Ô∏è‚É£ LAB 5 ‚Äì JOIN STRATEGIES\n",
        "\n",
        "C√°c chi·∫øn l∆∞·ª£c join\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    A[Join] --> B{Table nh·ªè?}\n",
        "    B -->|YES| C[BroadcastHashJoin]\n",
        "    B -->|NO| D{Sorted?}\n",
        "    D -->|YES| E[SortMergeJoin]\n",
        "    D -->|NO| F[ShuffleHashJoin]\n",
        "```\n",
        "\n",
        "**Th·ª© t·ª± ∆∞u ti√™n (Spark):**\n",
        "\n",
        "1.\tBroadcastHashJoin ‚≠ê\n",
        "2.\tSortMergeJoin\n",
        "3.\tShuffleHashJoin ‚ùå\n",
        "\n",
        "> üìå Join sai = cost + runtime + OOM\n",
        "\n",
        "---\n",
        "\n",
        "## üí• 4Ô∏è‚É£ LAB 6 ‚Äì DATA SKEW (K·∫∫ TH√ô TH·∫¶M L·∫∂NG)\n",
        "\n",
        "Skew tr√¥ng nh∆∞ th·∫ø n√†o?\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    P1[Partition 1] -->|1% data| OK1[Fast]\n",
        "    P2[Partition 2] -->|1% data| OK2[Fast]\n",
        "    P3[Partition 3] -->|97% data| SLOW[Very slow üò±]\n",
        "```\n",
        "\n",
        "> üìå 1 task ch·∫≠m ‚Üí c·∫£ stage ƒë·ª©ng h√¨nh\n",
        "\n",
        "---\n",
        "\n",
        "## üßÇ 5Ô∏è‚É£ SALTING vs AQE\n",
        "\n",
        "### Salting (manual)\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    HOT[HOT key] --> S1[HOT_1]\n",
        "    HOT --> S2[HOT_2]\n",
        "    HOT --> S3[HOT_3]\n",
        "```\n",
        "\n",
        "**‚úî Chia t·∫£i**\n",
        "‚ùå Code ph·ª©c t·∫°p\n",
        "\n",
        "‚ùå Kh√≥ maintain\n",
        "\n",
        "---\n",
        "\n",
        "### AQE (Adaptive Query Execution)\n",
        "\n",
        "```mermaid\n",
        "flowchart LR\n",
        "    Spark[Runtime] --> Detect[Detect Skew]\n",
        "    Detect --> Split[Split Partition]\n",
        "    Split --> Balanced[Balanced Tasks]\n",
        "```\n",
        "\n",
        "‚úî T·ª± ƒë·ªông\n",
        "\n",
        "‚úî Clean code\n",
        "\n",
        "‚úî Bank th√≠ch üòÑ\n",
        "\n",
        "‚ùå Kh√¥ng c·ª©u ƒë∆∞·ª£c design sai n·∫∑ng\n",
        "\n",
        "---\n",
        "\n",
        "## üè¶ 6Ô∏è‚É£ BANK-GRADE RULES (C·ª∞C QUAN TR·ªåNG)\n",
        "\n",
        "### ‚ùå KH√îNG L√ÄM\n",
        "-\tJoin nhi·ªÅu b·∫£ng l·ªõn ·ªü Gold\n",
        "-\tRetry v√¥ h·∫°n\n",
        "-\tBroadcast table skew\n",
        "-\tTin config c·ª©u design\n",
        "\n",
        "### ‚úÖ PH·∫¢I L√ÄM\n",
        "-\tExplain plan tr∆∞·ªõc khi ch·∫°y\n",
        "-\tDetect skew s·ªõm\n",
        "-\tAQE b·∫≠t m·∫∑c ƒë·ªãnh\n",
        "-\tJob < SLA / 2\n",
        "-\tIdempotent output\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 7Ô∏è‚É£ T∆Ø DUY KI·∫æN TR√öC S∆Ø (ƒêINH N√ÉO)\n",
        "\n",
        "Spark ch·∫≠m kh√¥ng ph·∫£i v√¨ cluster y·∫øu\n",
        "‚Üí M√† v√¨ data shape & execution plan sai\n",
        "\n",
        "---\n",
        "\n",
        "## üîú B∆Ø·ªöC TI·∫æP THEO ‚Äì LAB 7 (TH·ª∞C CHI·∫æN T·ªîNG H·ª¢P)\n",
        "\n",
        "LAB 7 s·∫Ω l√†m g√¨?\n",
        "\n",
        "‚úî Orders l·ªõn (skew)\n",
        "\n",
        "‚úî Customers nh·ªè\n",
        "\n",
        "‚úî Join ƒë√∫ng chi·∫øn l∆∞·ª£c\n",
        "\n",
        "‚úî AQE b·∫≠t\n",
        "\n",
        "‚úî Explain & reasoning nh∆∞ production\n",
        "\n",
        "‚úî Ph√¢n t√≠ch ‚Äún·∫øu job n√†y ch·∫°y ·ªü bank th√¨ ·ªïn ch∆∞a?‚Äù\n",
        "\n"
      ],
      "metadata": {
        "id": "hIvvzMfX6bCz"
      }
    }
  ]
}