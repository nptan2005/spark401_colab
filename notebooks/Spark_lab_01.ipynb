{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBB2omIg+GIJRtbNiP5AOC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nptan2005/spark401_colab/blob/main/notebooks/Spark_lab_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vVFChov3bToM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c735c16a-8969-4f5b-dc26-e1072b2dc71c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libgail-common libgail18 libgtk2.0-0\n",
            "  libgtk2.0-bin libgtk2.0-common librsvg2-common libxcomposite1 libxt-dev\n",
            "  libxtst6 libxxf86dga1 openjdk-17-jre session-migration x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libxt-doc openjdk-17-demo openjdk-17-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libgail-common libgail18 libgtk2.0-0\n",
            "  libgtk2.0-bin libgtk2.0-common librsvg2-common libxcomposite1 libxt-dev\n",
            "  libxtst6 libxxf86dga1 openjdk-17-jdk openjdk-17-jre session-migration\n",
            "  x11-utils\n",
            "0 upgraded, 24 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 8,212 kB of archives.\n",
            "After this operation, 24.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre amd64 17.0.17+10-1~22.04 [238 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk amd64 17.0.17+10-1~22.04 [1,521 kB]\n",
            "Fetched 8,212 kB in 2s (5,288 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../05-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../06-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../07-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../08-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../09-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../10-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../11-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../12-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../13-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../14-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../15-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../16-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../17-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../18-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../19-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../20-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../21-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-17-jre:amd64.\n",
            "Preparing to unpack .../22-openjdk-17-jre_17.0.17+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jre:amd64 (17.0.17+10-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-17-jdk:amd64.\n",
            "Preparing to unpack .../23-openjdk-17-jdk_17.0.17+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk:amd64 (17.0.17+10-1~22.04) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service ‚Üí /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up openjdk-17-jre:amd64 (17.0.17+10-1~22.04) ...\n",
            "Setting up openjdk-17-jdk:amd64 (17.0.17+10-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y openjdk-17-jdk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
        "!tar xf spark-4.0.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1PVveznIbssp",
        "outputId": "77dff5e5-729d-48fb-89c6-701732a6ca82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-10 01:48:59--  https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548955321 (524M) [application/x-gzip]\n",
            "Saving to: ‚Äòspark-4.0.1-bin-hadoop3.tgz‚Äô\n",
            "\n",
            "spark-4.0.1-bin-had 100%[===================>] 523.52M  21.0MB/s    in 24s     \n",
            "\n",
            "2026-01-10 01:49:24 (21.5 MB/s) - ‚Äòspark-4.0.1-bin-hadoop3.tgz‚Äô saved [548955321/548955321]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# Spark 4.0.1 Setup (REQUIRED)\n",
        "# ===============================\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-4.0.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":/content/spark-4.0.1-bin-hadoop3/bin\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark401-Training\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPFBOiclb_i8",
        "outputId": "f926e0a3-5d04-48df-fb68-4a2148be441d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LAB SPARK\n",
        "\n",
        "---\n",
        "\n",
        "##M·ª•c ti√™u cu·ªëi:\n",
        "\n",
        "B·∫°n c√≥ th·ªÉ t·ª± tin vi·∫øt ‚Äì ƒë·ªçc ‚Äì debug ‚Äì t·ªëi ∆∞u Spark job t·ª´ local ‚Üí cloud (Dataproc / EMR / Synapse)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KQlRueU9cLlm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ LAB 1 ‚Äì DATAFRAME C∆† B·∫¢N (FOUNDATION)\n",
        "\n",
        "## üéØ M·ª•c ti√™u\n",
        "*\tHi·ªÉu DataFrame l√† g√¨\n",
        "*\tHi·ªÉu Spark kh√°c Pandas ·ªü ƒëi·ªÉm n√†o\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ T·∫°o data m·∫´u\n",
        "\n",
        "### üîπ Spark\n"
      ],
      "metadata": {
        "id": "3udWuA4CcYQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "\n",
        "spark = SparkSession.builder.appName(\"lab1\").getOrCreate()\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(1, \"Alice\", 1000),\n",
        "     (2, \"Bob\", 2000),\n",
        "     (3, \"Charlie\", 3000)],\n",
        "    [\"id\", \"name\", \"balance\"]\n",
        ")"
      ],
      "metadata": {
        "id": "dxf1Skr6cpCT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ X·ª≠ l√Ω"
      ],
      "metadata": {
        "id": "4jukLkR1cvLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.filter(F.col(\"balance\") > 1500)"
      ],
      "metadata": {
        "id": "KEyHQDDocyZD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Gi·∫£i th√≠ch:\n",
        "\n",
        "*\tfilter() KH√îNG ch·∫°y\n",
        "*\tSpark ch·ªâ ghi l·∫°i logical plan\n",
        "*\tCh∆∞a c√≥ action ‚Üí ch∆∞a c√≥ CPU / RAM d√πng\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Action (k√≠ch ho·∫°t Spark)"
      ],
      "metadata": {
        "id": "qyeWO8c-c69i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hysz0N59dNyE",
        "outputId": "c72cf277-9d37-40c4-bee0-e6cbb2207056"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------+\n",
            "| id|   name|balance|\n",
            "+---+-------+-------+\n",
            "|  2|    Bob|   2000|\n",
            "|  3|Charlie|   3000|\n",
            "+---+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üëâ L√∫c n√†y Spark m·ªõi:\n",
        "\n",
        "*\tBuild DAG\n",
        "*\tT·∫°o stage\n",
        "*\tT·∫°o task\n",
        "*\tTh·ª±c thi"
      ],
      "metadata": {
        "id": "_xPMQcFDd2dm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ So v·ªõi Pandas"
      ],
      "metadata": {
        "id": "d4CGnvgtfFS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pdf = pd.DataFrame({\n",
        "    \"id\": [1,2,3],\n",
        "    \"name\": [\"Alice\",\"Bob\",\"Charlie\"],\n",
        "    \"balance\": [1000,2000,3000]\n",
        "})\n",
        "\n",
        "pdf[pdf[\"balance\"] > 1500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "P8XJb95zfG4H",
        "outputId": "0680d5d7-d2c1-44cd-cd06-827b3c29b49e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id     name  balance\n",
              "1   2      Bob     2000\n",
              "2   3  Charlie     3000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91a3e47d-5e53-4b4f-b5b0-95d724cfcf4c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>balance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Bob</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Charlie</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91a3e47d-5e53-4b4f-b5b0-95d724cfcf4c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-91a3e47d-5e53-4b4f-b5b0-95d724cfcf4c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-91a3e47d-5e53-4b4f-b5b0-95d724cfcf4c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-93b94f2a-5ee9-433f-8e35-e571f44b2827\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-93b94f2a-5ee9-433f-8e35-e571f44b2827')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-93b94f2a-5ee9-433f-8e35-e571f44b2827 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pdf[pdf[\\\"balance\\\"] > 1500]\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 3,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Charlie\",\n          \"Bob\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 707,\n        \"min\": 2000,\n        \"max\": 3000,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3000,\n          2000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "|**Pandas**|**Spark**|\n",
        "|----------|---------|\n",
        "|Ch·∫°y ngay|Lazy|\n",
        "|RAM m√°y|RAM cluster|\n",
        "|Nh·ªè|R·∫•t l·ªõn|\n",
        "\n",
        "üëâ Pandas = ti·ªán, Spark = s·ªëng s√≥t\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ C√¥ng c·ª• th·ª±c t·∫ø\n",
        "* Spark UI\n",
        "*\tPySpark\n",
        "*\tColab / Local / Databricks\n"
      ],
      "metadata": {
        "id": "5_xnliMvfOhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ LAB 2 ‚Äì READ FILE & PARTITION (C·ª∞C QUAN TR·ªåNG)\n",
        "\n",
        "## üéØ M·ª•c ti√™u\n",
        "*\tHi·ªÉu partition\n",
        "*\tBi·∫øt v√¨ sao file to = ch·∫øt\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ T·∫°o data (CSV gi·∫£ l·∫≠p)\n"
      ],
      "metadata": {
        "id": "ibBT6wcsgAUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(i, f\"user_{i}\", i*10) for i in range(1, 100_001)]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"score\"])"
      ],
      "metadata": {
        "id": "tqBwbSlbgJM1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Ki·ªÉm tra partition"
      ],
      "metadata": {
        "id": "HcCOUzr1gXZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH_-ottLgYkc",
        "outputId": "d1080075-3f9d-4c9b-bdf3-a6743cfc22bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Repartition"
      ],
      "metadata": {
        "id": "srDydZ2Aggol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.repartition(8)\n",
        "df2.rdd.getNumPartitions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ixRJ_FFgiOZ",
        "outputId": "f782bc41-a6e7-44ae-e0cd-6af06bcf92ac"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Gi·∫£i th√≠ch\n",
        "*\tPartition = s·ªë task\n",
        "*\tTask = 1 core\n",
        "*\t√çt partition ‚Üí CPU r·∫£nh\n",
        "*\tNhi·ªÅu qu√° ‚Üí overhead\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Pandas c√≥ kh√¥ng?\n",
        "\n",
        "* ‚ùå Kh√¥ng c√≥ kh√°i ni·ªám partition\n",
        "* üëâ Pandas kh√¥ng scale\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Th·ª±c t·∫ø\n",
        "*\tDataproc / EMR: partition quy·∫øt ƒë·ªãnh cost & SLA\n",
        "*\tBank review partition tr∆∞·ªõc khi review code\n"
      ],
      "metadata": {
        "id": "cUVl0Lsfg9Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü° LAB 3 ‚Äì GROUP BY & SHUFFLE (B∆Ø·ªöC NGO·∫∂T)\n",
        "\n",
        "## üéØ M·ª•c ti√™u\n",
        "*\tTh·∫•y shuffle th·∫≠t s·ª±\n",
        "*\tBi·∫øt v√¨ sao job stuck 99%\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Data"
      ],
      "metadata": {
        "id": "jwrB-sMNhT6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.range(0, 1_000_000).select(\n",
        "    (F.col(\"id\") % 10).alias(\"group_id\"),\n",
        "    F.rand().alias(\"amount\")\n",
        ")"
      ],
      "metadata": {
        "id": "JVNRIJyRhb4t"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ GroupBy"
      ],
      "metadata": {
        "id": "L-qdWBPghl9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agg = df.groupBy(\"group_id\").sum(\"amount\")\n",
        "agg.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUcZxjqQhoM0",
        "outputId": "f496b9f1-988c-4918-c193-2debb7bc44d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[group_id#17L], functions=[sum(amount#18)])\n",
            "   +- Exchange hashpartitioning(group_id#17L, 4), ENSURE_REQUIREMENTS, [plan_id=53]\n",
            "      +- HashAggregate(keys=[group_id#17L], functions=[partial_sum(amount#18)])\n",
            "         +- Project [(id#16L % 10) AS group_id#17L, rand(6847661680794689400) AS amount#18]\n",
            "            +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Gi·∫£i th√≠ch\n",
        "*\tExchange xu·∫•t hi·ªán\n",
        "*\tShuffle b·∫Øt bu·ªôc\n",
        "*\tD·ªØ li·ªáu di chuy·ªÉn gi·ªØa executor\n",
        "\n",
        "> üëâ T·ªën nh·∫•t trong Spark\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Tune nh·∫π:"
      ],
      "metadata": {
        "id": "3zSC5u6chzxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
        "\n",
        "agg = df.groupBy(\"group_id\").sum(\"amount\")\n",
        "agg.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tijXnuxkieaI",
        "outputId": "a834c8f8-aded-4837-e4cf-65a6c2334553"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[group_id#17L], functions=[sum(amount#18)])\n",
            "   +- Exchange hashpartitioning(group_id#17L, 10), ENSURE_REQUIREMENTS, [plan_id=70]\n",
            "      +- HashAggregate(keys=[group_id#17L], functions=[partial_sum(amount#18)])\n",
            "         +- Project [(id#16L % 10) AS group_id#17L, rand(6847661680794689400) AS amount#18]\n",
            "            +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hai k·∫ø ho·∫°ch th·ª±c thi (Physical Plan) m√† b·∫°n cung c·∫•p g·∫ßn nh∆∞ ho√†n to√†n gi·ªëng nhau, ngo·∫°i tr·ª´ m·ªôt ƒëi·ªÉm kh√°c bi·ªát duy nh·∫•t n·∫±m ·ªü b∆∞·ªõc Shuffle (Exchange).\n",
        "\n",
        "D∆∞·ªõi ƒë√¢y l√† b·∫£ng so s√°nh chi ti·∫øt:\n",
        "\n",
        "1. **ƒêi·ªÉm gi·ªëng nhau**\n",
        "\n",
        "C·∫£ hai k·∫ø ho·∫°ch ƒë·ªÅu th·ª±c hi·ªán c√πng m·ªôt logic x·ª≠ l√Ω:\n",
        "\n",
        "**Range:** T·∫°o ra 1.000.000 b·∫£n ghi, chia l√†m 2 partitions ban ƒë·∫ßu.\n",
        "\n",
        "**Project:** T√≠nh to√°n group_id (b·∫±ng c√°ch l·∫•y id % 10) v√† t·∫°o c·ªôt amount ng·∫´u nhi√™n.\n",
        "\n",
        "**Partial Aggregation (HashAggregate):** Spark th·ª±c hi·ªán c·ªông t·ªïng t·∫°m th·ªùi ngay t·∫°i c√°c partition local ƒë·ªÉ gi·∫£m l∆∞·ª£ng d·ªØ li·ªáu c·∫ßn truy·ªÅn qua m·∫°ng (shuffle).\n",
        "\n",
        "**AQE (Adaptive Spark Plan):** C·∫£ hai ƒë·ªÅu ƒëang b·∫≠t t√≠nh nƒÉng t·ª± ƒëi·ªÅu ch·ªânh k·∫ø ho·∫°ch, nh∆∞ng hi·ªán t·∫°i l√† `isFinalPlan=false` (ch∆∞a ph·∫£i b·∫£n ch·ªët cu·ªëi c√πng khi ch·∫°y).\n",
        "\n",
        "---\n",
        "\n",
        "2. **ƒêi·ªÉm kh√°c bi·ªát duy nh·∫•t:**\n",
        "\n",
        "|**ƒê·∫∑c ƒëi·ªÉm**|**K·∫ø ho·∫°ch 1**|**K·∫ø ho·∫°ch 2**|\n",
        "|------------|--------------|--------------|\n",
        "|S·ªë l∆∞·ª£ng Partition khi Shuffle|4 partitions|10 partitions|\n",
        "|Chi ti·∫øt t·∫°i Exchange|\"hashpartitioning(group_id#17L, 4)\"|\"hashpartitioning(group_id#17L, 10)\"|\n",
        "\n",
        "---\n",
        "\n",
        "3. **Ph√¢n t√≠ch t√°c ƒë·ªông**\n",
        "\n",
        "S·ª± kh√°c bi·ªát v·ªÅ con s·ªë **4** v√† **10** n√†y c√≥ √Ω nghƒ©a quan tr·ªçng v·ªÅ hi·ªáu su·∫•t d·ª±a tr√™n d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n:\n",
        "\n",
        "**ƒê·ªô song song (Parallelism):**\n",
        "\n",
        "·ªû k·∫ø ho·∫°ch 1, giai ƒëo·∫°n t√≠nh t·ªïng cu·ªëi c√πng (Final Aggregate) s·∫Ω ch·ªâ c√≥ 4 t√°c v·ª• (tasks) ch·∫°y song song.\n",
        "\n",
        "·ªû k·∫ø ho·∫°ch 2, b·∫°n c√≥ 10 t√°c v·ª• ch·∫°y song song. N·∫øu cluster c·ªßa b·∫°n c√≥ nhi·ªÅu core, k·∫ø ho·∫°ch 2 c√≥ th·ªÉ nhanh h∆°n.\n",
        "\n",
        "**Ph√¢n ph·ªëi d·ªØ li·ªáu (Data Distribution):**\n",
        "\n",
        "Trong code c·ªßa b·∫°n, group_id ƒë∆∞·ª£c t√≠nh b·∫±ng `id % 10`, nghƒ©a l√† c√≥ t·ªëi ƒëa 10 gi√° tr·ªã group_id duy nh·∫•t (t·ª´ 0 ƒë·∫øn 9).\n",
        "\n",
        "**K·∫ø ho·∫°ch 2 (10 partitions):** L√† con s·ªë \"l√Ω t∆∞·ªüng\". M·ªói partition s·∫Ω x·ª≠ l√Ω ch√≠nh x√°c 1 gi√° tr·ªã group_id. D·ªØ li·ªáu ƒë∆∞·ª£c chia ƒë·ªÅu tƒÉm t·∫Øp.\n",
        "\n",
        "**K·∫ø ho·∫°ch 1 (4 partitions):** Spark ph·∫£i d√πng h√†m Hash ƒë·ªÉ gom 10 lo·∫°i group_id n√†y v√†o 4 \"gi·ªè\". K·∫øt qu·∫£ l√† s·∫Ω c√≥ gi·ªè ch·ª©a 2 nh√≥m, c√≥ gi·ªè ch·ª©a 3 nh√≥m. ƒêi·ªÅu n√†y d·∫´n ƒë·∫øn hi·ªán t∆∞·ª£ng Data Skew nh·∫π (l·ªách d·ªØ li·ªáu), m·ªôt s·ªë task s·∫Ω ph·∫£i l√†m vi·ªác nhi·ªÅu h∆°n task kh√°c.\n",
        "\n",
        "---\n",
        "\n",
        "4. **T·∫°i sao l·∫°i c√≥ s·ª± kh√°c bi·ªát n√†y?**\n",
        "\n",
        "S·ª± kh√°c bi·ªát n√†y th∆∞·ªùng ƒë·∫øn t·ª´ vi·ªác c·∫•u h√¨nh tham s·ªë `spark.sql.shuffle.partitions`:\n",
        "\n",
        "**·ªû k·∫ø ho·∫°ch 1**, c√≥ th·ªÉ b·∫°n (ho·∫∑c h·ªá th·ªëng) ƒë√£ set `set(\"spark.sql.shuffle.partitions\", \"4\")`.\n",
        "\n",
        "**·ªû k·∫ø ho·∫°ch 2**, con s·ªë n√†y ƒë∆∞·ª£c set l√† `10`.\n",
        "\n",
        "**L·ªùi khuy√™n:** Trong tr∆∞·ªùng h·ª£p c·ª• th·ªÉ n√†y (c√≥ 10 groups), K·∫ø ho·∫°ch 2 t·ªët h∆°n v√¨ n√≥ t·∫≠n d·ª•ng t·ªëi ƒëa s·ª± ph√¢n b·ªï ƒë·ªÅu c·ªßa d·ªØ li·ªáu v√† tƒÉng kh·∫£ nƒÉng x·ª≠ l√Ω song song."
      ],
      "metadata": {
        "id": "c5NBEL2hjm2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Pandas?"
      ],
      "metadata": {
        "id": "hcI6M4WZi1AD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf.groupby(\"group_id\").sum()\n",
        "# print(pdf.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "kYQWNnglldyT",
        "outputId": "2ee36792-cfb6-483d-9140-34595137e76a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       name  balance\n",
              "id                  \n",
              "1     Alice     1000\n",
              "2       Bob     2000\n",
              "3   Charlie     3000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3ebeebef-5831-4ee6-bbcd-f6ab422003f8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>name</th>\n",
              "      <th>balance</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Alice</td>\n",
              "      <td>1000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bob</td>\n",
              "      <td>2000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Charlie</td>\n",
              "      <td>3000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3ebeebef-5831-4ee6-bbcd-f6ab422003f8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3ebeebef-5831-4ee6-bbcd-f6ab422003f8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3ebeebef-5831-4ee6-bbcd-f6ab422003f8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1567c892-0184-4cd0-9644-bb4e5b2c1b66\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1567c892-0184-4cd0-9644-bb4e5b2c1b66')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1567c892-0184-4cd0-9644-bb4e5b2c1b66 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# print(pdf\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Alice\",\n          \"Bob\",\n          \"Charlie\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"balance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1000,\n        \"min\": 1000,\n        \"max\": 3000,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1000,\n          2000,\n          3000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ‚ùå RAM ch·∫øt n·∫øu l·ªõn\n",
        "\n"
      ],
      "metadata": {
        "id": "GJr3mplFlra-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê·ªÉ so s√°nh m·ªôt c√°ch c√¥ng b·∫±ng gi·ªØa Spark v√† Pandas khi x·ª≠ l√Ω d·ªØ li·ªáu l·ªõn, ch√∫ng ta c·∫ßn l∆∞u √Ω r·∫±ng:\n",
        "\n",
        "Pandas: Ch·∫°y tr√™n RAM v√† s·ª≠ d·ª•ng ƒë∆°n nh√¢n (single-core) l√† ch√≠nh. N·∫øu d·ªØ li·ªáu v∆∞·ª£t qu√° RAM, n√≥ s·∫Ω b·ªã l·ªói (OOM).\n",
        "\n",
        "Spark: Ph√¢n t√°n d·ªØ li·ªáu ra nhi·ªÅu ph√¢n v√πng (partitions) v√† ch·∫°y song song tr√™n nhi·ªÅu nh√¢n/nhi·ªÅu m√°y.\n",
        "\n",
        "D∆∞·ªõi ƒë√¢y l√† m√£ ngu·ªìn ho√†n ch·ªânh ƒë·ªÉ b·∫°n ch·∫°y tr√™n m√¥i tr∆∞·ªùng c√≥ c·∫£ Spark v√† Pandas (nh∆∞ Google Colab ho·∫∑c m√°y local).\n",
        "\n",
        "---\n",
        "\n",
        "### Code So S√°nh Hi·ªáu NƒÉng GroupBy"
      ],
      "metadata": {
        "id": "rMd-Hp0xmZRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, rand\n",
        "import time\n",
        "\n",
        "# 1. Kh·ªüi t·∫°o Spark Session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkVsPandas\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"4g\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# C·∫•u h√¨nh s·ªë l∆∞·ª£ng b·∫£n ghi (V√≠ d·ª•: 10 tri·ªáu d√≤ng)\n",
        "n_rows = 10_000_000\n",
        "\n",
        "print(f\"--- ƒêang t·∫°o t·∫≠p d·ªØ li·ªáu {n_rows} d√≤ng ---\")\n",
        "\n",
        "# 2. T·∫°o d·ªØ li·ªáu m·∫´u b·∫±ng Spark (ƒë·ªÉ t·∫≠n d·ª•ng t·ªëc ƒë·ªô song song khi t·∫°o data l·ªõn)\n",
        "spark_df = spark.range(0, n_rows) \\\n",
        "    .withColumn(\"group_id\", (col(\"id\") % 100).cast(\"string\")) \\\n",
        "    .withColumn(\"amount\", rand(seed=42) * 100)\n",
        "\n",
        "# Chuy·ªÉn sang Pandas ƒë·ªÉ so s√°nh (L∆∞u √Ω: M√°y ph·∫£i ƒë·ªß RAM ƒë·ªÉ ch·ª©a n_rows d√≤ng n√†y)\n",
        "pdf = spark_df.toPandas()\n",
        "\n",
        "print(\"--- B·∫Øt ƒë·∫ßu ƒëo l∆∞·ªùng ---\")\n",
        "\n",
        "# --- TH·ª∞C THI V·ªöI PANDAS ---\n",
        "start_pd = time.time()\n",
        "pd_result = pdf.groupby(\"group_id\")[\"amount\"].sum()\n",
        "end_pd = time.time()\n",
        "pandas_time = end_pd - start_pd\n",
        "print(f\"Th·ªùi gian Pandas th·ª±c thi: {pandas_time:.4f} gi√¢y\")\n",
        "\n",
        "# --- TH·ª∞C THI V·ªöI SPARK ---\n",
        "# L∆∞u √Ω: Trong Spark ph·∫£i d√πng m·ªôt Action (nh∆∞ collect ho·∫∑c show) ƒë·ªÉ k√≠ch ho·∫°t t√≠nh to√°n\n",
        "start_spark = time.time()\n",
        "spark_result = spark_df.groupBy(\"group_id\").sum(\"amount\").collect()\n",
        "end_spark = time.time()\n",
        "spark_time = end_spark - start_spark\n",
        "print(f\"Th·ªùi gian Spark th·ª±c thi: {spark_time:.4f} gi√¢y\")\n",
        "\n",
        "# 3. So s√°nh k·∫øt qu·∫£\n",
        "print(\"\\n--- T√≥m t·∫Øt ---\")\n",
        "if pandas_time < spark_time:\n",
        "    print(f\"K·∫øt qu·∫£: Pandas nhanh h∆°n Spark {spark_time/pandas_time:.2f} l·∫ßn (do d·ªØ li·ªáu v·∫´n v·ª´a RAM v√† kh√¥ng m·∫•t chi ph√≠ qu·∫£n l√Ω cluster)\")\n",
        "else:\n",
        "    print(f\"K·∫øt qu·∫£: Spark nhanh h∆°n Pandas {pandas_time/spark_time:.2f} l·∫ßn\")\n",
        "\n",
        "# ƒê√≥ng Spark Session\n",
        "# spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "id": "nnZnWwFBmhmX",
        "outputId": "c73da129-bad8-4c91-db1c-3fd4ec21f660"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- ƒêang t·∫°o t·∫≠p d·ªØ li·ªáu 10000000 d√≤ng ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o129.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:440)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:463)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:462)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3637/0x00007c8fecea33d0.apply(Unknown Source)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToPython$1(Dataset.scala:2057)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda$3565/0x00007c8fecdc1638.apply(Unknown Source)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda$2443/0x00007c8fecd25210.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda$2082/0x00007c8fecbfaf58.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2106/0x00007c8fecc04b40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2103/0x00007c8fecc042f8.apply(Unknown Source)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager$$Lambda$2104/0x00007c8fecc045c0.apply(Unknown Source)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2097/0x00007c8fecbfe3d8.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4282131445.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Chuy·ªÉn sang Pandas ƒë·ªÉ so s√°nh (L∆∞u √Ω: M√°y ph·∫£i ƒë·ªß RAM ƒë·ªÉ ch·ª©a n_rows d√≤ng n√†y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mpdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- B·∫Øt ƒë·∫ßu ƒëo l∆∞·ªùng ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1791\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"PandasDataFrameLike\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1792\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPandasConversionMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexColumn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ColumnOrName\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mParentDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/pandas/conversion.py\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Below is toPandas without Arrow optimization.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             pdf = pd.DataFrame.from_records(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/classic/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    328\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o129.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:429)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:440)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:426)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:463)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:462)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$3637/0x00007c8fecea33d0.apply(Unknown Source)\n\tat scala.collection.ArrayOps$.foreach$extension(ArrayOps.scala:1324)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:462)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$collectToPython$1(Dataset.scala:2057)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda$3565/0x00007c8fecdc1638.apply(Unknown Source)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda$2443/0x00007c8fecd25210.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\n\tat org.apache.spark.sql.classic.Dataset$$Lambda$2082/0x00007c8fecbfaf58.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2106/0x00007c8fecc04b40.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2103/0x00007c8fecc042f8.apply(Unknown Source)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n\tat org.apache.spark.sql.artifact.ArtifactManager$$Lambda$2104/0x00007c8fecc045c0.apply(Unknown Source)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2097/0x00007c8fecbfe3d8.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ph√¢n T√≠ch S·ª± Kh√°c Bi·ªát\n",
        "Khi b·∫°n ch·∫°y code tr√™n v·ªõi c√°c k√≠ch th∆∞·ªõc d·ªØ li·ªáu kh√°c nhau, b·∫°n s·∫Ω th·∫•y k·∫øt qu·∫£ r·∫•t th√∫ v·ªã:\n",
        "\n",
        "|**K√≠ch th∆∞·ªõc d·ªØ li·ªáu**|**Ai s·∫Ω th·∫Øng?**|**T·∫°i sao?**|\n",
        "|----------------------|----------------|------------|\n",
        "|Nh·ªè (< 1 tri·ªáu d√≤ng)|Pandas|\"Spark m·∫•t th·ªùi gian \"\"kh·ªüi ƒë·ªông\"\" (overhead) ƒë·ªÉ l·∫≠p k·∫ø ho·∫°ch th·ª±c thi v√† chia nh·ªè c√¥ng vi·ªác.\"|\n",
        "|L·ªõn (10 - 100 tri·ªáu d√≤ng)|Spark|Spark t·∫≠n d·ª•ng ƒëa nh√¢n (multi-core) ƒë·ªÉ x·ª≠ l√Ω song song c√°c partitions. Pandas b·∫Øt ƒë·∫ßu b·ªã ngh·∫Ωn ·ªü 1 nh√¢n CPU.|\n",
        "|R·∫•t l·ªõn (> RAM m√°y)|Spark|Pandas s·∫Ω b·ªã l·ªói Out of Memory (OOM). Spark c√≥ th·ªÉ ghi d·ªØ li·ªáu t·∫°m xu·ªëng ƒëƒ©a (spill to disk) ƒë·ªÉ ti·∫øp t·ª•c ch·∫°y.|\n",
        "\n",
        "---\n",
        "\n",
        "2. V·ªÅ C∆° Ch·∫ø Th·ª±c Thi\n",
        "\n",
        "Pandas (Eager Evaluation): Khi b·∫°n g·ªçi .sum(), n√≥ t√≠nh to√°n ngay l·∫≠p t·ª©c.\n",
        "\n",
        "Spark (Lazy Evaluation): Khi b·∫°n g·ªçi .groupBy().sum(), Spark ch·ªâ ghi l·∫°i \"k·∫ø ho·∫°ch\" (nh∆∞ c√°i Explain b·∫°n xem ·ªü c√¢u tr∆∞·ªõc). N√≥ ch·ªâ th·ª±c s·ª± ch·∫°y khi b·∫°n g·ªçi .collect() ho·∫∑c .show().\n",
        "\n",
        "3. T·∫°i sao l·ªói KeyError c·ªßa b·∫°n x·∫£y ra?\n",
        "\n",
        "Trong code Spark, n·∫øu b·∫°n t·∫°o c·ªôt b·∫±ng id % 10 AS group_id, khi d√πng toPandas(), h√£y ƒë·∫£m b·∫£o t√™n c·ªôt kh√¥ng b·ªã vi·∫øt hoa ho·∫∑c thay ƒë·ªïi. L·ªói KeyError b·∫°n g·∫∑p l√∫c tr∆∞·ªõc th∆∞·ªùng do:\n",
        "\n",
        "B·∫°n ch∆∞a g·ªçi toPandas() m√† ƒë√£ d√πng c√∫ ph√°p c·ªßa Pandas tr√™n ƒë·ªëi t∆∞·ª£ng Spark.\n",
        "\n",
        "T√™n c·ªôt b·ªã thay ƒë·ªïi (v√≠ d·ª•: sum(amount) thay v√¨ amount).\n",
        "\n",
        "L∆∞u √Ω quan tr·ªçng: N·∫øu b·∫°n ƒëang ch·∫°y tr√™n m·ªôt m√°y c√° nh√¢n (Local), Spark ƒë√¥i khi ch·∫≠m h∆°n Pandas v√¨ chi ph√≠ qu·∫£n l√Ω d·ªØ li·ªáu ph√¢n t√°n tr√™n c√πng m·ªôt m√°y l√† kh√° l·ªõn. Spark ch·ªâ th·ª±c s·ª± \"v√¥ ƒë·ªëi\" khi ch·∫°y tr√™n m·ªôt Cluster nhi·ªÅu m√°y.\n"
      ],
      "metadata": {
        "id": "-8iczO_smvN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ Th·ª±c t·∫ø\n",
        "\n",
        "*\t80% incident Spark = shuffle\n",
        "*\tBank r·∫•t s·ª£ shuffle uncontrolle"
      ],
      "metadata": {
        "id": "PpgRVNoAnk5U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü° LAB 4 ‚Äì JOIN & BROADCAST (HAY CH·∫æT NH·∫§T)\n",
        "\n",
        "## üéØ M·ª•c ti√™u\n",
        "*\tHi·ªÉu join strategy\n",
        "*\tTr√°nh OOM\n",
        "\n",
        "---\n",
        "\n",
        "1Ô∏è‚É£ Data"
      ],
      "metadata": {
        "id": "osrT_QSwnwPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders = spark.range(0, 1_000_000).select(\n",
        "    (F.col(\"id\") % 1000).alias(\"customer_id\"),\n",
        "    F.rand().alias(\"amount\")\n",
        ")\n",
        "\n",
        "customers = spark.range(0, 1000).select(\n",
        "    F.col(\"id\").alias(\"customer_id\"),\n",
        "    F.lit(\"VN\").alias(\"country\")\n",
        ")"
      ],
      "metadata": {
        "id": "fDH6wQz4n4ts"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Join th∆∞·ªùng"
      ],
      "metadata": {
        "id": "QbJMw6TooG2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders.join(customers, \"customer_id\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we95n76PoH8X",
        "outputId": "2aee692e-b799-4a27-c139-4cea1c16008c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#35L, amount#36, VN AS country#39]\n",
            "   +- BroadcastHashJoin [customer_id#35L], [customer_id#38L], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(customer_id#35L)\n",
            "      :  +- Project [(id#34L % 1000) AS customer_id#35L, rand(6594456756986365534) AS amount#36]\n",
            "      :     +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=108]\n",
            "         +- Project [id#37L AS customer_id#38L]\n",
            "            +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Broadcast"
      ],
      "metadata": {
        "id": "07dzo1wdooM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "orders.join(broadcast(customers), \"customer_id\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEDFwyyropL1",
        "outputId": "8c0bea19-9d3b-473e-ef9c-fb11f47d4c7f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#35L, amount#36, VN AS country#39]\n",
            "   +- BroadcastHashJoin [customer_id#35L], [customer_id#38L], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(customer_id#35L)\n",
            "      :  +- Project [(id#34L % 1000) AS customer_id#35L, rand(6594456756986365534) AS amount#36]\n",
            "      :     +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=139]\n",
            "         +- Project [id#37L AS customer_id#38L]\n",
            "            +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Gi·∫£i th√≠ch\n",
        "*\tBroadcast = g·ª≠i table nh·ªè ƒë·∫øn m·ªçi executor\n",
        "*\tNhanh nh∆∞ng OOM n·∫øu sai\n"
      ],
      "metadata": {
        "id": "VkmLjffApHNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Th·ª±c t·∫ø v√≠ d·ª• tr√™n:\n",
        "\n",
        "ƒêi·ªÉm th√∫ v·ªã nh·∫•t trong hai b·∫£n explain n√†y l√†: Ch√∫ng ho√†n to√†n gi·ªëng h·ªát nhau.\n",
        "\n",
        "D√π ·ªü tr∆∞·ªùng h·ª£p th·ª© hai b·∫°n c√≥ d√πng h√†m broadcast() m·ªôt c√°ch t∆∞·ªùng minh, nh∆∞ng Spark ƒë·ªß th√¥ng minh ƒë·ªÉ t·ª± ƒë·ªông th·ª±c hi·ªán ƒëi·ªÅu ƒë√≥ ·ªü tr∆∞·ªùng h·ª£p th·ª© nh·∫•t. D∆∞·ªõi ƒë√¢y l√† ph√¢n t√≠ch chi ti·∫øt:\n",
        "\n",
        "1. **T·∫°i sao hai k·∫ø ho·∫°ch l·∫°i gi·ªëng h·ªát nhau?**\n",
        "\n",
        "Trong Spark, c√≥ m·ªôt c·∫•u h√¨nh t√™n l√† `spark.sql.autoBroadcastJoinThreshold` (m·∫∑c ƒë·ªãnh l√† 10MB).\n",
        "\n",
        "**B·∫£ng customers:** Ch·ªâ c√≥ 1.000 d√≤ng v·ªõi d·ªØ li·ªáu r·∫•t nh·∫π (m·ªôt c·ªôt Long v√† m·ªôt c·ªôt String \"VN\"). K√≠ch th∆∞·ªõc n√†y nh·ªè h∆°n r·∫•t nhi·ªÅu so v·ªõi ng∆∞·ª°ng 10MB.\n",
        "\n",
        "**C∆° ch·∫ø t·ª± ƒë·ªông:** Khi b·∫°n th·ª±c hi·ªán Join, b·ªô t·ªëi ∆∞u h√≥a **Catalyst** c·ªßa Spark s·∫Ω ki·ªÉm tra k√≠ch th∆∞·ªõc c√°c b·∫£ng. V√¨ b·∫£ng customers r·∫•t nh·ªè, Spark t·ª± ƒë·ªông chuy·ªÉn n√≥ th√†nh **BroadcastHashJoin** ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t m√† kh√¥ng c·∫ßn b·∫°n ph·∫£i ra l·ªánh b·∫±ng h√†m broadcast().\n",
        "\n",
        "---\n",
        "\n",
        "2. **Ph√¢n t√≠ch c√°c th√†nh ph·∫ßn chung trong Physical Plan**\n",
        "\n",
        "C·∫£ hai b·∫£n k·∫ø ho·∫°ch ƒë·ªÅu c√≥ c√°c b∆∞·ªõc then ch·ªët sau:\n",
        "\n",
        "**BroadcastHashJoin:** ƒê√¢y l√† ki·ªÉu Join nhanh nh·∫•t trong Spark. Thay v√¨ ph·∫£i x√°o tr·ªôn (Shuffle) c·∫£ hai b·∫£ng l·ªõn qua m·∫°ng, Spark l·∫•y b·∫£ng nh·ªè (customers), g·ª≠i b·∫£n sao c·ªßa n√≥ ƒë·∫øn t·∫•t c·∫£ c√°c Executor.\n",
        "\n",
        "**BuildRight:** Spark ch·ªçn b·∫£ng b√™n ph·∫£i (l√† customers) ƒë·ªÉ l√†m b·∫£ng b·ªã \"broadcast\" (b·∫£ng d√πng ƒë·ªÉ x√¢y d·ª±ng Hash Table).\n",
        "\n",
        "**BroadcastExchange:** ƒê√¢y l√† b∆∞·ªõc v·∫≠t l√Ω th·ª±c hi·ªán vi·ªác g·ª≠i d·ªØ li·ªáu b·∫£ng nh·ªè ƒëi kh·∫Øp c√°c node.\n",
        "\n",
        "**Filter isnotnull:** Spark t·ª± ƒë·ªông th√™m b∆∞·ªõc ki·ªÉm tra null tr√™n kh√≥a Join ƒë·ªÉ lo·∫°i b·ªè c√°c d·ªØ li·ªáu th·ª´a ngay t·ª´ ƒë·∫ßu, gi√∫p tƒÉng t·ªëc qu√° tr√¨nh.\n",
        "\n",
        "---\n",
        "\n",
        "3. **Quy tr√¨nh ho·∫°t ƒë·ªông c·ªßa Broadcast Hash Join**\n",
        "\n",
        "Driver thu th·∫≠p d·ªØ li·ªáu t·ª´ b·∫£ng nh·ªè (customers).\n",
        "\n",
        "**Driver** g·ª≠i b·∫£n sao b·∫£ng n√†y ƒë·∫øn m·ªçi Executor ƒëang gi·ªØ c√°c ph√¢n v√πng (partitions) c·ªßa b·∫£ng l·ªõn (orders).\n",
        "\n",
        "M·ªói Executor th·ª±c hi·ªán Join ngay t·∫°i ch·ªó (local join) m√† kh√¥ng c·∫ßn di chuy·ªÉn d·ªØ li·ªáu c·ªßa b·∫£ng orders.\n",
        "\n",
        "---\n",
        "\n",
        "4. **Khi n√†o th√¨ 2 c√¢u l·ªánh n√†y s·∫Ω cho k·∫øt qu·∫£ kh√°c nhau?**\n",
        "\n",
        "B·∫°n s·∫Ω th·∫•y s·ª± kh√°c bi·ªát n·∫øu b·∫£ng customers l·ªõn h∆°n ng∆∞·ª°ng m·∫∑c ƒë·ªãnh (v√≠ d·ª• 100MB):\n",
        "\n",
        "**N·∫øu kh√¥ng d√πng broadcast():** Spark s·∫Ω th·ª±c hi·ªán SortMergeJoin. ƒê√¢y l√† ki·ªÉu Join y√™u c·∫ßu Shuffle c·∫£ hai b·∫£ng. B·∫°n s·∫Ω th·∫•y b∆∞·ªõc Exchange hashpartitioning xu·∫•t hi·ªán cho c·∫£ hai b√™n trong b·∫£n explain. Vi·ªác n√†y t·ªën t√†i nguy√™n m·∫°ng v√† ch·∫≠m h∆°n.\n",
        "\n",
        "**N·∫øu c√≥ d√πng broadcast():** B·∫°n \"√©p\" Spark ph·∫£i d√πng Broadcast Join b·∫•t k·ªÉ b·∫£ng ƒë√≥ l·ªõn h∆°n 10MB (mi·ªÖn l√† n√≥ v·∫´n v·ª´a b·ªô nh·ªõ c·ªßa Driver v√† Executor).\n",
        "\n",
        "---\n",
        "\n",
        "**T√≥m t·∫Øt so s√°nh:**\n",
        "\n",
        "|**ƒê·∫∑c ƒëi·ªÉm**|**C√°ch 1 (T·ª± ƒë·ªông)**|**C√°ch 2 (D√πng broadcast())**|\n",
        "|------------|--------------------|-----------------------------|\n",
        "|Physical Plan|BroadcastHashJoin|BroadcastHashJoin|\n",
        "|Hi·ªáu su·∫•t|T·ªëi ∆∞u nh·∫•t|T·ªëi ∆∞u nh·∫•t|\n",
        "|T√≠nh an to√†n|An to√†n (Ch·ªâ broadcast n·∫øu b·∫£ng nh·ªè)|\"C√≥ r·ªßi ro (N·∫øu b·∫£ng customers ƒë·ªôt ng·ªôt l·ªõn l√™n, c√≥ th·ªÉ g√¢y l·ªói h·∫øt b·ªô nh·ªõ - OOM)\"|\n",
        "\n",
        "**L·ªùi khuy√™n:** B·∫°n n√™n tin t∆∞·ªüng v√†o b·ªô t·ªëi ∆∞u h√≥a c·ªßa Spark ·ªü tr∆∞·ªùng h·ª£p d·ªØ li·ªáu nh·ªè. Ch·ªâ n√™n d√πng h√†m broadcast() khi b·∫°n bi·∫øt ch·∫Øc ch·∫Øn b·∫£ng ƒë√≥ nh·ªè nh∆∞ng Spark kh√¥ng t·ª± nh·∫≠n di·ªán ƒë∆∞·ª£c (v√≠ d·ª• d·ªØ li·ªáu l·∫•y t·ª´ m·ªôt query ph·ª©c t·∫°p m√† Spark kh√¥ng t√≠nh to√°n tr∆∞·ªõc ƒë∆∞·ª£c k√≠ch th∆∞·ªõc)."
      ],
      "metadata": {
        "id": "7KeEN59GpW__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê·ªÉ th·∫•y s·ª± kh√°c bi·ªát r√µ r·ªát nh·∫•t m√† kh√¥ng c·∫ßn ph·∫£i t·∫°o ra h√†ng GB d·ªØ li·ªáu (l√†m treo m√°y), c√°ch t·ªët nh·∫•t l√† ch√∫ng ta s·∫Ω ƒëi·ªÅu ch·ªânh tham s·ªë c·∫•u h√¨nh `spark.sql.autoBroadcastJoinThreshold`.\n",
        "\n",
        "Khi c·∫•u h√¨nh n√†y ƒë∆∞·ª£c ƒë·∫∑t v·ªÅ `-1`, Spark s·∫Ω b·ªã c·∫•m s·ª≠ d·ª•ng **Broadcast Join** v√† bu·ªôc ph·∫£i d√πng **SortMergeJoin**. ƒê√¢y ch√≠nh l√† c√°ch c√°c h·ªá th·ªëng Big Data x·ª≠ l√Ω khi c·∫£ hai b·∫£ng ƒë·ªÅu c·ª±c k·ª≥ l·ªõn.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Code th·ª±c thi chi ti·∫øt"
      ],
      "metadata": {
        "id": "4iLIgY6ertph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Kh·ªüi t·∫°o Spark\n",
        "spark = SparkSession.builder.appName(\"SortMergeJoinDemo\").getOrCreate()\n",
        "\n",
        "# 1. T·∫°o d·ªØ li·ªáu m·∫´u\n",
        "orders = spark.range(0, 1000000).select(\n",
        "    (F.col(\"id\") % 1000).alias(\"customer_id\"),\n",
        "    F.rand().alias(\"amount\")\n",
        ")\n",
        "\n",
        "customers = spark.range(0, 1000).select(\n",
        "    F.col(\"id\").alias(\"customer_id\"),\n",
        "    F.lit(\"VN\").alias(\"country\")\n",
        ")\n",
        "\n",
        "print(\"=== 1. TR∆Ø·ªúNG H·ª¢P M·∫∂C ƒê·ªäNH (BROADCAST JOIN) ===\")\n",
        "# Spark t·ª± nh·∫≠n di·ªán b·∫£ng customers nh·ªè n√™n d√πng Broadcast\n",
        "orders.join(customers, \"customer_id\").explain()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "print(\"=== 2. TR∆Ø·ªúNG H·ª¢P √âP D√ôNG SORT MERGE JOIN ===\")\n",
        "# T·∫Øt t√≠nh nƒÉng t·ª± ƒë·ªông Broadcast b·∫±ng c√°ch set threshold v·ªÅ -1\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "orders.join(customers, \"customer_id\").explain()\n",
        "\n",
        "# Reset l·∫°i c·∫•u h√¨nh v·ªÅ m·∫∑c ƒë·ªãnh (10MB) sau khi test xong\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peS7M2WDr8Fb",
        "outputId": "9c7dbad9-b223-4992-b15c-7b062b6d37bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== 1. TR∆Ø·ªúNG H·ª¢P M·∫∂C ƒê·ªäNH (BROADCAST JOIN) ===\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#41L, amount#42, VN AS country#45]\n",
            "   +- BroadcastHashJoin [customer_id#41L], [customer_id#44L], Inner, BuildRight, false\n",
            "      :- Filter isnotnull(customer_id#41L)\n",
            "      :  +- Project [(id#40L % 1000) AS customer_id#41L, rand(-396041844424334531) AS amount#42]\n",
            "      :     +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=170]\n",
            "         +- Project [id#43L AS customer_id#44L]\n",
            "            +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "=== 2. TR∆Ø·ªúNG H·ª¢P √âP D√ôNG SORT MERGE JOIN ===\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#41L, amount#42, VN AS country#45]\n",
            "   +- SortMergeJoin [customer_id#41L], [customer_id#44L], Inner\n",
            "      :- Sort [customer_id#41L ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(customer_id#41L, 10), ENSURE_REQUIREMENTS, [plan_id=202]\n",
            "      :     +- Filter isnotnull(customer_id#41L)\n",
            "      :        +- Project [(id#40L % 1000) AS customer_id#41L, rand(-396041844424334531) AS amount#42]\n",
            "      :           +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- Sort [customer_id#44L ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(customer_id#44L, 10), ENSURE_REQUIREMENTS, [plan_id=203]\n",
            "            +- Project [id#43L AS customer_id#44L]\n",
            "               +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. So s√°nh 2 b·∫£n Explain m·ªõi\n",
        "\n",
        "Khi b·∫°n ch·∫°y code tr√™n, ·ªü Tr∆∞·ªùng h·ª£p 2, b·∫°n s·∫Ω th·∫•y m·ªôt k·∫ø ho·∫°ch th·ª±c thi ho√†n to√†n kh√°c v√† ph·ª©c t·∫°p h∆°n nhi·ªÅu:\n",
        "\n",
        "### C√°c ƒëi·ªÉm m·ªõi xu·∫•t hi·ªán trong SortMergeJoin:\n",
        "\n",
        "**Exchange hashpartitioning:** Spark th·ª±c hi·ªán Shuffle. N√≥ bƒÉm (hash) customer_id c·ªßa c·∫£ hai b·∫£ng v√† g·ª≠i c√°c d√≤ng c√≥ c√πng m√£ bƒÉm v·ªÅ c√πng m·ªôt node. ƒê√¢y l√† b∆∞·ªõc t·ªën k√©m nh·∫•t v√¨ d·ªØ li·ªáu ph·∫£i ch·∫°y qua m·∫°ng.\n",
        "\n",
        "`Sort [customer_id#... ASC NULLS FIRST]`: Sau khi d·ªØ li·ªáu v·ªÅ c√πng m·ªôt node, Spark s·∫Øp x·∫øp ch√∫ng theo th·ª© t·ª± tƒÉng d·∫ßn c·ªßa customer_id.\n",
        "\n",
        "**SortMergeJoin:** Cu·ªëi c√πng, Spark ch·ªâ c·∫ßn qu√©t qua hai danh s√°ch ƒë√£ s·∫Øp x·∫øp ƒë·ªÉ kh·ªõp c√°c c·∫∑p d·ªØ li·ªáu (gi·ªëng nh∆∞ vi·ªác b·∫°n so s√°nh hai danh s√°ch danh b·∫° ƒë√£ x·∫øp theo b·∫£ng ch·ªØ c√°i).\n",
        "\n",
        "---\n",
        "\n",
        "### 3. B·∫£ng so s√°nh nhanh\n",
        "\n",
        "|**ƒê·∫∑c ƒëi·ªÉm**|**Broadcast Hash Join (Tr∆∞·ªùng h·ª£p 1)**|**Sort Merge Join (Tr∆∞·ªùng h·ª£p 2)**|\n",
        "|------------|---------------------------------------|----------------|\n",
        "|Shuffle|Kh√¥ng c√≥. Ti·∫øt ki·ªám bƒÉng th√¥ng m·∫°ng.|C√≥. Ph·∫£i di chuy·ªÉn d·ªØ li·ªáu c·ªßa c·∫£ 2 b·∫£ng.|\n",
        "|S·∫Øp x·∫øp (Sort)|Kh√¥ng c·∫ßn.|B·∫Øt bu·ªôc. T·ªën th√™m CPU v√† Disk.|\n",
        "|Gi·ªõi h·∫°n d·ªØ li·ªáu|M·ªôt b·∫£ng ph·∫£i ƒë·ªß nh·ªè ƒë·ªÉ nh√©t v·ª´a RAM.|Kh√¥ng gi·ªõi h·∫°n. C√≥ th·ªÉ join 2 b·∫£ng h√†ng Terabyte.|\n",
        "|T·ªëc ƒë·ªô|R·∫•t nhanh.|Ch·∫≠m h∆°n (do t·ªën c√¥ng Shuffle & Sort).|\n",
        "\n",
        "---\n",
        "\n",
        "#### T·∫°i sao ch√∫ng ta c·∫ßn bi·∫øt ƒëi·ªÅu n√†y?\n",
        "\n",
        "Trong th·ª±c t·∫ø, n·∫øu b·∫°n th·∫•y Spark ch·∫°y m·ªôt c√¢u l·ªánh Join qu√° ch·∫≠m (v√†i ti·∫øng ƒë·ªìng h·ªì), h√£y ki·ªÉm tra `.explain()`.\n",
        "\n",
        "N·∫øu n√≥ ƒëang d√πng *SortMergeJoin* cho m·ªôt b·∫£ng m√† b·∫°n bi·∫øt l√† n√≥ nh·ªè, h√£y d√πng *broadcast()* ƒë·ªÉ √©p n√≥ ch·∫°y nhanh h∆°n.\n",
        "\n",
        "N·∫øu n√≥ ƒëang d√πng ***BroadcastJoin** m√† b·ªã l·ªói **Out of Memory (OOM)**, h√£y tƒÉng b·ªô nh·ªõ cho Driver ho·∫∑c √©p n√≥ quay v·ªÅ **SortMergeJoin**."
      ],
      "metadata": {
        "id": "eYf_XXPUsg2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Pandas?\n",
        "\n",
        "‚ùå Join l·ªõn = ch·∫øt\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ Th·ª±c t·∫ø\n",
        "*\tBroadcast sai = incident\n",
        "*\tBank th∆∞·ªùng disable auto broadcast"
      ],
      "metadata": {
        "id": "KSV5TuSorNhp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî¥ LAB 5 ‚Äì SKEW & SALTING (BANK GRADE)\n",
        "\n",
        "## üéØ M·ª•c ti√™u\n",
        "*\tHi·ªÉu skew\n",
        "*\tFix skew ƒë√∫ng c√°ch\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Data skew\n"
      ],
      "metadata": {
        "id": "vJf2s22TuD8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.range(0, 1_000_000).select(\n",
        "    F.when(F.rand() < 0.7, 1).otherwise(F.col(\"id\") % 1000).alias(\"customer_id\"),\n",
        "    F.rand().alias(\"amount\")\n",
        ")"
      ],
      "metadata": {
        "id": "KZRoNCxauZM1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ GroupBy ‚Üí ch·∫≠m"
      ],
      "metadata": {
        "id": "5MV3YwVeu0Ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"customer_id\").sum(\"amount\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfyceUDXu1ha",
        "outputId": "7a368ea7-9e7a-4080-a183-1f529d1069fe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[customer_id#47L], functions=[sum(amount#48)])\n",
            "   +- Exchange hashpartitioning(customer_id#47L, 10), ENSURE_REQUIREMENTS, [plan_id=223]\n",
            "      +- HashAggregate(keys=[customer_id#47L], functions=[partial_sum(amount#48)])\n",
            "         +- Project [CASE WHEN (rand(-5359486187627201171) < 0.7) THEN 1 ELSE (id#46L % 1000) END AS customer_id#47L, rand(-1491132137368338991) AS amount#48]\n",
            "            +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Salting"
      ],
      "metadata": {
        "id": "tPfCwkeZvFg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\n",
        "    \"salt\",\n",
        "    (F.rand() * 10).cast(\"int\")\n",
        ")\n",
        "\n",
        "df2.groupBy(\"customer_id\", \"salt\").sum(\"amount\") \\\n",
        "   .groupBy(\"customer_id\").sum(\"sum(amount)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngAqurM7vIpq",
        "outputId": "7ba2233e-88db-4fcc-e78b-d594cb4dc09c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[customer_id: bigint, sum(sum(amount)): double]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Gi·∫£i th√≠ch\n",
        "*\tChia hot key\n",
        "*\tPh√¢n t√°n task\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Th·ª±c t·∫ø\n",
        "*\tSkew = design issue\n",
        "*\tRetry kh√¥ng c·ª©u ƒë∆∞·ª£c\n",
        "\n"
      ],
      "metadata": {
        "id": "AcEa_OS6vNrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ph√¢n t√≠ch:\n",
        "\n",
        "V√≠ d·ª• tr√™n ƒë∆∞a ra l√† m·ªôt k·ªãch b·∫£n kinh ƒëi·ªÉn v·ªÅ Data Skew (L·ªách d·ªØ li·ªáu) trong Big Data. Khi `70%` d·ªØ li·ªáu t·∫≠p trung v√†o m·ªôt gi√° tr·ªã (customer_id = 1), n√≥ s·∫Ω t·∫°o ra hi·ªán t∆∞·ª£ng **\"n√∫t th·∫Øt c·ªï chai\"**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Ph√¢n t√≠ch Physical Plan ban ƒë·∫ßu (B·ªã Skew)\n",
        "\n",
        "Trong b·∫£n explain c·ªßa b·∫°n, Spark th·ª±c hi·ªán theo quy tr√¨nh:\n",
        "\n",
        "**PartialAggregate:** Gom nh√≥m t·∫°m th·ªùi tr√™n t·ª´ng partition.\n",
        "\n",
        "**Exchange hashpartitioning:** ƒê√¢y l√† n∆°i th·∫£m h·ªça x·∫£y ra. Spark d√πng h√†m Hash ƒë·ªÉ quy·∫øt ƒë·ªãnh d·ªØ li·ªáu ƒëi v·ªÅ partition n√†o: `hash(customer_id) % 10`.\n",
        "\n",
        "V√¨ `70%` d√≤ng c√≥ `customer_id = 1`, n√™n to√†n b·ªô `700.000` d√≤ng n√†y s·∫Ω b·ªã ƒë·∫©y v√†o c√πng 1 partition v·∫≠t l√Ω.\n",
        "\n",
        "**HashAggregate (Final):** Trong khi **9 Executor** kh√°c c√≥ th·ªÉ ch·ªâ x·ª≠ l√Ω v√†i ch·ª•c ngh√¨n d√≤ng v√† xong trong 1 gi√¢y, th√¨ 1 Executor \"s·ªë nh·ªç\" ph·∫£i x·ª≠ l√Ω 700.000 d√≤ng.\n",
        "\n",
        "**H·ªá qu·∫£:** T·ªïng th·ªùi gian ch·∫°y c·ªßa c·∫£ Job b·∫±ng th·ªùi gian ch·∫°y c·ªßa Task ch·∫≠m nh·∫•t ƒë√≥. Th·∫≠m ch√≠ c√≥ th·ªÉ g√¢y l·ªói **Out Of Memory (OOM)** t·∫°i Executor ƒë√≥.\n",
        "\n",
        "## 2. Ph√¢n t√≠ch gi·∫£i ph√°p Salting (Mu·ªëi d·ªØ li·ªáu)\n",
        "\n",
        "Gi·∫£i ph√°p b·∫°n ƒë∆∞a ra l√† chia nh·ªè c√°i \"n√∫t th·∫Øt\" ƒë√≥ ra.\n",
        "\n",
        "### C∆° ch·∫ø:\n",
        "\n",
        "#### B∆∞·ªõc 1:\n",
        "Th√™m m·ªôt s·ªë ng·∫´u nhi√™n (salt t·ª´ 0-9) v√†o kh√≥a join/groupby. Kh√≥a m·ªõi b√¢y gi·ªù l√† (1, 0), (1, 1), ..., (1, 9).\n",
        "\n",
        "#### B∆∞·ªõc 2:\n",
        "`GroupBy` theo c·ª•m (customer_id, salt). Thay v√¨ 1 task x·ª≠ l√Ω 700k d√≤ng, b√¢y gi·ªù 10 task x·ª≠ l√Ω, m·ªói task ~70k d√≤ng. D·ªØ li·ªáu ƒë∆∞·ª£c tr·∫£i ƒë·ªÅu ra c√°c core.\n",
        "\n",
        "#### B∆∞·ªõc 3:\n",
        "GroupBy l·∫ßn n·ªØa theo customer_id ƒë·ªÉ c·ªông d·ªìn k·∫øt qu·∫£ cu·ªëi c√πng t·ª´ 10 c·ª•m nh·ªè kia.\n",
        "\n",
        "## 3. So s√°nh v√† H∆∞·ªõng x·ª≠ l√Ω t·ªët nh·∫•t\n",
        "\n",
        "M·∫∑c d√π Salting gi·∫£i quy·∫øt ƒë∆∞·ª£c Skew, nh∆∞ng n√≥ l√†m code ph·ª©c t·∫°p h∆°n v√† t·∫°o ra 2 l·∫ßn Shuffle. Trong Spark hi·ªán ƒë·∫°i (3.0 tr·ªü ƒëi), ch√∫ng ta c√≥ nh·ªØng l·ª±a ch·ªçn sau:\n",
        "\n",
        "### C√°ch 1: T·∫≠n d·ª•ng AQE (Adaptive Query Execution) - ∆Øu ti√™n s·ªë 1\n",
        "\n",
        "N·∫øu b·∫°n d√πng Spark 3.x, h√£y b·∫≠t t√≠nh nƒÉng t·ª± ƒë·ªông x·ª≠ l√Ω Skew. Spark s·∫Ω t·ª± ph√°t hi·ªán partition n√†o qu√° l·ªõn v√† chia nh·ªè n√≥ ra cho b·∫°n m√† kh√¥ng c·∫ßn s·ª≠a code.\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") # Ch·ªß y·∫øu cho Join\n",
        "```\n",
        "\n",
        "**L∆∞u √Ω:** AQE h·ªó tr·ª£ c·ª±c t·ªët cho Join, nh∆∞ng v·ªõi GroupBy, c∆° ch·∫ø Partial Aggregate c·ªßa Spark th∆∞·ªùng ƒë√£ x·ª≠ l√Ω kh√° t·ªët tr·ª´ khi s·ªë l∆∞·ª£ng key qu√° √≠t.\n",
        "\n",
        "### C√°ch 2: Salting (N·∫øu AQE kh√¥ng ƒë·ªß c√¢n) - ∆Øu ti√™n s·ªë 2\n",
        "\n",
        "B·∫°n ch·ªâ n√™n d√πng ***Salting*** khi:\n",
        "\n",
        "D·ªØ li·ªáu c·ª±c l·ªõn v√† c·ª±c l·ªách.\n",
        "\n",
        "B·∫°n th·∫•y m·ªôt v√†i Task ch·∫°y m√£i kh√¥ng xong trong khi c√°c Task kh√°c ƒë√£ ho√†n th√†nh t·ª´ l√¢u.\n",
        "\n",
        "**Code t·ªëi ∆∞u cho Salting:** Thay v√¨ d√πng rand(), b·∫°n c√≥ th·ªÉ d√πng (id % 10) ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n (deterministic)."
      ],
      "metadata": {
        "id": "v6RFNLGwwAF_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî• LAB 6 ‚Äì WRITE GOLD LAYER (BANK STYLE)\n",
        "\n",
        "## üéØ M·ª•c ti√™u\n",
        "*\tIdempotent\n",
        "*\tPredictable\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Write ƒë√∫ng\n",
        "\n"
      ],
      "metadata": {
        "id": "eVmu9-6VyZDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df\n",
        " .repartition(4)\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .partitionBy(\"date\")\n",
        " .parquet(\"/gold/orders\"))"
      ],
      "metadata": {
        "id": "9SNB1AUjyjOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Gi·∫£i th√≠ch\n",
        "\t‚Ä¢\tKh√¥ng append lung tung\n",
        "\t‚Ä¢\tC√≥ rollback\n",
        "\t‚Ä¢\tAudit ƒë∆∞·ª£c\n",
        "\n",
        "--\n",
        "\n",
        "## 3Ô∏è‚É£ Pandas?\n",
        "\n",
        "‚ùå Kh√¥ng ƒë·ªß an to√†n\n",
        "\n",
        "---\n",
        "\n",
        "## üß∞ C√îNG C·ª§ TH·ª∞C T·∫æ (BANK)\n",
        "\t‚Ä¢\tSpark UI\n",
        "\t‚Ä¢\tDataproc / EMR\n",
        "\t‚Ä¢\tAirflow / Composer\n",
        "\t‚Ä¢\tDelta / Iceberg\n",
        "\t‚Ä¢\tGreat Expectations (data quality)\n",
        "\t‚Ä¢\tOpenLineage / Atlas\n",
        "  "
      ],
      "metadata": {
        "id": "vXUxht4KyvK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üü¢ LAB 1 ‚Äì DATAFRAME FOUNDATION (ƒêI CH·∫¨M ‚Äì HI·ªÇU S√ÇU)\n",
        "\n",
        "## M·ª•c ti√™u LAB 1\n",
        "\n",
        "*\tHi·ªÉu Spark DataFrame l√† g√¨ (so v·ªõi Pandas)\n",
        "*\tHi·ªÉu lazy evaluation\n",
        "*\tHi·ªÉu transform vs action\n",
        "*\tKh√¥ng code m√°y m√≥c\n",
        "\n",
        "---\n",
        "\n",
        "## 0Ô∏è‚É£ Chu·∫©n b·ªã (lu√¥n c√≥ trong m·ªçi notebook)\n",
        "\n",
        "```python\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "```\n",
        "\n",
        "* `functions as F`\n",
        "> ‚Üí convention industry, 99% code Spark ngo√†i ƒë·ªùi d√πng F\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ T·∫°o SparkSession (tr√°i tim c·ªßa Spark)\n",
        "\n",
        "```python\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"lab1_dataframe_basic\") \\\n",
        "    .getOrCreate()\n",
        "```\n",
        "\n",
        "### üß† Gi·∫£i th√≠ch\n",
        "*\tSparkSession = entry point\n",
        "*\tM·ªçi DataFrame ƒë·ªÅu s·ªëng trong SparkSession\n",
        "*\tKh√¥ng c√≥ SparkSession ‚Üí kh√¥ng c√≥ Spark\n",
        "\n",
        "> üìå Trong Pandas kh√¥ng c√≥ kh√°i ni·ªám n√†y\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ T·∫°o DataFrame th·ªß c√¥ng (c√°ch d·ªÖ hi·ªÉu nh·∫•t)\n",
        "\n",
        "```python\n",
        "data = [\n",
        "    (1, \"Alice\", 1000),\n",
        "    (2, \"Bob\", 2000),\n",
        "    (3, \"Charlie\", 3000)\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"name\", \"balance\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "```\n",
        "\n",
        "### üß† So s√°nh Pandas\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "pdf = pd.DataFrame(data, columns=columns)\n",
        "```\n",
        "\n",
        "|**Pandas**|**Spark**|\n",
        "|----------|---------|\n",
        "|DataFrame| n·∫±m trong RAM|\tDataFrame l√† metadata|\n",
        "|Nh·ªè|\tR·∫•t l·ªõn|\n",
        "|Ch·∫°y ngay|\tCh∆∞a ch·∫°y|\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ Nh√¨n schema (c·ª±c k·ª≥ quan tr·ªçng)\n",
        "\n",
        "```python\n",
        "df.printSchema()\n",
        "```\n",
        "\n",
        "### üìå Output:\n",
        "\n",
        "```text\n",
        "root\n",
        " |-- id: long\n",
        " |-- name: string\n",
        " |-- balance: long\n",
        "```\n",
        "\n",
        "### üß† V√¨ sao ph·∫£i xem schema?\n",
        "*\tSpark r·∫•t strict v·ªÅ type\n",
        "*\t80% bug Spark = type mismatch\n",
        "*\tKh√°c Pandas (r·∫•t ‚Äútho√°ng‚Äù)\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ Transformation ƒë·∫ßu ti√™n ‚Äì select\n",
        "\n",
        "```python\n",
        "df_select = df.select(\"id\", \"balance\")\n",
        "```\n",
        "\n",
        "### ‚õî CH∆ØA C√ì G√å CH·∫†Y\n",
        "\n",
        "#### üß† V√¨ sao?\n",
        "*\tƒê√¢y l√† TRANSFORMATION\n",
        "*\tSpark ch·ªâ ghi nh·ªõ:\n",
        ">- ‚Äú√Ä, user mu·ªën select 2 c·ªôt‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Transformation th·ª© hai ‚Äì filter\n",
        "\n",
        "```python\n",
        "df_filtered = df.filter(F.col(\"balance\") > 1500)\n",
        "```\n",
        "\n",
        "### ‚õî V·∫™N CH∆ØA CH·∫†Y\n",
        "\n",
        "#### üß† ƒê√¢y l√† ch·ªó nhi·ªÅu ng∆∞·ªùi nh·∫ßm\n",
        "*\tSpark kh√¥ng gi·ªëng Pandas\n",
        "*\tKh√¥ng c√≥ CPU, kh√¥ng c√≥ RAM d√πng ·ªü ƒë√¢y\n",
        "\n",
        "---\n",
        "\n",
        "## 6Ô∏è‚É£ ACTION ‚Äì th·ªùi ƒëi·ªÉm Spark TH·ª∞C S·ª∞ ch·∫°y\n",
        "\n",
        "```python\n",
        "df_filtered.show()\n",
        "```\n",
        "\n",
        "### üí• L√öC N√ÄY M·ªöI CH·∫†Y\n",
        "\n",
        "#### üß† Nh·ªØng g√¨ Spark l√†m l√∫c n√†y (r·∫•t quan tr·ªçng)\n",
        "1.\tBuild Logical Plan\n",
        "2.\tOptimize (Catalyst)\n",
        "3.\tBuild Physical Plan\n",
        "4.\tChia Stage\n",
        "5.\tT·∫°o Task\n",
        "6.\tG·ª≠i task t·ªõi executor\n",
        "7.\tCh·∫°y\n",
        "\n",
        "> üëâ Tr∆∞·ªõc show() ‚Üí kh√¥ng c√≥ job\n",
        "\n",
        "---\n",
        "\n",
        "## 7Ô∏è‚É£ Action kh√°c ‚Äì count()\n",
        "\n",
        "```python\n",
        "df_filtered.count()\n",
        "```\n",
        "\n",
        "### üß† Kh√°c show() th·∫ø n√†o?\n",
        "\n",
        "|**show()**|**count()**|\n",
        "|----------|-----------|\n",
        "|Hi·ªÉn th·ªã|Tr·∫£ v·ªÅ s·ªë|\n",
        "|C√≥ limit|Full scan|\n",
        "|Debug|Logic|\n",
        "\n",
        "\n",
        "> üìå **count()** r·∫•t nguy hi·ªÉm v·ªõi data l·ªõn\n",
        "\n",
        "---\n",
        "\n",
        "## 8Ô∏è‚É£ Ki·ªÉm tra k·∫ø ho·∫°ch th·ª±c thi (b∆∞·ªõc architect)\n",
        "\n",
        "```python\n",
        "df_filtered.explain()\n",
        "```\n",
        "\n",
        "### üëâ B·∫°n s·∫Ω th·∫•y:\n",
        "* Logical Plan\n",
        "*\tPhysical Plan\n",
        "*\tKh√¥ng th·∫•y Exchange (v√¨ ch∆∞a shuffle)\n",
        "\n",
        "> üß† Architect ƒë·ªçc explain tr∆∞·ªõc khi approve job\n",
        "\n",
        "---\n",
        "\n",
        "## 9Ô∏è‚É£ T·ªïng k·∫øt LAB 1 (C·ª∞C QUAN TR·ªåNG)\n",
        "\n",
        "### ‚úÖ B·∫°n c·∫ßn nh·ªõ ch·∫Øc:\n",
        "*\tSpark LAZY\n",
        "*\tTransformation ‚â† Action\n",
        "*\tDataFrame ‚â† d·ªØ li·ªáu th·∫≠t\n",
        "*\tSpark DataFrame = k·∫ø ho·∫°ch x·ª≠ l√Ω d·ªØ li·ªáu\n",
        "\n",
        "---\n",
        "\n",
        "## üß† C√ÇU H·ªéI B·∫ÆT BU·ªòC (b·∫°n t·ª± tr·∫£ l·ªùi)\n",
        "1.\tV√¨ sao filter() kh√¥ng ch·∫°y ngay?\n",
        "2.\tN·∫øu vi·∫øt 10 d√≤ng transformation li√™n ti·∫øp, Spark ch·∫°y m·∫•y l·∫ßn?\n",
        "3.\tV√¨ sao count() nguy hi·ªÉm?\n",
        "4.\tSpark DataFrame c√≥ gi·ªØ d·ªØ li·ªáu kh√¥ng?\n",
        "5.\tV√¨ sao Spark s·ªëng m√† Pandas ch·∫øt?\n"
      ],
      "metadata": {
        "id": "SjCrs2dCjpzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "4HoRzvqNmLgJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"lab1_dataframe_basic\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "yLXVzST0mVxz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (1, \"Alice\", 1000),\n",
        "    (2, \"Bob\", 2000),\n",
        "    (3, \"Charlie\", 3000)\n",
        "]\n",
        "\n",
        "columns = [\"id\", \"name\", \"balance\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "X5Nm2hTEmaeC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pdf = pd.DataFrame(data, columns=columns)"
      ],
      "metadata": {
        "id": "xAp3iCMBmdyv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwYLXi0FmhD5",
        "outputId": "e56986f4-3c62-44cc-8141-49a399f80e5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- balance: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_select = df.select(\"id\", \"balance\")"
      ],
      "metadata": {
        "id": "y299wYtBmqVN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = df.filter(F.col(\"balance\") > 1500)"
      ],
      "metadata": {
        "id": "wlsn9HpkmucJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFnrfCOKm0Uo",
        "outputId": "9c7305e1-7f3c-4cf2-e37d-9a3b6b5a071c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+-------+\n",
            "| id|   name|balance|\n",
            "+---+-------+-------+\n",
            "|  2|    Bob|   2000|\n",
            "|  3|Charlie|   3000|\n",
            "+---+-------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbWwmXF5m7HB",
        "outputId": "35aca120-8afc-4fc4-d6d1-46c19e73fc23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQDJ1uUUnAgA",
        "outputId": "b0312f31-1a9f-47fd-b068-a61409ab4054"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "*(1) Filter (isnotnull(balance#2L) AND (balance#2L > 1500))\n",
            "+- *(1) Scan ExistingRDD[id#0L,name#1,balance#2L]\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}