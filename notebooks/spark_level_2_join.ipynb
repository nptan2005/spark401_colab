{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0OqZhG+r0/yjAMT+jDrhN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nptan2005/spark401_colab/blob/main/notebooks/spark_level_2_join.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y openjdk-17-jdk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j5Bw8Up4dZaG",
        "outputId": "f0e3ae41-8055-472d-c451-d7440f16ba42"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libgail-common libgail18 libgtk2.0-0\n",
            "  libgtk2.0-bin libgtk2.0-common librsvg2-common libxcomposite1 libxt-dev\n",
            "  libxtst6 libxxf86dga1 openjdk-17-jre session-migration x11-utils\n",
            "Suggested packages:\n",
            "  gvfs libxt-doc openjdk-17-demo openjdk-17-source visualvm mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  at-spi2-core fonts-dejavu-core fonts-dejavu-extra gsettings-desktop-schemas\n",
            "  libatk-bridge2.0-0 libatk-wrapper-java libatk-wrapper-java-jni libatk1.0-0\n",
            "  libatk1.0-data libatspi2.0-0 libgail-common libgail18 libgtk2.0-0\n",
            "  libgtk2.0-bin libgtk2.0-common librsvg2-common libxcomposite1 libxt-dev\n",
            "  libxtst6 libxxf86dga1 openjdk-17-jdk openjdk-17-jre session-migration\n",
            "  x11-utils\n",
            "0 upgraded, 24 newly installed, 0 to remove and 1 not upgraded.\n",
            "Need to get 8,212 kB of archives.\n",
            "After this operation, 24.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatspi2.0-0 amd64 2.44.0-3 [80.9 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxtst6 amd64 2:1.2.3-1build4 [13.4 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 at-spi2-core amd64 2.44.0-3 [54.4 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-core all 2.37-2build1 [1,041 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-dejavu-extra all 2.37-2build1 [2,041 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-data all 2.36.0-3build1 [2,824 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk1.0-0 amd64 2.36.0-3build1 [51.9 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-bridge2.0-0 amd64 2.38.0-3 [66.6 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcomposite1 amd64 1:0.4.5-1build2 [7,192 B]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu3 [12.6 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-utils amd64 7.7+5build2 [206 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java all 0.38.0-5build1 [53.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libatk-wrapper-java-jni amd64 0.38.0-5build1 [49.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-common all 2.24.33-2ubuntu2.1 [125 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-0 amd64 2.24.33-2ubuntu2.1 [2,038 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail18 amd64 2.24.33-2ubuntu2.1 [15.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgail-common amd64 2.24.33-2ubuntu2.1 [132 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgtk2.0-bin amd64 2.24.33-2ubuntu2.1 [7,936 B]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 librsvg2-common amd64 2.52.5+dfsg-3ubuntu0.2 [17.7 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jre amd64 17.0.17+10-1~22.04 [238 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 openjdk-17-jdk amd64 17.0.17+10-1~22.04 [1,521 kB]\n",
            "Fetched 8,212 kB in 2s (5,344 kB/s)\n",
            "Selecting previously unselected package libatspi2.0-0:amd64.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libatspi2.0-0_2.44.0-3_amd64.deb ...\n",
            "Unpacking libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "Preparing to unpack .../01-libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../02-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../03-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package at-spi2-core.\n",
            "Preparing to unpack .../04-at-spi2-core_2.44.0-3_amd64.deb ...\n",
            "Unpacking at-spi2-core (2.44.0-3) ...\n",
            "Selecting previously unselected package fonts-dejavu-core.\n",
            "Preparing to unpack .../05-fonts-dejavu-core_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-core (2.37-2build1) ...\n",
            "Selecting previously unselected package fonts-dejavu-extra.\n",
            "Preparing to unpack .../06-fonts-dejavu-extra_2.37-2build1_all.deb ...\n",
            "Unpacking fonts-dejavu-extra (2.37-2build1) ...\n",
            "Selecting previously unselected package libatk1.0-data.\n",
            "Preparing to unpack .../07-libatk1.0-data_2.36.0-3build1_all.deb ...\n",
            "Unpacking libatk1.0-data (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk1.0-0:amd64.\n",
            "Preparing to unpack .../08-libatk1.0-0_2.36.0-3build1_amd64.deb ...\n",
            "Unpacking libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Selecting previously unselected package libatk-bridge2.0-0:amd64.\n",
            "Preparing to unpack .../09-libatk-bridge2.0-0_2.38.0-3_amd64.deb ...\n",
            "Unpacking libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Selecting previously unselected package libxcomposite1:amd64.\n",
            "Preparing to unpack .../10-libxcomposite1_1%3a0.4.5-1build2_amd64.deb ...\n",
            "Unpacking libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "Preparing to unpack .../11-libxxf86dga1_2%3a1.1.5-0ubuntu3_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../12-x11-utils_7.7+5build2_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+5build2) ...\n",
            "Selecting previously unselected package libatk-wrapper-java.\n",
            "Preparing to unpack .../13-libatk-wrapper-java_0.38.0-5build1_all.deb ...\n",
            "Unpacking libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libatk-wrapper-java-jni:amd64.\n",
            "Preparing to unpack .../14-libatk-wrapper-java-jni_0.38.0-5build1_amd64.deb ...\n",
            "Unpacking libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Selecting previously unselected package libgtk2.0-common.\n",
            "Preparing to unpack .../15-libgtk2.0-common_2.24.33-2ubuntu2.1_all.deb ...\n",
            "Unpacking libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-0:amd64.\n",
            "Preparing to unpack .../16-libgtk2.0-0_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail18:amd64.\n",
            "Preparing to unpack .../17-libgail18_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgail-common:amd64.\n",
            "Preparing to unpack .../18-libgail-common_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package libgtk2.0-bin.\n",
            "Preparing to unpack .../19-libgtk2.0-bin_2.24.33-2ubuntu2.1_amd64.deb ...\n",
            "Unpacking libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Selecting previously unselected package librsvg2-common:amd64.\n",
            "Preparing to unpack .../20-librsvg2-common_2.52.5+dfsg-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../21-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package openjdk-17-jre:amd64.\n",
            "Preparing to unpack .../22-openjdk-17-jre_17.0.17+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jre:amd64 (17.0.17+10-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-17-jdk:amd64.\n",
            "Preparing to unpack .../23-openjdk-17-jdk_17.0.17+10-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-17-jdk:amd64 (17.0.17+10-1~22.04) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service ‚Üí /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu3) ...\n",
            "Setting up libatspi2.0-0:amd64 (2.44.0-3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up fonts-dejavu-core (2.37-2build1) ...\n",
            "Setting up librsvg2-common:amd64 (2.52.5+dfsg-3ubuntu0.2) ...\n",
            "Setting up libatk1.0-data (2.36.0-3build1) ...\n",
            "Setting up fonts-dejavu-extra (2.37-2build1) ...\n",
            "Setting up libgtk2.0-common (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk1.0-0:amd64 (2.36.0-3build1) ...\n",
            "Setting up libxcomposite1:amd64 (1:0.4.5-1build2) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libgtk2.0-0:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libatk-bridge2.0-0:amd64 (2.38.0-3) ...\n",
            "Setting up libgail18:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up libgtk2.0-bin (2.24.33-2ubuntu2.1) ...\n",
            "Setting up x11-utils (7.7+5build2) ...\n",
            "Setting up libatk-wrapper-java (0.38.0-5build1) ...\n",
            "Setting up libgail-common:amd64 (2.24.33-2ubuntu2.1) ...\n",
            "Setting up openjdk-17-jre:amd64 (17.0.17+10-1~22.04) ...\n",
            "Setting up openjdk-17-jdk:amd64 (17.0.17+10-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-17-openjdk-amd64/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode\n",
            "Setting up libatk-wrapper-java-jni:amd64 (0.38.0-5build1) ...\n",
            "Processing triggers for libgdk-pixbuf-2.0-0:amd64 (2.42.8+dfsg-1ubuntu0.4) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up at-spi2-core (2.44.0-3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
        "!tar xf spark-4.0.1-bin-hadoop3.tgz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8j0_ek35dz3K",
        "outputId": "7d151ea6-f0b4-4f58-e46e-5fffb84ca388"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-12-30 02:21:25--  https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 548955321 (524M) [application/x-gzip]\n",
            "Saving to: ‚Äòspark-4.0.1-bin-hadoop3.tgz‚Äô\n",
            "\n",
            "spark-4.0.1-bin-had 100%[===================>] 523.52M  15.0MB/s    in 32s     \n",
            "\n",
            "2025-12-30 02:21:58 (16.3 MB/s) - ‚Äòspark-4.0.1-bin-hadoop3.tgz‚Äô saved [548955321/548955321]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx3iurVwdRe8",
        "outputId": "aa268e03-fa46-4237-9b52-be079b5f783d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 4.0.1\n"
          ]
        }
      ],
      "source": [
        "# ===============================\n",
        "# Spark 4.0.1 Setup (REQUIRED)\n",
        "# ===============================\n",
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-4.0.1-bin-hadoop3\"\n",
        "os.environ[\"PATH\"] += \":/content/spark-4.0.1-bin-hadoop3/bin\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Spark401-Training\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark version:\", spark.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Spark Level 2 ‚Äì B√†i 2: Filter, Select, WithColumn & Execution Plan\n",
        "\n",
        "## üéØ M·ª•c ti√™u b√†i n√†y\n",
        "\n",
        "**Sau b√†i n√†y b·∫°n s·∫Ω:**\n",
        ">*\tHi·ªÉu Transformation vs Action\n",
        ">*\tBi·∫øt Spark c√≥ ch·∫°y ngay hay kh√¥ng\n",
        ">*\tB·∫Øt ƒë·∫ßu ch·∫°m v√†o shuffle (r·∫•t quan tr·ªçng)\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ B√†i to√°n\n",
        "\n",
        "**D·ªØ li·ªáu giao d·ªãch:**\n",
        "\n",
        "|order_id|customer_id|amount|country|\n",
        "|--------|-----------|------|-------|\n",
        "|1|C001|120|VN|\n",
        "|2|C002|80|VN|\n",
        "|3|C003|200|SG|\n",
        "|4|C001|50|VN|\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XKd5fDAgeRWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå B∆∞·ªõc 1 ‚Äì T·∫°o DataFrame:"
      ],
      "metadata": {
        "id": "MjX4Tjn0fKgB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "data = [\n",
        "    (1, \"C001\", 120, \"VN\"),\n",
        "    (2, \"C002\", 80, \"VN\"),\n",
        "    (3, \"C003\", 200, \"SG\"),\n",
        "    (4, \"C001\", 50, \"VN\"),\n",
        "]\n",
        "\n",
        "columns = [\"order_id\", \"customer_id\", \"amount\", \"country\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEyL-zDceSXQ",
        "outputId": "1ebd279a-555d-4cb4-9419-1671cf2e3e36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+-------+\n",
            "|order_id|customer_id|amount|country|\n",
            "+--------+-----------+------+-------+\n",
            "|       1|       C001|   120|     VN|\n",
            "|       2|       C002|    80|     VN|\n",
            "|       3|       C003|   200|     SG|\n",
            "|       4|       C001|    50|     VN|\n",
            "+--------+-----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå B∆∞·ªõc 2 ‚Äì Transformation (KH√îNG ch·∫°y ngay)\n",
        "\n",
        "```python\n",
        "df_vn = df.filter(df.country == \"VN\")\n",
        "df_vn_high = df_vn.filter(df_vn.amount > 100)\n",
        "```\n",
        "\n",
        ">‚ùì Spark ƒë√£ ch·∫°y ch∆∞a?\n",
        "\n",
        ">üëâ CH∆ØA\n"
      ],
      "metadata": {
        "id": "83d9ow6gfzaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vn = df.filter(df.country == \"VN\")\n",
        "df_vn_high = df_vn.filter(df_vn.amount > 100)"
      ],
      "metadata": {
        "id": "s9kVMccJf-em"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå B∆∞·ªõc 3 ‚Äì Action (Spark b·∫Øt ƒë·∫ßu ch·∫°y)\n",
        "\n",
        "```python\n",
        "df_vn_high.show()\n",
        "```\n",
        "\n",
        "#### üß† Nguy√™n l√Ω quan tr·ªçng\n",
        "\n",
        "|Lo·∫°i|V√≠ d·ª•|\n",
        "|----|-----|\n",
        "|Transformation|filter, select, withColumn|\n",
        "|Action|show, count, collect|\n",
        "\n",
        "> üëâ Spark lazy execution\n"
      ],
      "metadata": {
        "id": "SdSDAFg1gFMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_vn_high.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B78wWbIdgeDA",
        "outputId": "75a7dc13-1aa2-40ab-8449-0a12c790df8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+-------+\n",
            "|order_id|customer_id|amount|country|\n",
            "+--------+-----------+------+-------+\n",
            "|       1|       C001|   120|     VN|\n",
            "+--------+-----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå B∆∞·ªõc 4 ‚Äì Th√™m c·ªôt m·ªõi"
      ],
      "metadata": {
        "id": "OxfeUzL2giom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.withColumn(\n",
        "    \"amount_category\",\n",
        "    F.when(df.amount >= 100, \"HIGH\").otherwise(\"LOW\")\n",
        ")\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnImyu4Hgxb5",
        "outputId": "fbeb09ce-aaa6-4573-f9b2-afbe0eba503f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+-------+---------------+\n",
            "|order_id|customer_id|amount|country|amount_category|\n",
            "+--------+-----------+------+-------+---------------+\n",
            "|       1|       C001|   120|     VN|           HIGH|\n",
            "|       2|       C002|    80|     VN|            LOW|\n",
            "|       3|       C003|   200|     SG|           HIGH|\n",
            "|       4|       C001|    50|     VN|            LOW|\n",
            "+--------+-----------+------+-------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå B∆∞·ªõc 5 ‚Äì Xem Execution Plan:\n",
        "\n",
        "```python\n",
        "df2.explain(True)\n",
        "```\n",
        "\n",
        "Result:\n",
        "\n",
        "```code\n",
        "== Physical Plan ==\n",
        "*(1) Project\n",
        "+- *(1) Scan ExistingRDD\n",
        "```\n",
        "\n",
        "> üëâ Ch∆∞a c√≥ shuffle ‚Üí nh·∫π"
      ],
      "metadata": {
        "id": "juP4wBo4g6hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-EYB5b6hPe2",
        "outputId": "041ae566-1107-42bd-d600-2c6f228a03db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Project [unresolvedstarwithcolumns(amount_category, CASE WHEN '`>=`(amount#2L, 100) THEN HIGH ELSE LOW END, None)]\n",
            "+- LogicalRDD [order_id#0L, customer_id#1, amount#2L, country#3], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "order_id: bigint, customer_id: string, amount: bigint, country: string, amount_category: string\n",
            "Project [order_id#0L, customer_id#1, amount#2L, country#3, CASE WHEN (amount#2L >= cast(100 as bigint)) THEN HIGH ELSE LOW END AS amount_category#30]\n",
            "+- LogicalRDD [order_id#0L, customer_id#1, amount#2L, country#3], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Project [order_id#0L, customer_id#1, amount#2L, country#3, CASE WHEN (amount#2L >= 100) THEN HIGH ELSE LOW END AS amount_category#30]\n",
            "+- LogicalRDD [order_id#0L, customer_id#1, amount#2L, country#3], false\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [order_id#0L, customer_id#1, amount#2L, country#3, CASE WHEN (amount#2L >= 100) THEN HIGH ELSE LOW END AS amount_category#30]\n",
            "+- *(1) Scan ExistingRDD[order_id#0L,customer_id#1,amount#2L,country#3]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Spark Level 2 ‚Äì B√†i 3: groupBy, agg & SHUFFLE (c·ªët l√µi Spark)\n",
        "\n",
        "> **ƒê√¢y l√† b√†i QUAN TR·ªåNG NH·∫§T tr∆∞·ªõc khi b·∫°n l√†m Spark th·∫≠t s·ª± trong CDP / Dataproc / EMR**\n",
        "\n",
        "\n",
        "## üéØ M·ª•c ti√™u b√†i n√†y\n",
        "\n",
        "**Sau b√†i n√†y b·∫°n s·∫Ω:**\n",
        ">*\tHi·ªÉu shuffle l√† g√¨ (ƒë√∫ng b·∫£n ch·∫•t)\n",
        ">*\tBi·∫øt v√¨ sao groupBy r·∫•t ƒë·∫Øt\n",
        ">*\tƒê·ªçc ƒë∆∞·ª£c execution plan c√≥ shuffle\n",
        ">*\tBi·∫øt khi n√†o Spark scale / khi n√†o ch·∫øt"
      ],
      "metadata": {
        "id": "R_a_Yguqi4cY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ B√†i to√°n\n",
        "\n",
        "D·ªØ li·ªáu order (nh∆∞ tr∆∞·ªõc):"
      ],
      "metadata": {
        "id": "kilyEjRSjXMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "data = [\n",
        "    (1, \"C001\", 120, \"VN\"),\n",
        "    (2, \"C002\", 80, \"VN\"),\n",
        "    (3, \"C003\", 200, \"SG\"),\n",
        "    (4, \"C001\", 50, \"VN\"),\n",
        "    (5, \"C002\", 70, \"SG\"),\n",
        "]\n",
        "\n",
        "columns = [\"order_id\", \"customer_id\", \"amount\", \"country\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MhjgobKjcuw",
        "outputId": "b986e723-d1b0-4418-d22e-faa5ca40486c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-----------+------+-------+\n",
            "|order_id|customer_id|amount|country|\n",
            "+--------+-----------+------+-------+\n",
            "|       1|       C001|   120|     VN|\n",
            "|       2|       C002|    80|     VN|\n",
            "|       3|       C003|   200|     SG|\n",
            "|       4|       C001|    50|     VN|\n",
            "|       5|       C002|    70|     SG|\n",
            "+--------+-----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ GroupBy c∆° b·∫£n\n",
        "\n",
        "‚ùì Y√™u c·∫ßu\n",
        "\n",
        "üëâ T·ªïng ti·ªÅn theo customer_id"
      ],
      "metadata": {
        "id": "YQX77vQNjnwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_group = df.groupBy(\"customer_id\").agg(\n",
        "    F.sum(\"amount\").alias(\"total_amount\")\n",
        ")\n",
        "\n",
        "df_group.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmrMq4nqjs9e",
        "outputId": "6732e92f-92dd-47b7-dcec-f851fe17f333"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|customer_id|total_amount|\n",
            "+-----------+------------+\n",
            "|       C001|         170|\n",
            "|       C002|         150|\n",
            "|       C003|         200|\n",
            "+-----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üî• STOP ‚Äì ƒê√¢y l√† l√∫c SHUFFLE x·∫£y ra\n",
        "\n",
        "## 3Ô∏è‚É£ Shuffle l√† g√¨? (Hi·ªÉu ƒë√∫ng, kh√¥ng m∆° h·ªì)\n",
        "\n",
        "### üß† ƒê·ªãnh nghƒ©a CHU·∫®N:\n",
        "\n",
        "**Shuffle** = **Spark** ph·∫£i di chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c executor ƒë·ªÉ gom c√°c key gi·ªëng nhau v·ªÅ c√πng 1 n∆°i\n",
        "\n",
        "**V√≠ d·ª•:**\n",
        ">*\tOrder c·ªßa C001 n·∫±m ·ªü partition 1\n",
        ">*\tOrder kh√°c c·ªßa C001 n·∫±m ·ªü partition 5\n",
        ">> ‚Üí Spark b·∫Øt bu·ªôc ph·∫£i chuy·ªÉn d·ªØ li·ªáu\n",
        "\n",
        ">üëâ Network + Disk + Serialize = t·ªën t√†i nguy√™n\n",
        "\n",
        "### üß© Minh h·ªça logic\n",
        "\n",
        "```code\n",
        "Partition 1: C001, C002\n",
        "Partition 2: C003\n",
        "Partition 3: C001, C002\n",
        "\n",
        "groupBy(customer_id)\n",
        "        ‚Üì\n",
        "Shuffle\n",
        "        ‚Üì\n",
        "Partition A: C001\n",
        "Partition B: C002\n",
        "Partition C: C003\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EeKPcDSrj6Qp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Xem Execution Plan (B·∫ÆT BU·ªòC)\n",
        "\n",
        "B·∫°n s·∫Ω th·∫•y ƒëo·∫°n gi·ªëng:\n",
        "\n",
        "```code\n",
        "Exchange hashpartitioning(customer_id, 200)\n",
        "```\n",
        "\n",
        "üìå Exchange = SHUFFLE"
      ],
      "metadata": {
        "id": "k79LbXxsko2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_group.explain(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmT1PsG9kqb4",
        "outputId": "52e6f64f-28d4-443c-f175-c05b8c7fbb15"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Aggregate ['customer_id], ['customer_id, 'sum('amount) AS total_amount#64]\n",
            "+- LogicalRDD [order_id#47L, customer_id#48, amount#49L, country#50], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "customer_id: string, total_amount: bigint\n",
            "Aggregate [customer_id#48], [customer_id#48, sum(amount#49L) AS total_amount#64L]\n",
            "+- LogicalRDD [order_id#47L, customer_id#48, amount#49L, country#50], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Aggregate [customer_id#48], [customer_id#48, sum(amount#49L) AS total_amount#64L]\n",
            "+- Project [customer_id#48, amount#49L]\n",
            "   +- LogicalRDD [order_id#47L, customer_id#48, amount#49L, country#50], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- HashAggregate(keys=[customer_id#48], functions=[sum(amount#49L)], output=[customer_id#48, total_amount#64L])\n",
            "   +- Exchange hashpartitioning(customer_id#48, 4), ENSURE_REQUIREMENTS, [plan_id=119]\n",
            "      +- HashAggregate(keys=[customer_id#48], functions=[partial_sum(amount#49L)], output=[customer_id#48, sum#73L])\n",
            "         +- Project [customer_id#48, amount#49L]\n",
            "            +- Scan ExistingRDD[order_id#47L,customer_id#48,amount#49L,country#50]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ V√¨ sao shuffle NGUY HI·ªÇM?\n",
        "\n",
        "|V·∫•n ƒë·ªÅ|H·∫≠u qu·∫£|\n",
        "|------|-------|\n",
        "|Nhi·ªÅu d·ªØ li·ªáu|Ch·∫≠m|\n",
        "|Skew key|Executor ch·∫øt|\n",
        "|Network y·∫øu|Timeout|\n",
        "|Disk ch·∫≠m|Spill|\n",
        "\n",
        "> üëâ 90% job Spark ch·∫≠m = shuffle k√©m ki·ªÉm so√°t\n"
      ],
      "metadata": {
        "id": "J6L38dz8lA55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6Ô∏è‚É£ V√≠ d·ª• SHUFFLE T·ªÜ (anti-pattern)\n",
        "\n",
        "```python\n",
        "df.groupBy(\"country\", \"customer_id\").count().show()\n",
        "```\n",
        "\n",
        ">‚ùå GroupBy nhi·ªÅu c·ªôt kh√¥ng c·∫ßn thi·∫øt\n",
        "\n",
        ">‚ùå Cardinality cao ‚Üí shuffle n·∫∑ng\n"
      ],
      "metadata": {
        "id": "F5MKXwlclzOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"country\", \"customer_id\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mwc9VfS_l4DE",
        "outputId": "cadfb3b7-de3b-4797-cf0d-ef635ccbadfe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+-----+\n",
            "|country|customer_id|count|\n",
            "+-------+-----------+-----+\n",
            "|     VN|       C001|    2|\n",
            "|     VN|       C002|    1|\n",
            "|     SG|       C002|    1|\n",
            "|     SG|       C003|    1|\n",
            "+-------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ Gi·∫£m shuffle ‚Äì c√°ch ƒë·∫ßu ti√™n (c∆° b·∫£n)\n",
        "\n",
        "### ‚úÖ Ch·ªâ groupBy ƒë√∫ng th·ª© c·∫ßn\n",
        "\n",
        "```python\n",
        "df.groupBy(\"country\").sum(\"amount\").show()\n",
        "```"
      ],
      "metadata": {
        "id": "fcoa0T_VmFOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"country\").sum(\"amount\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65qVKb6mmMMj",
        "outputId": "1c681d2a-5e88-4ded-f26e-71ed038e7303"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|country|sum(amount)|\n",
            "+-------+-----------+\n",
            "|     VN|        250|\n",
            "|     SG|        270|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8Ô∏è‚É£ Ki·ªÉm so√°t s·ªë partition khi shuffle"
      ],
      "metadata": {
        "id": "Sb4dRS9LmQwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M·∫∑c ƒë·ªãnh:\n",
        "\n",
        "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
        "\n",
        "# ‚Üí th∆∞·ªùng l√† 200 (QU√Å NHI·ªÄU cho dataset nh·ªè)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "L_OLay-cmTCa",
        "outputId": "0c2da4df-0606-4a96-cc49-d0c26f703e08"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîß Gi·∫£m xu·ªëng khi test / small data\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
        "\n",
        "# üëâ Ch·∫°y l·∫°i groupBy v√† explain"
      ],
      "metadata": {
        "id": "T8J7ycTEmjKT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† C√ÇU H·ªéI & TR·∫¢ L·ªúI\n",
        "\n",
        "## üîπ Spark Lazy Evaluation & Action\n",
        "\n",
        "### ‚ùì 1. V√¨ sao filter() kh√¥ng ch·∫°y ngay?\n",
        "\n",
        "### Tr·∫£ l·ªùi:\n",
        "\n",
        "**filter()** l√† **transformation**, **Spark** s·ª≠ d·ª•ng **lazy evaluation**, n√™n ch∆∞a th·ª±c thi ngay m√† ch·ªâ x√¢y d·ª±ng logical execution plan.\n",
        "\n",
        "**Gi·∫£i th√≠ch ng·∫Øn:**\n",
        ">*\tSpark ch∆∞a ƒë·ªçc d·ªØ li·ªáu\n",
        ">*\tCh·ªâ ghi nh·ªõ: ‚Äúkhi n√†o c·∫ßn th√¨ filter th·∫ø n√†y‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì 2. show() kh√°c collect() ·ªü ƒëi·ªÉm n√†o?\n",
        "\n",
        "### Tr·∫£ l·ªùi:\n",
        "\n",
        "|**show()**|**collect()**|\n",
        "|----------|-------------|\n",
        "|L√† action|L√† action|\n",
        "|Ch·ªâ l·∫•y m·ªôt ph·∫ßn d·ªØ li·ªáu (m·∫∑c ƒë·ªãnh 20 d√≤ng)|L·∫•y to√†n b·ªô d·ªØ li·ªáu|\n",
        "|An to√†n v·ªõi data l·ªõn|R·∫§T NGUY HI·ªÇM v·ªõi data l·ªõn|\n",
        "|D√πng ƒë·ªÉ debug|Ch·ªâ d√πng khi data r·∫•t nh·ªè|\n",
        "\n",
        "#### K·∫øt lu·∫≠n:\n",
        "\n",
        "> ‚ùå Kh√¥ng d√πng collect() trong production\n",
        "\n",
        "> ‚úÖ D√πng show(), take(), limit()\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùì 3. Khi n√†o Spark m·ªõi th·ª±c s·ª± ƒë·ªçc d·ªØ li·ªáu?\n",
        "\n",
        "### Tr·∫£ l·ªùi:\n",
        "\n",
        "> Spark ch·ªâ th·ª±c s·ª± ƒë·ªçc v√† x·ª≠ l√Ω d·ªØ li·ªáu khi g·∫∑p ACTION nh∆∞:\n",
        ">> show(), count(), collect(), write()\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ GroupBy & Shuffle:\n",
        "\n",
        "### ‚ùì 4. V√¨ sao groupBy() lu√¥n g√¢y shuffle?\n",
        "\n",
        ">V√¨ **Spark** c·∫ßn gom t·∫•t c·∫£ c√°c record c√≥ c√πng **key** v·ªÅ c√πng m·ªôt **executor**, n√™n b·∫Øt bu·ªôc ph·∫£i di chuy·ªÉn d·ªØ li·ªáu gi·ªØa c√°c **partition** ‚Üí g√¢y **shuffle**.\n",
        "\n",
        ">> üìå Kh√¥ng c√≥ c√°ch n√†o groupBy m√† kh√¥ng shuffle (tr·ª´ v√†i case r·∫•t ƒë·∫∑c bi·ªát).\n",
        "\n",
        "---\n",
        "\n",
        "###‚ùì 5. Exchange trong execution plan nghƒ©a l√† g√¨?\n",
        "\n",
        "### Tr·∫£ l·ªùi:\n",
        "\n",
        "**Exchange bi·ªÉu th·ªã giai ƒëo·∫°n shuffle, n∆°i Spark:**\n",
        "\n",
        ">*\trepartition d·ªØ li·ªáu\n",
        ">*\ttruy·ªÅn d·ªØ li·ªáu qua network\n",
        ">*\tghi/ƒë·ªçc disk n·∫øu c·∫ßn\n",
        "\n",
        ">>üìå ƒê√¢y l√† b∆∞·ªõc ƒë·∫Øt nh·∫•t trong Spark.\n",
        "\n",
        "---\n",
        "\n",
        "###‚ùì 6. V√¨ sao spark.sql.shuffle.partitions = 200 nguy hi·ªÉm v·ªõi dataset nh·ªè?\n",
        "\n",
        "### Tr·∫£ l·ªùi:\n",
        "\n",
        "V√¨ **Spark** t·∫°o **200 task shuffle**, trong khi d·ªØ li·ªáu r·∫•t √≠t ‚Üí\n",
        "**overhead** (task scheduling, network, file) l·ªõn h∆°n x·ª≠ l√Ω d·ªØ li·ªáu.\n",
        "\n",
        "**üìå V·ªõi data nh·ªè:**\n",
        ">*\t200 partitions = l√£ng ph√≠\n",
        ">*\tJob ch·∫≠m h∆°n thay v√¨ nhanh\n",
        "\n",
        "---\n",
        "\n",
        "###‚ùì 7. Khi n√†o shuffle b·∫Øt bu·ªôc, khi n√†o tr√°nh ƒë∆∞·ª£c?\n",
        "\n",
        "### Tr·∫£ l·ªùi:\n",
        "\n",
        "**Shuffle B·∫ÆT BU·ªòC khi:**\n",
        ">*\tgroupBy\n",
        ">*\tjoin (kh√¥ng broadcast)\n",
        ">*\tdistinct\n",
        ">*\torderBy\n",
        "\n",
        "**Shuffle C√ì TH·ªÇ TR√ÅNH khi:**\n",
        ">*\tfilter\n",
        ">*\tselect\n",
        ">*\twithColumn\n",
        ">*\tmap\n",
        ">*\tlimit\n",
        ">*\tbroadcast join\n"
      ],
      "metadata": {
        "id": "nPu1nMH6Iv0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ PH·∫¶N 2 ‚Äì Spark Level 2 ‚Äì B√†i 4\n",
        "\n",
        "## üî• repartition() vs coalesce() (R·∫§T QUAN TR·ªåNG)\n",
        "\n",
        "> ƒê√¢y l√† ch·ªó **90% ng∆∞·ªùi d√πng Spark sai**\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ M·ª•c ti√™u b√†i h·ªçc\n",
        "\n",
        "**Sau b√†i n√†y b·∫°n s·∫Ω:**\n",
        ">* Bi·∫øt khi n√†o Spark shuffle\n",
        ">* D√πng ƒë√∫ng repartition / coalesce\n",
        ">* Tr√°nh job ch·∫≠m & crash executor\n"
      ],
      "metadata": {
        "id": "HtudishKNum4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Kh·ªüi t·∫°o DataFrame l·ªõn (demo):\n",
        "\n",
        "M·∫∑c ƒë·ªãnh: ~8 ho·∫∑c ~16 partitions (tu·ª≥ Colab)"
      ],
      "metadata": {
        "id": "-QdFwBnoOqBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.range(0, 1_000_000)\n",
        "print(df.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZnHKdDgOulI",
        "outputId": "62e12326-0d95-44a5-cb01-f1c43a4b2315"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ repartition() ‚Äì LU√îN SHUFFLE\n",
        "\n",
        "```python\n",
        "df2 = df.repartition(4)\n",
        "df2.explain()\n",
        "```\n",
        "\n",
        "### üî• ƒêi·ªÅu g√¨ x·∫£y ra?\n",
        ">*\tSpark shuffle to√†n b·ªô d·ªØ li·ªáu\n",
        ">*\tRedistribute ƒë·ªÅu sang 4 partition\n",
        "\n",
        "###üìå D√ôNG KHI:\n",
        ">*\tTr∆∞·ªõc join\n",
        ">*\tTr∆∞·ªõc groupBy\n",
        ">*\tMu·ªën ph√¢n ph·ªëi l·∫°i data ƒë·ªÅu\n",
        "\n",
        "> ‚ùå KH√îNG d√πng n·∫øu ch·ªâ mu·ªën gi·∫£m partition"
      ],
      "metadata": {
        "id": "mcZCViu5O8N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df.repartition(4)\n",
        "df2.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unEJOhBZPB-M",
        "outputId": "fadbbe6c-cb62-4cae-feaa-a222cae63d30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=76]\n",
            "   +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ coalesce() ‚Äì KH√îNG shuffle (ho·∫∑c r·∫•t √≠t)\n",
        "\n",
        "```python\n",
        "df3 = df.coalesce(4)\n",
        "df3.explain()\n",
        "```\n",
        "\n",
        "### üî• ƒêi·ªÅu g√¨ x·∫£y ra?\n",
        ">*\tSpark gom partition l·∫°i\n",
        ">*\tKH√îNG di chuy·ªÉn d·ªØ li·ªáu qua network\n",
        "\n",
        "### üìå D√ôNG KHI:\n",
        ">*\tSau filter\n",
        ">*\tTr∆∞·ªõc write\n",
        ">*\tMu·ªën gi·∫£m s·ªë file output"
      ],
      "metadata": {
        "id": "WICW1KsIPfxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df.coalesce(4)\n",
        "df3.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CEqEM3XPkcJ",
        "outputId": "6910e23e-c579-4871-b329-528c796bc639"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "Coalesce 4\n",
            "+- *(1) Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ So s√°nh nhanh (PH·∫¢I NH·ªö)\n",
        "\n",
        "|**Ti√™u ch√≠**|**repartition**|**coalesce**|\n",
        "|------------|---------------|-------------|\n",
        "|Shuffle|‚úÖ C√≥|‚ùå Kh√¥ng|\n",
        "|TƒÉng partition|‚úÖ|‚ùå|\n",
        "|Gi·∫£m partition|‚úÖ|‚úÖ|\n",
        "|Nhanh|‚ùå|‚úÖ|\n",
        "|D√πng tr∆∞·ªõc groupBy|‚úÖ|‚ùå|\n",
        "\n",
        "## 6Ô∏è‚É£ Case th·ª±c t·∫ø (PRODUCTION)\n",
        "\n",
        "### ‚ùå Sai (r·∫•t hay g·∫∑p)\n",
        "\n",
        "```python\n",
        "df.filter(\"amount > 100\").repartition(4)\n",
        "```\n",
        ">üëâ Shuffle kh√¥ng c·∫ßn thi·∫øt\n",
        "\n",
        "### ‚úÖ ƒê√∫ng\n",
        "\n",
        "```python\n",
        "df.filter(\"amount > 100\").coalesce(4)\n",
        "```\n",
        "\n",
        "### ‚úÖ Tr∆∞·ªõc groupBy\n",
        "\n",
        "```python\n",
        "df.repartition(\"customer_id\") \\\n",
        "  .groupBy(\"customer_id\") \\\n",
        "  .sum(\"amount\")\n",
        "```\n",
        "\n",
        "> üëâ Shuffle c√≥ ki·ªÉm so√°t\n"
      ],
      "metadata": {
        "id": "0siyuIicVxnw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7Ô∏è‚É£ B√ÄI T·∫¨P (L√ÄM TRONG NOTEBOOK)\n",
        "\n",
        "1.\tT·∫°o DataFrame 1 tri·ªáu d√≤ng\n",
        "2.\tSo s√°nh:\n",
        "*\trepartition(4)\n",
        "*\tcoalesce(4)\n",
        "3.\tD√πng explain() ‚Üí t√¨m Exchange\n",
        "4.\tGhi nh·∫≠n x√©t b·∫±ng markdown\n",
        "\n",
        "> A: ‚úÖ coalesce() kh√¥ng t·∫°o Exchange ‚Üí kh√¥ng shuffle\n"
      ],
      "metadata": {
        "id": "VWdZB6LvXT6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\tT·∫°o DataFrame 1 tri·ªáu d√≤ng\n",
        "df = spark.range(0, 1_000_000)\n",
        "print(df.rdd.getNumPartitions())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRsamUldXkBI",
        "outputId": "237b8410-8a20-4d05-d610-3e1828bfd40c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.\tSo s√°nh:\n",
        "## repartition\n",
        "df2 = df.repartition(4)\n",
        "df2.explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_NV7rSkXnh_",
        "outputId": "57658f2d-6634-491d-d50c-eba14c448b3d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Exchange RoundRobinPartitioning(4), REPARTITION_BY_NUM, [plan_id=92]\n",
            "   +- Range (0, 1000000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Spark Level 2 ‚Äì B√†i 5: Join Strategies\n",
        "\n",
        "> ‚ö†Ô∏è **80% job Spark** ch·∫≠m / **fail OOM** l√† do **JOIN SAI**\n",
        "\n",
        "‚∏ª\n",
        "\n",
        "## üéØ M·ª•c ti√™u b√†i h·ªçc\n",
        "\n",
        "### Sau b√†i n√†y, b·∫°n s·∫Ω:\n",
        ">*\tHi·ªÉu Spark join ho·∫°t ƒë·ªông th·∫ø n√†o\n",
        ">*\tƒê·ªçc ƒë∆∞·ª£c physical plan c·ªßa join\n",
        ">*\tBi·∫øt khi n√†o d√πng broadcast join\n",
        ">*\tTr√°nh shuffle join ngu ng·ªëc\n",
        ">*\t√Åp d·ª•ng tr·ª±c ti·∫øp cho Dataproc / BigQuery / CDP\n",
        "\n",
        "‚∏ª\n",
        "\n",
        "## 1Ô∏è‚É£ C√°c lo·∫°i Join trong Spark\n",
        "\n",
        "**Spark** KH√îNG ch·ªâ c√≥ 1 lo·∫°i join, m√† c√≥ nhi·ªÅu chi·∫øn l∆∞·ª£c th·ª±c thi.\n",
        "\n",
        "### üîπ 4 chi·∫øn l∆∞·ª£c ch√≠nh:\n",
        "1.\tBroadcast Hash Join ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "2.\tShuffle Hash Join\n",
        "3.\tSort Merge Join\n",
        "4.\tCartesian / Nested Loop Join (‚ùå tr√°nh)\n",
        "\n",
        "‚∏ª\n",
        "\n",
        "### 2Ô∏è‚É£ Broadcast Join ‚Äì VUA HI·ªÜU NƒÇNG\n",
        "\n",
        "#### üî• Khi n√†o d√πng?\n",
        ">*\t1 b·∫£ng r·∫•t nh·ªè\n",
        ">*\tB·∫£ng c√≤n l·∫°i r·∫•t l·ªõn\n",
        "\n",
        "#### üìå V√≠ d·ª• th·ª±c t·∫ø:\n",
        ">*\torders (100M rows)\n",
        ">*\tcustomers (10K rows)\n",
        "\n",
        "#### ‚úÖ Code chu·∫©n\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "joined = orders.join(\n",
        "    broadcast(customers),\n",
        "    \"customer_id\"\n",
        ")\n",
        "```\n",
        "\n",
        "#### üîç Execution plan\n",
        "\n",
        "```python\n",
        "joined.explain()\n",
        "```\n",
        "\n",
        "S·∫Ω th·∫•y\n",
        "\n",
        "```code\n",
        "BroadcastHashJoin\n",
        "```\n",
        "\n",
        "#### üìå Nghƒ©a l√†:\n",
        ">*\tB·∫£ng nh·ªè ƒë∆∞·ª£c copy v√†o RAM c·ªßa m·ªói executor\n",
        ">*\tKH√îNG SHUFFLE b·∫£ng l·ªõn\n",
        ">*\tNhanh nh·∫•t c√≥ th·ªÉ\n",
        "\n",
        "---\n",
        "\n",
        "#### ‚ùå Kh√¥ng d√πng broadcast khi:\n",
        ">*\tB·∫£ng > v√†i trƒÉm MB\n",
        ">*\tExecutor RAM nh·ªè\n",
        "\n",
        "> üëâ D·ªÖ OOM executor\n",
        "\n",
        "---\n",
        "\n",
        "### 3Ô∏è‚É£ Shuffle Join ‚Äì NGUY HI·ªÇM\n",
        "\n",
        "#### üî• Khi n√†o x·∫£y ra?\n",
        ">*\tJoin 2 b·∫£ng l·ªõn\n",
        ">*\tKh√¥ng c√≥ broadcast\n",
        "\n",
        "```python\n",
        "orders.join(customers, \"customer_id\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### üîç Execution plan\n",
        "\n",
        "```code\n",
        "SortMergeJoin\n",
        "Exchange\n",
        "Exchange\n",
        "```\n",
        "\n",
        "#### üìå √ù nghƒ©a:\n",
        ">*\tC·∫£ 2 b·∫£ng ƒë·ªÅu shuffle\n",
        ">*\tSort theo join key\n",
        ">*\tNetwork + disk + CPU = üí∏üí∏üí∏\n",
        "\n",
        "---\n",
        "\n",
        "####‚ùó ƒê√¢y l√† l√Ω do:\n",
        ">* Job ch·∫≠m\n",
        ">*\tDisk spill\n",
        ">*\tExecutor ch·∫øt\n",
        "\n",
        "---\n",
        "\n",
        "### 4Ô∏è‚É£ Sort Merge Join (m·∫∑c ƒë·ªãnh)\n",
        "\n",
        "Spark d√πng Sort Merge Join khi:\n",
        ">*\tKh√¥ng broadcast ƒë∆∞·ª£c\n",
        ">*\tData l·ªõn\n",
        ">*\tKey c√≥ th·ªÉ sort\n",
        "\n",
        "> üìå ƒê√¢y l√† default join strategy\n",
        "\n",
        "---\n",
        "\n",
        "### 5Ô∏è‚É£ Shuffle Hash Join (√≠t g·∫∑p)\n",
        "\n",
        "X·∫£y ra khi:\n",
        ">*\tJoin key nh·ªè\n",
        ">*\tspark.sql.join.preferSortMergeJoin = false\n",
        "\n",
        "> üìå Hi·∫øm khi d√πng trong production.\n",
        "\n",
        "---\n",
        "\n",
        "### 6Ô∏è‚É£ Join Strategy Summary (PH·∫¢I NH·ªö)\n",
        "\n",
        "|**Strategy**|**Shuffle**|**Nhanh**|**Khi d√πng**|\n",
        "|------------|-----------|---------|------------|\n",
        "|BroadcastHashJoin|‚ùå|‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê|1 b·∫£ng nh·ªè|\n",
        "|SortMergeJoin|‚úÖ|‚≠ê‚≠ê|2 b·∫£ng l·ªõn|\n",
        "|ShuffleHashJoin|‚úÖ|‚≠ê‚≠ê|Hi·∫øm|\n",
        "|Cartesian|‚ùå|üíÄ|Tr√°nh|\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhvaCKlTZCno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T·∫°o customer dataframe\n",
        "\n",
        "data = [\n",
        "    (1, \"Nguy·ªÖn VƒÉn A\", 1987, \"IT\"),\n",
        "    (2, \"Nguy·ªÖn VƒÉn B\", 1988, \"KD\"),\n",
        "    (3, \"Nguy·ªÖn VƒÉn C\", 1989, \"KT\"),\n",
        "    (4, \"Nguy·ªÖn VƒÉn D\", 1990, \"IT\"),\n",
        "    (5, \"Nguy·ªÖn VƒÉn E\", 1991, \"KD\"),\n",
        "    (6, \"Nguy·ªÖn VƒÉn F\", 1992, \"KT\"),\n",
        "    (7, \"Nguy·ªÖn VƒÉn G\", 1993, \"IT\"),\n",
        "    (8, \"Nguy·ªÖn VƒÉn H\", 1994, \"KD\"),\n",
        "    (9, \"Nguy·ªÖn VƒÉn I\", 1995, \"KT\"),\n",
        "    (10, \"Nguy·ªÖn VƒÉn K\", 1996, \"IT\"),\n",
        "    (11, \"Nguy·ªÖn VƒÉn L\", 1997, \"KD\"),\n",
        "    (12, \"Nguy·ªÖn VƒÉn M\", 1998, \"KT\"),\n",
        "    (13, \"Nguy·ªÖn VƒÉn N\", 1999, \"IT\")\n",
        "]\n",
        "\n",
        "cus_df = spark.createDataFrame(data, [\"id\", \"name\", \"birth_year\", \"department\"])\n",
        "cus_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwaoxbhcbqOA",
        "outputId": "c9d341ce-8036-4b8f-bb5f-284a701b7f30"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+----------+----------+\n",
            "| id|        name|birth_year|department|\n",
            "+---+------------+----------+----------+\n",
            "|  1|Nguy·ªÖn VƒÉn A|      1987|        IT|\n",
            "|  2|Nguy·ªÖn VƒÉn B|      1988|        KD|\n",
            "|  3|Nguy·ªÖn VƒÉn C|      1989|        KT|\n",
            "|  4|Nguy·ªÖn VƒÉn D|      1990|        IT|\n",
            "|  5|Nguy·ªÖn VƒÉn E|      1991|        KD|\n",
            "|  6|Nguy·ªÖn VƒÉn F|      1992|        KT|\n",
            "|  7|Nguy·ªÖn VƒÉn G|      1993|        IT|\n",
            "|  8|Nguy·ªÖn VƒÉn H|      1994|        KD|\n",
            "|  9|Nguy·ªÖn VƒÉn I|      1995|        KT|\n",
            "| 10|Nguy·ªÖn VƒÉn K|      1996|        IT|\n",
            "| 11|Nguy·ªÖn VƒÉn L|      1997|        KD|\n",
            "| 12|Nguy·ªÖn VƒÉn M|      1998|        KT|\n",
            "| 13|Nguy·ªÖn VƒÉn N|      1999|        IT|\n",
            "+---+------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# order df\n",
        "\n",
        "order_data = [\n",
        "    (1, 1, 100, \"VN\"),\n",
        "    (2, 2, 200, \"SG\"),\n",
        "    (3, 3, 300, \"VN\"),\n",
        "    (4, 4, 400, \"SG\"),\n",
        "    (5, 5, 500, \"VN\"),\n",
        "    (6, 6, 600, \"SG\"),\n",
        "    (7, 7, 700, \"VN\"),\n",
        "    (8, 8, 800, \"SG\"),\n",
        "    (9, 9, 900, \"VN\"),\n",
        "    (10, 10, 1000, \"SG\"),\n",
        "    (11, 11, 1100, \"VN\"),\n",
        "    (12, 12, 1200, \"SG\"),\n",
        "    (13, 13, 1300, \"VN\")\n",
        "]\n",
        "\n",
        "order_df = spark.createDataFrame(order_data, [\"id\", \"customer_id\", \"amount\", \"country\"])\n",
        "order_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28PoJjqUcnxB",
        "outputId": "308692f3-1deb-426c-e898-191bc4d40a66"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------+------+-------+\n",
            "| id|customer_id|amount|country|\n",
            "+---+-----------+------+-------+\n",
            "|  1|          1|   100|     VN|\n",
            "|  2|          2|   200|     SG|\n",
            "|  3|          3|   300|     VN|\n",
            "|  4|          4|   400|     SG|\n",
            "|  5|          5|   500|     VN|\n",
            "|  6|          6|   600|     SG|\n",
            "|  7|          7|   700|     VN|\n",
            "|  8|          8|   800|     SG|\n",
            "|  9|          9|   900|     VN|\n",
            "| 10|         10|  1000|     SG|\n",
            "| 11|         11|  1100|     VN|\n",
            "| 12|         12|  1200|     SG|\n",
            "| 13|         13|  1300|     VN|\n",
            "+---+-----------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7Ô∏è‚É£ B√†i t·∫≠p TH·ª∞C CHI·∫æN (R·∫§T QUAN TR·ªåNG)"
      ],
      "metadata": {
        "id": "NCfr5RxGf8qE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "orders = spark.range(0, 1_000_000) \\\n",
        "    .withColumn(\"customer_id\", (col(\"id\") % 1000))\n",
        "\n",
        "customers = spark.range(0, 1000) \\\n",
        "    .withColumnRenamed(\"id\", \"customer_id\")"
      ],
      "metadata": {
        "id": "i7RAbIGKf-sX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîπ Case 1: Join th∆∞·ªùng (SAI)\n",
        "\n",
        "üëâ Ghi:\n",
        ">*\tC√≥ Exchange kh√¥ng?\n",
        ">> C√≥\n",
        ">*\tStrategy g√¨?\n",
        ">> SortMergeJoin\n",
        "\n",
        "#### üîç Gi·∫£i th√≠ch\n",
        ">* \tB·∫°n KH√îNG d√πng broadcast()\n",
        ">*\tSpark s·∫Ω:\n",
        ">> 1.\tShuffle c·∫£ orders\n",
        ">> 2.\tShuffle c·∫£ customers\n",
        ">> 3.\tSort theo customer_id\n",
        ">> 4.\tJoin\n"
      ],
      "metadata": {
        "id": "a8g_JTiWgiT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orders.join(customers, \"customer_id\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GuVDEb0gr6a",
        "outputId": "5f07a37a-2b07-4620-82a4-ffff4aa78e45"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#109L, id#108L]\n",
            "   +- BroadcastHashJoin [customer_id#109L], [customer_id#111L], Inner, BuildRight, false\n",
            "      :- Project [id#108L, (id#108L % 1000) AS customer_id#109L]\n",
            "      :  +- Filter isnotnull((id#108L % 1000))\n",
            "      :     +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=146]\n",
            "         +- Project [id#110L AS customer_id#111L]\n",
            "            +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîπ Case 2: Broadcast join (ƒê√öNG)\n",
        "\n",
        "üëâ Ghi:\n",
        ">*\tExchange c√≤n kh√¥ng?\n",
        ">> ‚ùå Kh√¥ng (ph√≠a b·∫£ng nh·ªè)\n",
        ">*\tNhanh h∆°n v√¨ sao?\n",
        ">> V√¨ Spark kh√¥ng shuffle b·∫£ng l·ªõn,\n",
        "  ch·ªâ broadcast b·∫£ng nh·ªè v√†o RAM c·ªßa m·ªói executor,\n",
        "  tr√°nh network + disk IO.\n",
        "\n",
        "#### üîç Gi·∫£i th√≠ch\n",
        "\n",
        "Trong Broadcast Join:\n",
        ">*\tcustomers (nh·ªè):\n",
        ">*\tüëâ Broadcast ‚Üí copy v√†o RAM executor\n",
        ">*\t‚ùå Kh√¥ng Exchange\n",
        ">*\torders (l·ªõn):\n",
        ">*\t‚ùå Kh√¥ng c·∫ßn shuffle\n",
        ">*\tJoin tr·ª±c ti·∫øp v·ªõi hash map trong RAM"
      ],
      "metadata": {
        "id": "MxEpNgOKhLfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "orders.join(\n",
        "    broadcast(customers),\n",
        "    \"customer_id\"\n",
        ").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fImQQ5hehKKX",
        "outputId": "123ada2b-ceaa-4d98-aa61-2a7751be1f67"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#109L, id#108L]\n",
            "   +- BroadcastHashJoin [customer_id#109L], [customer_id#111L], Inner, BuildRight, false\n",
            "      :- Project [id#108L, (id#108L % 1000) AS customer_id#109L]\n",
            "      :  +- Filter isnotnull((id#108L % 1000))\n",
            "      :     +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=177]\n",
            "         +- Project [id#110L AS customer_id#111L]\n",
            "            +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üîπ Case 3: Disable broadcast (ƒë·ªÉ h·ªçc)\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "orders.join(customers, \"customer_id\").explain()\n",
        "```\n",
        "\n",
        "##### ‚úÖ ƒêi·ªÅu g√¨ x·∫£y ra?\n",
        "\n",
        "* Spark C·∫§M broadcast\n",
        "* D√π b·∫£ng nh·ªè, Spark v·∫´n d√πng SortMergeJoin\n",
        "* Exchange xu·∫•t hi·ªán ·ªü c·∫£ 2 b·∫£ng\n",
        "\n",
        "##### üìå Execution plan quay l·∫°i:\n",
        "\n",
        "```code\n",
        "SortMergeJoin\n",
        "Exchange\n",
        "Exchange\n",
        "```\n",
        "\n",
        "##### üëâ ƒê√¢y l√† b·∫±ng ch·ª©ng:\n",
        "*\tBroadcast KH√îNG t·ª± ƒë·ªông\n",
        "*\tSpark ch·ªâ broadcast khi:\n",
        ">*\tB·∫£ng nh·ªè\n",
        ">*\tThreshold cho ph√©p\n",
        ">*\tHo·∫∑c b·∫°n √©p broadcast()"
      ],
      "metadata": {
        "id": "J_QqGP0Oh_Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
        "\n",
        "orders.join(customers, \"customer_id\").explain()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urqR12TCiCN3",
        "outputId": "a23dd1a9-0d81-4c35-b17b-91ec4acb96c4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [customer_id#109L, id#108L]\n",
            "   +- SortMergeJoin [customer_id#109L], [customer_id#111L], Inner\n",
            "      :- Sort [customer_id#109L ASC NULLS FIRST], false, 0\n",
            "      :  +- Exchange hashpartitioning(customer_id#109L, 4), ENSURE_REQUIREMENTS, [plan_id=209]\n",
            "      :     +- Project [id#108L, (id#108L % 1000) AS customer_id#109L]\n",
            "      :        +- Filter isnotnull((id#108L % 1000))\n",
            "      :           +- Range (0, 1000000, step=1, splits=2)\n",
            "      +- Sort [customer_id#111L ASC NULLS FIRST], false, 0\n",
            "         +- Exchange hashpartitioning(customer_id#111L, 4), ENSURE_REQUIREMENTS, [plan_id=210]\n",
            "            +- Project [id#110L AS customer_id#111L]\n",
            "               +- Range (0, 1000, step=1, splits=2)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† T√ìM T·∫ÆT\n",
        "\n",
        "### Spark Join Strategy Summary\n",
        "\n",
        "1. orders.join(customers)\n",
        "- SortMergeJoin\n",
        "- Shuffle c·∫£ 2 b·∫£ng\n",
        "- Ch·∫≠m v·ªõi d·ªØ li·ªáu l·ªõn\n",
        "\n",
        "2. orders.join(broadcast(customers))\n",
        "- BroadcastHashJoin\n",
        "- Kh√¥ng shuffle b·∫£ng l·ªõn\n",
        "- Nhanh nh·∫•t\n",
        "\n",
        "3. Disable autoBroadcastJoinThreshold\n",
        "- Spark bu·ªôc d√πng SortMergeJoin\n",
        "- D√πng ƒë·ªÉ debug / h·ªçc"
      ],
      "metadata": {
        "id": "UT8s7OYeHO07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Spark Level 2 ‚Äì B√†i 6: Data Skew & Skew Join\n",
        "\n",
        "## ‚ö†Ô∏è ƒê√¢y l√† nguy√™n nh√¢n s·ªë 1 khi·∫øn Spark:\n",
        "*\tCh·∫°y m√£i kh√¥ng xong\n",
        "*\t1 executor ch·∫øt\n",
        "*\tTask stuck 99%\n",
        "\n",
        "---\n",
        "\n",
        "## 1Ô∏è‚É£ Data Skew l√† g√¨?\n",
        "\n",
        "Data Skew x·∫£y ra khi:\n",
        "- M·ªôt (ho·∫∑c v√†i) key chi·∫øm ph·∫ßn l·ªõn d·ªØ li·ªáu\n",
        "- M·ªôt task ph·∫£i x·ª≠ l√Ω qu√° nhi·ªÅu records\n",
        "- C√°c task kh√°c r·∫£nh ‚Üí cluster kh√¥ng c√¢n b·∫±ng\n",
        "\n",
        "### üìå V√≠ d·ª• th·ª±c t·∫ø (ng√¢n h√†ng):\n",
        "-\t1 merchant c·ª±c l·ªõn\n",
        "-\t1 kh√°ch h√†ng VIP\n",
        "-\t1 ng√†y giao d·ªãch cao ƒëi·ªÉm\n",
        "\n",
        "---\n",
        "\n",
        "## 2Ô∏è‚É£ D·∫•u hi·ªáu nh·∫≠n bi·∫øt Skew\n",
        "-\tJob ch·∫°y l√¢u ·ªü 1 stage\n",
        "-\tSpark UI:\n",
        ">*\t1 task ch·∫°y r·∫•t l√¢u\n",
        ">*\tC√°c task kh√°c DONE\n",
        "-\tExecutor OOM / Disk spill\n",
        "\n",
        "---\n",
        "\n",
        "## 3Ô∏è‚É£ V√≠ d·ª• Skew kinh ƒëi·ªÉn\n",
        "\n",
        "```python\n",
        "data = (\n",
        "    spark.range(0, 10_000_000)\n",
        "    .withColumn(\"key\",\n",
        "        when(col(\"id\") < 9_000_000, \"HOT\")\n",
        "        .otherwise(col(\"id\"))\n",
        "    )\n",
        ")\n",
        "\n",
        "data.groupBy(\"key\").count().explain()\n",
        "```\n",
        "\n",
        ">* üëâ Key \"HOT\" chi·∫øm 90% d·ªØ li·ªáu\n",
        ">* üëâ 1 reducer g√°nh h·∫øt\n",
        "\n",
        "---\n",
        "\n",
        "## 4Ô∏è‚É£ C√°ch x·ª≠ l√Ω Skew (C·ª∞C K·ª≤ QUAN TR·ªåNG)\n",
        "\n",
        "### üîπ C√°ch 1: Salting (th·ªß c√¥ng)\n",
        "\n",
        "```python\n",
        "from pyspark.sql.functions import rand, concat\n",
        "\n",
        "salted = data.withColumn(\n",
        "    \"salted_key\",\n",
        "    concat(col(\"key\"), (rand()*10).cast(\"int\"))\n",
        ")\n",
        "```\n",
        "\n",
        "> üëâ Chia 1 key l·ªõn th√†nh nhi·ªÅu key nh·ªè\n",
        "\n",
        "### üîπ C√°ch 2: AQE ‚Äì Adaptive Query Execution ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
        "\n",
        "```python\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
        "```\n",
        "\n",
        "#### üìå Spark 3+/4 s·∫Ω:\n",
        "-\tT·ª± detect skew\n",
        "-\tT·ª± chia task l·ªõn\n",
        "\n",
        "> **üëâ R·∫§T KHUY·∫æN NGH·ªä**\n",
        "\n",
        "### üîπ C√°ch 3: Broadcast b·∫£ng nh·ªè (n·∫øu c√≥)\n",
        "\n",
        "Skew join + broadcast = üî•üî•üî•\n",
        "\n",
        "---\n",
        "\n",
        "## 5Ô∏è‚É£ Skew Join th·ª±c t·∫ø trong CDP\n",
        "\n",
        "\n",
        "|**Case**|**Gi·∫£i ph√°p**|\n",
        "|--------|-------------|\n",
        "|Merchant l·ªõn|Salting|\n",
        "|Lookup nh·ªè|Broadcast|\n",
        "|Data Lake l·ªõn|AQE|\n",
        "|Batch ƒë√™m|Repartition tr∆∞·ªõc|\n",
        "\n",
        "---\n",
        "\n",
        "## üß† C√ÇU H·ªéI B·∫ÆT BU·ªòC ‚Äì B√ÄI 6\n",
        "\n",
        "1.\tV√¨ sao skew l√†m Spark ch·∫≠m d√π cluster c√≤n t√†i nguy√™n?\n",
        ">Data skew l√†m Spark ch·∫≠m v√¨:\n",
        ">>- M·ªôt (ho·∫∑c v√†i) key chi·∫øm ph·∫ßn l·ªõn d·ªØ li·ªáu\n",
        ">>- M·ªôt task ph·∫£i x·ª≠ l√Ω l∆∞·ª£ng d·ªØ li·ªáu v∆∞·ª£t tr·ªôi\n",
        ">>- C√°c executor kh√°c r·∫£nh nh∆∞ng kh√¥ng th·ªÉ h·ªó tr·ª£\n",
        ">>- Cluster b·ªã m·∫•t c√¢n b·∫±ng (resource under-utilization)\n",
        "> üëâ ƒêI·ªÇM C·ªêT L√ïI:\n",
        ">> Spark kh√¥ng th·ªÉ chia nh·ªè 1 task ƒëang ch·∫°y, d√π cluster c√≤n CPU/RAM.\n",
        "2.\tV√¨ sao 1 task ch·∫°y l√¢u l√†m c·∫£ stage b·ªã block?\n",
        ">V√¨ m·ªôt stage ch·ªâ ho√†n th√†nh khi T·∫§T C·∫¢ task trong stage ho√†n th√†nh.\n",
        ">N·∫øu 1 task b·ªã ch·∫≠m (straggler), stage b·ªã block,\n",
        ">k√©o theo to√†n b·ªô DAG kh√¥ng th·ªÉ ti·∫øn ti·∫øp.\n",
        ">> üëâ ƒê√¢y g·ªçi l√† Straggler Problem\n",
        "3.\tSalting gi·∫£i quy·∫øt skew b·∫±ng c√°ch n√†o?\n",
        "> Salting gi·∫£i quy·∫øt skew b·∫±ng c√°ch:\n",
        ">>- Th√™m m·ªôt \"salt\" ng·∫´u nhi√™n v√†o key\n",
        ">>- Bi·∫øn m·ªôt key l·ªõn th√†nh nhi·ªÅu key nh·ªè\n",
        ">>- Ph√¢n t√°n d·ªØ li·ªáu sang nhi·ªÅu task kh√°c nhau\n",
        "4.\tAQE x·ª≠ l√Ω skew t·ªët h∆°n salting ·ªü ƒëi·ªÉm n√†o?\n",
        ">AQE x·ª≠ l√Ω skew t·ªët h∆°n salting v√¨:\n",
        ">>- T·ª± ƒë·ªông detect skew d·ª±a tr√™n runtime statistics\n",
        ">>- T·ª± chia l·∫°i partition l·ªõn th√†nh nhi·ªÅu partition nh·ªè\n",
        ">>- Kh√¥ng c·∫ßn thay ƒë·ªïi code\n",
        ">>- √çt r·ªßi ro logic h∆°n salting\n",
        "\n",
        ">**üëâ AQE = production best practice (Spark 3+)**\n",
        "5.\tKhi n√†o KH√îNG n√™n d√πng salting?\n",
        ">KH√îNG n√™n d√πng salting khi:\n",
        ">>- Join c·∫ßn t√≠nh ch√≠nh x√°c theo key (logic ph·ª©c t·∫°p)\n",
        ">>- D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c AQE x·ª≠ l√Ω t·ªët\n",
        ">>- Lookup table nh·ªè c√≥ th·ªÉ broadcast\n",
        ">>- Pipeline c·∫ßn ƒë∆°n gi·∫£n, d·ªÖ b·∫£o tr√¨\n",
        "\n",
        "**üìå Salting l√† gi·∫£i ph√°p th·ªß c√¥ng, d·ªÖ g√¢y:**\n",
        ">*\tBug logic\n",
        ">*\tCode kh√≥ maintain\n",
        "\n",
        "## üéØ K·∫æT LU·∫¨N B√ÄI 6\n",
        "\n",
        "**üëâ C·∫ßn HI·ªÇU ƒê√öNG:**\n",
        ">*\tD√πng thu·∫≠t ng·ªØ ch√≠nh x√°c h∆°n\n",
        ">*\tNh·∫•n m·∫°nh ‚Äústage barrier‚Äù & ‚Äústraggler‚Äù"
      ],
      "metadata": {
        "id": "5y5GkgDAHpG4"
      }
    }
  ]
}